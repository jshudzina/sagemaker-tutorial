{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "***\n",
    "\n",
    "Amazon SageMaker NTM (Neural Topic Model) is an unsupervised learning algorithm that attempts to describe a set of observations as a mixture of distinct categories. NTM is most commonly used to discover a user-specified number of topics shared by documents within a text corpus. Here each observation is a document, the features are the presence (or occurrence count) of each word, and the categories are the topics. Since the method is unsupervised, the topics are not specified up front, and are not guaranteed to align with how a human may naturally categorize documents. The topics are learned as a probability distribution over the words that occur in each document. Each document, in turn, is described as a mixture of topics.\n",
    "\n",
    "In this notebook we will use the Amazon SageMaker NTM algorithm to train a model on some example synthetic data. We will then use this model to classify (perform inference on) the data. The main goals of this notebook are to,\n",
    "\n",
    "* create an AWS SageMaker training job on a data set to produce a NTM model,\n",
    "* use the model to perform inference with an Amazon SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Created S3 bucket: sagemaker-us-east-1-228889150161\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Once the data is preprocessed and available in a recommended format the next step is to train our model on the data. There are number of parameters required by the NTM algorithm to configure the model and define the computational environment in which training will take place. The first of these is to point to a container image which holds the algorithms training and hosting code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "containers = {'us-west-2': '174872318107.dkr.ecr.us-west-2.amazonaws.com/ntm:latest',\n",
    "              'us-east-1': '382416733822.dkr.ecr.us-east-1.amazonaws.com/ntm:latest',\n",
    "              'us-east-2': '404615174143.dkr.ecr.us-east-2.amazonaws.com/ntm:latest',\n",
    "              'eu-west-1': '438346466558.dkr.ecr.eu-west-1.amazonaws.com/ntm:latest'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An NTM model uses the following hyperparameters:\n",
    "\n",
    "- **num_topics** - The number of topics or categories in the NTM model. \n",
    "- **feature_dim** - The size of the \"vocabulary\". In this case, this has been set to 1000 by the nytimes pyspark data prep.\n",
    "\n",
    "In addition to these NTM model hyperparameters, we provide additional parameters defining things like the EC2 instance type on which training will run, the S3 bucket containing the data, and the AWS access role.\n",
    "\n",
    "> Note: Try adjusting the mini_batch_size if running on a GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics=20\n",
    "vocabulary_size=5000\n",
    "output = 's3://{}/data/nytimes-model/sagemaker-ntm'.format(sagemaker_session.default_bucket())\n",
    "\n",
    "ntm = sagemaker.estimator.Estimator(containers[boto3.Session().region_name],\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.p3.2xlarge',\n",
    "                                    output_path=output,\n",
    "                                    sagemaker_session=sagemaker_session)\n",
    "\n",
    "ntm.set_hyperparameters(num_topics=num_topics,\n",
    "                        feature_dim=vocabulary_size,\n",
    "                        mini_batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll train against the bag-of-words extracted from the NY Times comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3_client = boto3.client('s3')\n",
    "objects = s3_client.list_objects(Bucket=bucket, Prefix='data/nyt-record-io/training.rec')\n",
    "training_key = objects['Contents'][0]['Key']\n",
    "training_input = 's3://{}/{}'.format(bucket, training_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: ntm-2018-06-03-17-15-41-948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......................\n",
      "\u001b[31mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:11 INFO 139893340792640] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/default-input.json: {u'num_patience_epochs': u'3', u'clip_gradient': u'Inf', u'encoder_layers': u'auto', u'optimizer': u'adadelta', u'_kvstore': u'auto', u'rescale_gradient': u'1.0', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'learning_rate': u'0.01', u'_data_format': u'record', u'epochs': u'50', u'weight_decay': u'0.0', u'_num_kv_servers': u'auto', u'encoder_layers_activation': u'sigmoid', u'mini_batch_size': u'256', u'tolerance': u'0.001', u'batch_norm': u'false'}\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:11 INFO 139893340792640] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'feature_dim': u'5000', u'mini_batch_size': u'1024', u'num_topics': u'20'}\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:11 INFO 139893340792640] Final configuration: {u'num_patience_epochs': u'3', u'clip_gradient': u'Inf', u'encoder_layers': u'auto', u'optimizer': u'adadelta', u'_kvstore': u'auto', u'rescale_gradient': u'1.0', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'learning_rate': u'0.01', u'_data_format': u'record', u'epochs': u'50', u'feature_dim': u'5000', u'weight_decay': u'0.0', u'num_topics': u'20', u'_num_kv_servers': u'auto', u'encoder_layers_activation': u'sigmoid', u'mini_batch_size': u'1024', u'tolerance': u'0.001', u'batch_norm': u'false'}\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:11 INFO 139893340792640] Using default worker.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:11 INFO 139893340792640] Initializing\u001b[0m\n",
      "\u001b[31m/opt/amazon/lib/python2.7/site-packages/ai_algorithms_sdk/config/config_helper.py:122: DeprecationWarning: deprecated\n",
      "  warnings.warn(\"deprecated\", DeprecationWarning)\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:11 INFO 139893340792640] nvidia-smi took: 0.0251090526581 secs to identify 1 gpus\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:11 INFO 139893340792640] Number of GPUs being used: 1\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:21 INFO 139893340792640] Create Store: local\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}}, \"EndTime\": 1528046361.057926, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1528046361.057883}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:21 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:21 INFO 139893340792640] # Starting training for epoch 1\u001b[0m\n",
      "\u001b[31m[17:19:21] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.200341.0/RHEL5_64/generic-flavor/src/src/ndarray/./../operator/tensor/.././../common/utils.h:416: \u001b[0m\n",
      "\u001b[31mStorage fallback detected:\u001b[0m\n",
      "\u001b[31mCopy from csr storage type on cpu to default storage type on gpu.\u001b[0m\n",
      "\u001b[31mA temporary ndarray with default storage type will be generated in order to perform the copy. You can set environment variable MXNET_STORAGE_FALLBACK_LOG_VERBOSE to 0 to suppress this warning.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:25 INFO 139893340792640] # Finished training epoch 1 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:25 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:25 INFO 139893340792640] Loss (name: value) total: 7.75356005736\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:25 INFO 139893340792640] Loss (name: value) kld: 0.011004561047\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:25 INFO 139893340792640] Loss (name: value) recons: 7.74255547716\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:25 INFO 139893340792640] Loss (name: value) logppx: 7.75356005736\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:25 INFO 139893340792640] #quality_metric: host=algo-1, epoch=1, train total_loss <loss>=7.75356005736\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:25 INFO 139893340792640] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Total Records Seen\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1528046365.504933, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 0}, \"StartTime\": 1528046361.058263}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:25 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=55526.7098663 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:25 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:25 INFO 139893340792640] # Starting training for epoch 2\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:29 INFO 139893340792640] # Finished training epoch 2 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:29 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:29 INFO 139893340792640] Loss (name: value) total: 7.7005846961\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:29 INFO 139893340792640] Loss (name: value) kld: 0.0131448668126\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:29 INFO 139893340792640] Loss (name: value) recons: 7.68743983872\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:29 INFO 139893340792640] Loss (name: value) logppx: 7.7005846961\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:29 INFO 139893340792640] #quality_metric: host=algo-1, epoch=2, train total_loss <loss>=7.7005846961\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:29 INFO 139893340792640] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 484, \"sum\": 484.0, \"min\": 484}, \"Total Records Seen\": {\"count\": 1, \"max\": 493836, \"sum\": 493836.0, \"min\": 493836}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 4, \"sum\": 4.0, \"min\": 4}}, \"EndTime\": 1528046369.906304, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 1}, \"StartTime\": 1528046365.512779}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:29 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=56197.9747806 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:29 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:29 INFO 139893340792640] # Starting training for epoch 3\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:34 INFO 139893340792640] # Finished training epoch 3 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:34 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:34 INFO 139893340792640] Loss (name: value) total: 7.68947731759\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:34 INFO 139893340792640] Loss (name: value) kld: 0.0165079108955\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:34 INFO 139893340792640] Loss (name: value) recons: 7.6729694233\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:34 INFO 139893340792640] Loss (name: value) logppx: 7.68947731759\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:34 INFO 139893340792640] #quality_metric: host=algo-1, epoch=3, train total_loss <loss>=7.68947731759\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:34 INFO 139893340792640] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 726, \"sum\": 726.0, \"min\": 726}, \"Total Records Seen\": {\"count\": 1, \"max\": 740754, \"sum\": 740754.0, \"min\": 740754}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 6, \"sum\": 6.0, \"min\": 6}}, \"EndTime\": 1528046374.357862, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 2}, \"StartTime\": 1528046369.906638}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:34 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=55470.0275255 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:34 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:34 INFO 139893340792640] # Starting training for epoch 4\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:38 INFO 139893340792640] # Finished training epoch 4 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:38 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:38 INFO 139893340792640] Loss (name: value) total: 7.68232412525\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:38 INFO 139893340792640] Loss (name: value) kld: 0.0180146205066\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:38 INFO 139893340792640] Loss (name: value) recons: 7.66430950116\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:38 INFO 139893340792640] Loss (name: value) logppx: 7.68232412525\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:38 INFO 139893340792640] #quality_metric: host=algo-1, epoch=4, train total_loss <loss>=7.68232412525\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:38 INFO 139893340792640] patience losses:[7.7535600573563377, 7.700584696097807, 7.6894773175893736] min patience loss:7.68947731759 current loss:7.68232412525 absolute loss difference:0.00715319233492\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:38 INFO 139893340792640] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 968, \"sum\": 968.0, \"min\": 968}, \"Total Records Seen\": {\"count\": 1, \"max\": 987672, \"sum\": 987672.0, \"min\": 987672}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}}, \"EndTime\": 1528046378.778351, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 3}, \"StartTime\": 1528046374.365222}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:38 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=55948.2285778 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:38 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:38 INFO 139893340792640] # Starting training for epoch 5\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[06/03/2018 17:19:43 INFO 139893340792640] # Finished training epoch 5 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:43 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:43 INFO 139893340792640] Loss (name: value) total: 7.67566204317\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:43 INFO 139893340792640] Loss (name: value) kld: 0.0193515777624\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:43 INFO 139893340792640] Loss (name: value) recons: 7.65631049256\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:43 INFO 139893340792640] Loss (name: value) logppx: 7.67566204317\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:43 INFO 139893340792640] #quality_metric: host=algo-1, epoch=5, train total_loss <loss>=7.67566204317\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:43 INFO 139893340792640] patience losses:[7.700584696097807, 7.6894773175893736, 7.6823241252544499] min patience loss:7.68232412525 current loss:7.67566204317 absolute loss difference:0.006662082081\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:43 INFO 139893340792640] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1210, \"sum\": 1210.0, \"min\": 1210}, \"Total Records Seen\": {\"count\": 1, \"max\": 1234590, \"sum\": 1234590.0, \"min\": 1234590}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}}, \"EndTime\": 1528046383.238778, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 4}, \"StartTime\": 1528046378.778718}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:43 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=55360.0867519 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:43 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:43 INFO 139893340792640] # Starting training for epoch 6\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:47 INFO 139893340792640] # Finished training epoch 6 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:47 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:47 INFO 139893340792640] Loss (name: value) total: 7.67282763296\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:47 INFO 139893340792640] Loss (name: value) kld: 0.0202872809427\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:47 INFO 139893340792640] Loss (name: value) recons: 7.65254030494\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:47 INFO 139893340792640] Loss (name: value) logppx: 7.67282763296\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:47 INFO 139893340792640] #quality_metric: host=algo-1, epoch=6, train total_loss <loss>=7.67282763296\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:47 INFO 139893340792640] patience losses:[7.6894773175893736, 7.6823241252544499, 7.6756620431734515] min patience loss:7.67566204317 current loss:7.67282763296 absolute loss difference:0.00283441021423\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:47 INFO 139893340792640] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1452, \"sum\": 1452.0, \"min\": 1452}, \"Total Records Seen\": {\"count\": 1, \"max\": 1481508, \"sum\": 1481508.0, \"min\": 1481508}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 12, \"sum\": 12.0, \"min\": 12}}, \"EndTime\": 1528046387.681198, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 5}, \"StartTime\": 1528046383.242505}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:47 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=55626.4573361 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:47 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:47 INFO 139893340792640] # Starting training for epoch 7\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:52 INFO 139893340792640] # Finished training epoch 7 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:52 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:52 INFO 139893340792640] Loss (name: value) total: 7.66886634669\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:52 INFO 139893340792640] Loss (name: value) kld: 0.0212516611392\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:52 INFO 139893340792640] Loss (name: value) recons: 7.6476146798\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:52 INFO 139893340792640] Loss (name: value) logppx: 7.66886634669\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:52 INFO 139893340792640] #quality_metric: host=algo-1, epoch=7, train total_loss <loss>=7.66886634669\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:52 INFO 139893340792640] patience losses:[7.6823241252544499, 7.6756620431734515, 7.6728276329592235] min patience loss:7.67282763296 current loss:7.66886634669 absolute loss difference:0.00396128626894\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:52 INFO 139893340792640] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1694, \"sum\": 1694.0, \"min\": 1694}, \"Total Records Seen\": {\"count\": 1, \"max\": 1728426, \"sum\": 1728426.0, \"min\": 1728426}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 14, \"sum\": 14.0, \"min\": 14}}, \"EndTime\": 1528046392.077833, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 6}, \"StartTime\": 1528046387.685921}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:52 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=56218.923505 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:52 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:52 INFO 139893340792640] # Starting training for epoch 8\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:56 INFO 139893340792640] # Finished training epoch 8 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:56 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:56 INFO 139893340792640] Loss (name: value) total: 7.66544020127\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:56 INFO 139893340792640] Loss (name: value) kld: 0.0223593635106\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:56 INFO 139893340792640] Loss (name: value) recons: 7.64308083944\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:56 INFO 139893340792640] Loss (name: value) logppx: 7.66544020127\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:56 INFO 139893340792640] #quality_metric: host=algo-1, epoch=8, train total_loss <loss>=7.66544020127\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:56 INFO 139893340792640] patience losses:[7.6756620431734515, 7.6728276329592235, 7.6688663466902804] min patience loss:7.66886634669 current loss:7.66544020127 absolute loss difference:0.00342614542354\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:56 INFO 139893340792640] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1936, \"sum\": 1936.0, \"min\": 1936}, \"Total Records Seen\": {\"count\": 1, \"max\": 1975344, \"sum\": 1975344.0, \"min\": 1975344}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 16, \"sum\": 16.0, \"min\": 16}}, \"EndTime\": 1528046396.534322, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 7}, \"StartTime\": 1528046392.078233}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:56 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=55408.7469155 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:56 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:19:56 INFO 139893340792640] # Starting training for epoch 9\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:00 INFO 139893340792640] # Finished training epoch 9 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:00 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:00 INFO 139893340792640] Loss (name: value) total: 7.66294347797\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:00 INFO 139893340792640] Loss (name: value) kld: 0.0235288414646\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:00 INFO 139893340792640] Loss (name: value) recons: 7.63941462523\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:00 INFO 139893340792640] Loss (name: value) logppx: 7.66294347797\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:00 INFO 139893340792640] #quality_metric: host=algo-1, epoch=9, train total_loss <loss>=7.66294347797\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:00 INFO 139893340792640] patience losses:[7.6728276329592235, 7.6688663466902804, 7.6654402012667378] min patience loss:7.66544020127 current loss:7.66294347797 absolute loss difference:0.00249672330115\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:00 INFO 139893340792640] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2178, \"sum\": 2178.0, \"min\": 2178}, \"Total Records Seen\": {\"count\": 1, \"max\": 2222262, \"sum\": 2222262.0, \"min\": 2222262}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 18, \"sum\": 18.0, \"min\": 18}}, \"EndTime\": 1528046400.966611, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 8}, \"StartTime\": 1528046396.534695}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:00 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=55711.551108 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:00 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:00 INFO 139893340792640] # Starting training for epoch 10\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[06/03/2018 17:20:05 INFO 139893340792640] # Finished training epoch 10 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:05 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:05 INFO 139893340792640] Loss (name: value) total: 7.66044490175\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:05 INFO 139893340792640] Loss (name: value) kld: 0.0243345500366\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:05 INFO 139893340792640] Loss (name: value) recons: 7.63611034347\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:05 INFO 139893340792640] Loss (name: value) logppx: 7.66044490175\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:05 INFO 139893340792640] #quality_metric: host=algo-1, epoch=10, train total_loss <loss>=7.66044490175\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:05 INFO 139893340792640] patience losses:[7.6688663466902804, 7.6654402012667378, 7.6629434779655838] min patience loss:7.66294347797 current loss:7.66044490175 absolute loss difference:0.00249857621745\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:05 INFO 139893340792640] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2420, \"sum\": 2420.0, \"min\": 2420}, \"Total Records Seen\": {\"count\": 1, \"max\": 2469180, \"sum\": 2469180.0, \"min\": 2469180}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 20, \"sum\": 20.0, \"min\": 20}}, \"EndTime\": 1528046405.421093, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 9}, \"StartTime\": 1528046400.966965}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:05 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=55433.8256432 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:05 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:05 INFO 139893340792640] # Starting training for epoch 11\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:09 INFO 139893340792640] # Finished training epoch 11 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:09 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:09 INFO 139893340792640] Loss (name: value) total: 7.65782475077\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:09 INFO 139893340792640] Loss (name: value) kld: 0.0252978503483\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:09 INFO 139893340792640] Loss (name: value) recons: 7.63252694819\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:09 INFO 139893340792640] Loss (name: value) logppx: 7.65782475077\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:09 INFO 139893340792640] #quality_metric: host=algo-1, epoch=11, train total_loss <loss>=7.65782475077\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:09 INFO 139893340792640] patience losses:[7.6654402012667378, 7.6629434779655838, 7.6604449017481366] min patience loss:7.66044490175 current loss:7.65782475077 absolute loss difference:0.00262015097397\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:09 INFO 139893340792640] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2662, \"sum\": 2662.0, \"min\": 2662}, \"Total Records Seen\": {\"count\": 1, \"max\": 2716098, \"sum\": 2716098.0, \"min\": 2716098}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 22, \"sum\": 22.0, \"min\": 22}}, \"EndTime\": 1528046409.845981, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 10}, \"StartTime\": 1528046405.421436}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:09 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=55804.3978994 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:09 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:09 INFO 139893340792640] # Starting training for epoch 12\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:14 INFO 139893340792640] # Finished training epoch 12 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:14 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:14 INFO 139893340792640] Loss (name: value) total: 7.65572415502\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:14 INFO 139893340792640] Loss (name: value) kld: 0.0260352659259\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:14 INFO 139893340792640] Loss (name: value) recons: 7.6296888777\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:14 INFO 139893340792640] Loss (name: value) logppx: 7.65572415502\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:14 INFO 139893340792640] #quality_metric: host=algo-1, epoch=12, train total_loss <loss>=7.65572415502\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:14 INFO 139893340792640] patience losses:[7.6629434779655838, 7.6604449017481366, 7.6578247507741626] min patience loss:7.65782475077 current loss:7.65572415502 absolute loss difference:0.00210059575798\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:14 INFO 139893340792640] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2904, \"sum\": 2904.0, \"min\": 2904}, \"Total Records Seen\": {\"count\": 1, \"max\": 2963016, \"sum\": 2963016.0, \"min\": 2963016}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 24, \"sum\": 24.0, \"min\": 24}}, \"EndTime\": 1528046414.235267, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 11}, \"StartTime\": 1528046409.853265}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:14 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=56345.5821889 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:14 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:14 INFO 139893340792640] # Starting training for epoch 13\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:18 INFO 139893340792640] # Finished training epoch 13 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:18 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:18 INFO 139893340792640] Loss (name: value) total: 7.65414266744\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:18 INFO 139893340792640] Loss (name: value) kld: 0.0269348397986\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:18 INFO 139893340792640] Loss (name: value) recons: 7.62720784274\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:18 INFO 139893340792640] Loss (name: value) logppx: 7.65414266744\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:18 INFO 139893340792640] #quality_metric: host=algo-1, epoch=13, train total_loss <loss>=7.65414266744\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:18 INFO 139893340792640] patience losses:[7.6604449017481366, 7.6578247507741626, 7.6557241550161823] min patience loss:7.65572415502 current loss:7.65414266744 absolute loss difference:0.00158148757682\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:18 INFO 139893340792640] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3146, \"sum\": 3146.0, \"min\": 3146}, \"Total Records Seen\": {\"count\": 1, \"max\": 3209934, \"sum\": 3209934.0, \"min\": 3209934}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 26, \"sum\": 26.0, \"min\": 26}}, \"EndTime\": 1528046418.608298, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 12}, \"StartTime\": 1528046414.235805}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:18 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=56468.7329632 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:18 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:18 INFO 139893340792640] # Starting training for epoch 14\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:22 INFO 139893340792640] # Finished training epoch 14 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:22 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:22 INFO 139893340792640] Loss (name: value) total: 7.6516125813\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:22 INFO 139893340792640] Loss (name: value) kld: 0.0276173064043\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:22 INFO 139893340792640] Loss (name: value) recons: 7.62399528711\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:22 INFO 139893340792640] Loss (name: value) logppx: 7.6516125813\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:22 INFO 139893340792640] #quality_metric: host=algo-1, epoch=14, train total_loss <loss>=7.6516125813\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:22 INFO 139893340792640] patience losses:[7.6578247507741626, 7.6557241550161823, 7.6541426674393582] min patience loss:7.65414266744 current loss:7.6516125813 absolute loss difference:0.00253008613902\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:22 INFO 139893340792640] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3388, \"sum\": 3388.0, \"min\": 3388}, \"Total Records Seen\": {\"count\": 1, \"max\": 3456852, \"sum\": 3456852.0, \"min\": 3456852}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 28, \"sum\": 28.0, \"min\": 28}}, \"EndTime\": 1528046422.999833, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 13}, \"StartTime\": 1528046418.610529}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:22 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=56252.1708729 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:23 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:23 INFO 139893340792640] # Starting training for epoch 15\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:27 INFO 139893340792640] # Finished training epoch 15 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:27 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:27 INFO 139893340792640] Loss (name: value) total: 7.64914277319\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:27 INFO 139893340792640] Loss (name: value) kld: 0.0299862456077\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:27 INFO 139893340792640] Loss (name: value) recons: 7.61915652959\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:27 INFO 139893340792640] Loss (name: value) logppx: 7.64914277319\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:27 INFO 139893340792640] #quality_metric: host=algo-1, epoch=15, train total_loss <loss>=7.64914277319\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:27 INFO 139893340792640] patience losses:[7.6557241550161823, 7.6541426674393582, 7.6516125813003413] min patience loss:7.6516125813 current loss:7.64914277319 absolute loss difference:0.00246980810954\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:27 INFO 139893340792640] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3630, \"sum\": 3630.0, \"min\": 3630}, \"Total Records Seen\": {\"count\": 1, \"max\": 3703770, \"sum\": 3703770.0, \"min\": 3703770}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 30, \"sum\": 30.0, \"min\": 30}}, \"EndTime\": 1528046427.396899, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 14}, \"StartTime\": 1528046423.000206}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:27 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=56157.9328979 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:27 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:27 INFO 139893340792640] # Starting training for epoch 16\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:31 INFO 139893340792640] # Finished training epoch 16 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:31 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:31 INFO 139893340792640] Loss (name: value) total: 7.64194814849\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:31 INFO 139893340792640] Loss (name: value) kld: 0.0378602123704\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:31 INFO 139893340792640] Loss (name: value) recons: 7.60408795348\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:31 INFO 139893340792640] Loss (name: value) logppx: 7.64194814849\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:31 INFO 139893340792640] #quality_metric: host=algo-1, epoch=16, train total_loss <loss>=7.64194814849\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:31 INFO 139893340792640] patience losses:[7.6541426674393582, 7.6516125813003413, 7.6491427731908059] min patience loss:7.64914277319 current loss:7.64194814849 absolute loss difference:0.00719462470575\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:31 INFO 139893340792640] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3872, \"sum\": 3872.0, \"min\": 3872}, \"Total Records Seen\": {\"count\": 1, \"max\": 3950688, \"sum\": 3950688.0, \"min\": 3950688}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 32, \"sum\": 32.0, \"min\": 32}}, \"EndTime\": 1528046431.829235, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 15}, \"StartTime\": 1528046427.411845}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:31 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=55890.785386 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:31 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:31 INFO 139893340792640] # Starting training for epoch 17\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[06/03/2018 17:20:36 INFO 139893340792640] # Finished training epoch 17 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:36 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:36 INFO 139893340792640] Loss (name: value) total: 7.63342382248\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:36 INFO 139893340792640] Loss (name: value) kld: 0.0447227580007\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:36 INFO 139893340792640] Loss (name: value) recons: 7.58870105335\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:36 INFO 139893340792640] Loss (name: value) logppx: 7.63342382248\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:36 INFO 139893340792640] #quality_metric: host=algo-1, epoch=17, train total_loss <loss>=7.63342382248\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:36 INFO 139893340792640] patience losses:[7.6516125813003413, 7.6491427731908059, 7.6419481484850573] min patience loss:7.64194814849 current loss:7.63342382248 absolute loss difference:0.00852432600723\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:36 INFO 139893340792640] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 4114, \"sum\": 4114.0, \"min\": 4114}, \"Total Records Seen\": {\"count\": 1, \"max\": 4197606, \"sum\": 4197606.0, \"min\": 4197606}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 34, \"sum\": 34.0, \"min\": 34}}, \"EndTime\": 1528046436.279305, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 16}, \"StartTime\": 1528046431.850498}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:36 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=55750.6226064 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:36 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:36 INFO 139893340792640] # Starting training for epoch 18\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:40 INFO 139893340792640] # Finished training epoch 18 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:40 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:40 INFO 139893340792640] Loss (name: value) total: 7.62725878107\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:40 INFO 139893340792640] Loss (name: value) kld: 0.0491926917741\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:40 INFO 139893340792640] Loss (name: value) recons: 7.57806609805\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:40 INFO 139893340792640] Loss (name: value) logppx: 7.62725878107\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:40 INFO 139893340792640] #quality_metric: host=algo-1, epoch=18, train total_loss <loss>=7.62725878107\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:40 INFO 139893340792640] patience losses:[7.6491427731908059, 7.6419481484850573, 7.6334238224778295] min patience loss:7.63342382248 current loss:7.62725878107 absolute loss difference:0.00616504141122\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:40 INFO 139893340792640] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 4356, \"sum\": 4356.0, \"min\": 4356}, \"Total Records Seen\": {\"count\": 1, \"max\": 4444524, \"sum\": 4444524.0, \"min\": 4444524}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}}, \"EndTime\": 1528046440.693043, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 17}, \"StartTime\": 1528046436.27999}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:40 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=55949.8123897 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:40 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:40 INFO 139893340792640] # Starting training for epoch 19\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:45 INFO 139893340792640] # Finished training epoch 19 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:45 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:45 INFO 139893340792640] Loss (name: value) total: 7.62325194355\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:45 INFO 139893340792640] Loss (name: value) kld: 0.0529054144844\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:45 INFO 139893340792640] Loss (name: value) recons: 7.57034653401\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:45 INFO 139893340792640] Loss (name: value) logppx: 7.62325194355\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:45 INFO 139893340792640] #quality_metric: host=algo-1, epoch=19, train total_loss <loss>=7.62325194355\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:45 INFO 139893340792640] patience losses:[7.6419481484850573, 7.6334238224778295, 7.6272587810666108] min patience loss:7.62725878107 current loss:7.62325194355 absolute loss difference:0.00400683751776\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:45 INFO 139893340792640] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 4598, \"sum\": 4598.0, \"min\": 4598}, \"Total Records Seen\": {\"count\": 1, \"max\": 4691442, \"sum\": 4691442.0, \"min\": 4691442}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 38, \"sum\": 38.0, \"min\": 38}}, \"EndTime\": 1528046445.135532, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 18}, \"StartTime\": 1528046440.693477}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:45 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=55581.0799666 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:45 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:45 INFO 139893340792640] # Starting training for epoch 20\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:49 INFO 139893340792640] # Finished training epoch 20 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:49 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:49 INFO 139893340792640] Loss (name: value) total: 7.62053587589\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:49 INFO 139893340792640] Loss (name: value) kld: 0.0564985080384\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:49 INFO 139893340792640] Loss (name: value) recons: 7.56403734615\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:49 INFO 139893340792640] Loss (name: value) logppx: 7.62053587589\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:49 INFO 139893340792640] #quality_metric: host=algo-1, epoch=20, train total_loss <loss>=7.62053587589\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:49 INFO 139893340792640] patience losses:[7.6334238224778295, 7.6272587810666108, 7.6232519435488486] min patience loss:7.62325194355 current loss:7.62053587589 absolute loss difference:0.00271606765503\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:49 INFO 139893340792640] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 4840, \"sum\": 4840.0, \"min\": 4840}, \"Total Records Seen\": {\"count\": 1, \"max\": 4938360, \"sum\": 4938360.0, \"min\": 4938360}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 40, \"sum\": 40.0, \"min\": 40}}, \"EndTime\": 1528046449.514667, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 19}, \"StartTime\": 1528046445.149566}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:49 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=56563.6687145 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:49 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:49 INFO 139893340792640] # Starting training for epoch 21\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:53 INFO 139893340792640] # Finished training epoch 21 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:53 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:53 INFO 139893340792640] Loss (name: value) total: 7.61632352328\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:53 INFO 139893340792640] Loss (name: value) kld: 0.0597515276379\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:53 INFO 139893340792640] Loss (name: value) recons: 7.5565719703\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:53 INFO 139893340792640] Loss (name: value) logppx: 7.61632352328\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:53 INFO 139893340792640] #quality_metric: host=algo-1, epoch=21, train total_loss <loss>=7.61632352328\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:53 INFO 139893340792640] patience losses:[7.6272587810666108, 7.6232519435488486, 7.6205358758938218] min patience loss:7.62053587589 current loss:7.61632352328 absolute loss difference:0.00421235260885\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:53 INFO 139893340792640] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 5082, \"sum\": 5082.0, \"min\": 5082}, \"Total Records Seen\": {\"count\": 1, \"max\": 5185278, \"sum\": 5185278.0, \"min\": 5185278}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 42, \"sum\": 42.0, \"min\": 42}}, \"EndTime\": 1528046453.874159, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 20}, \"StartTime\": 1528046449.514982}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:53 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=56641.3510953 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:53 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:53 INFO 139893340792640] # Starting training for epoch 22\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:58 INFO 139893340792640] # Finished training epoch 22 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:58 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:58 INFO 139893340792640] Loss (name: value) total: 7.6129691522\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:58 INFO 139893340792640] Loss (name: value) kld: 0.0632582918187\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:58 INFO 139893340792640] Loss (name: value) recons: 7.54971084614\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:58 INFO 139893340792640] Loss (name: value) logppx: 7.6129691522\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:58 INFO 139893340792640] #quality_metric: host=algo-1, epoch=22, train total_loss <loss>=7.6129691522\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:58 INFO 139893340792640] patience losses:[7.6232519435488486, 7.6205358758938218, 7.6163235232849749] min patience loss:7.61632352328 current loss:7.6129691522 absolute loss difference:0.00335437108662\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:58 INFO 139893340792640] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 5324, \"sum\": 5324.0, \"min\": 5324}, \"Total Records Seen\": {\"count\": 1, \"max\": 5432196, \"sum\": 5432196.0, \"min\": 5432196}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 44, \"sum\": 44.0, \"min\": 44}}, \"EndTime\": 1528046458.316123, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 21}, \"StartTime\": 1528046453.881362}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:58 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=55671.3312795 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:58 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:20:58 INFO 139893340792640] # Starting training for epoch 23\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[06/03/2018 17:21:02 INFO 139893340792640] # Finished training epoch 23 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:02 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:02 INFO 139893340792640] Loss (name: value) total: 7.60737993082\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:02 INFO 139893340792640] Loss (name: value) kld: 0.0674365092885\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:02 INFO 139893340792640] Loss (name: value) recons: 7.53994339384\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:02 INFO 139893340792640] Loss (name: value) logppx: 7.60737993082\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:02 INFO 139893340792640] #quality_metric: host=algo-1, epoch=23, train total_loss <loss>=7.60737993082\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:02 INFO 139893340792640] patience losses:[7.6205358758938218, 7.6163235232849749, 7.6129691521983505] min patience loss:7.6129691522 current loss:7.60737993082 absolute loss difference:0.00558922138096\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:02 INFO 139893340792640] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 5566, \"sum\": 5566.0, \"min\": 5566}, \"Total Records Seen\": {\"count\": 1, \"max\": 5679114, \"sum\": 5679114.0, \"min\": 5679114}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 46, \"sum\": 46.0, \"min\": 46}}, \"EndTime\": 1528046462.761588, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 22}, \"StartTime\": 1528046458.317004}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:02 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=55552.8669423 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:02 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:02 INFO 139893340792640] # Starting training for epoch 24\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:07 INFO 139893340792640] # Finished training epoch 24 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:07 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:07 INFO 139893340792640] Loss (name: value) total: 7.60112801761\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:07 INFO 139893340792640] Loss (name: value) kld: 0.0724000387425\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:07 INFO 139893340792640] Loss (name: value) recons: 7.52872796566\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:07 INFO 139893340792640] Loss (name: value) logppx: 7.60112801761\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:07 INFO 139893340792640] #quality_metric: host=algo-1, epoch=24, train total_loss <loss>=7.60112801761\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:07 INFO 139893340792640] patience losses:[7.6163235232849749, 7.6129691521983505, 7.6073799308173911] min patience loss:7.60737993082 current loss:7.60112801761 absolute loss difference:0.00625191321058\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:07 INFO 139893340792640] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 5808, \"sum\": 5808.0, \"min\": 5808}, \"Total Records Seen\": {\"count\": 1, \"max\": 5926032, \"sum\": 5926032.0, \"min\": 5926032}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 48, \"sum\": 48.0, \"min\": 48}}, \"EndTime\": 1528046467.173898, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 23}, \"StartTime\": 1528046462.76875}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:07 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=56050.1497886 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:07 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:07 INFO 139893340792640] # Starting training for epoch 25\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:11 INFO 139893340792640] # Finished training epoch 25 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:11 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:11 INFO 139893340792640] Loss (name: value) total: 7.59451366973\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:11 INFO 139893340792640] Loss (name: value) kld: 0.0778466606713\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:11 INFO 139893340792640] Loss (name: value) recons: 7.51666703475\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:11 INFO 139893340792640] Loss (name: value) logppx: 7.59451366973\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:11 INFO 139893340792640] #quality_metric: host=algo-1, epoch=25, train total_loss <loss>=7.59451366973\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:11 INFO 139893340792640] patience losses:[7.6129691521983505, 7.6073799308173911, 7.6011280176068139] min patience loss:7.60112801761 current loss:7.59451366973 absolute loss difference:0.00661434787364\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:11 INFO 139893340792640] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 6050, \"sum\": 6050.0, \"min\": 6050}, \"Total Records Seen\": {\"count\": 1, \"max\": 6172950, \"sum\": 6172950.0, \"min\": 6172950}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 50, \"sum\": 50.0, \"min\": 50}}, \"EndTime\": 1528046471.54076, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 24}, \"StartTime\": 1528046467.174445}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:11 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=56548.2386967 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:11 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:11 INFO 139893340792640] # Starting training for epoch 26\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:16 INFO 139893340792640] # Finished training epoch 26 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:16 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:16 INFO 139893340792640] Loss (name: value) total: 7.58896241085\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:16 INFO 139893340792640] Loss (name: value) kld: 0.0816696315296\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:16 INFO 139893340792640] Loss (name: value) recons: 7.50729276326\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:16 INFO 139893340792640] Loss (name: value) logppx: 7.58896241085\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:16 INFO 139893340792640] #quality_metric: host=algo-1, epoch=26, train total_loss <loss>=7.58896241085\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:16 INFO 139893340792640] patience losses:[7.6073799308173911, 7.6011280176068139, 7.5945136697331739] min patience loss:7.59451366973 current loss:7.58896241085 absolute loss difference:0.00555125888714\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:16 INFO 139893340792640] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 6292, \"sum\": 6292.0, \"min\": 6292}, \"Total Records Seen\": {\"count\": 1, \"max\": 6419868, \"sum\": 6419868.0, \"min\": 6419868}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 52, \"sum\": 52.0, \"min\": 52}}, \"EndTime\": 1528046476.005942, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 25}, \"StartTime\": 1528046471.541113}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:16 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=55300.9680185 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:16 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:16 INFO 139893340792640] # Starting training for epoch 27\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:20 INFO 139893340792640] # Finished training epoch 27 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:20 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:20 INFO 139893340792640] Loss (name: value) total: 7.58494360289\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:20 INFO 139893340792640] Loss (name: value) kld: 0.0847821269061\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:20 INFO 139893340792640] Loss (name: value) recons: 7.50016149066\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:20 INFO 139893340792640] Loss (name: value) logppx: 7.58494360289\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:20 INFO 139893340792640] #quality_metric: host=algo-1, epoch=27, train total_loss <loss>=7.58494360289\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:20 INFO 139893340792640] patience losses:[7.6011280176068139, 7.5945136697331739, 7.5889624108460323] min patience loss:7.58896241085 current loss:7.58494360289 absolute loss difference:0.00401880795305\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:20 INFO 139893340792640] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 6534, \"sum\": 6534.0, \"min\": 6534}, \"Total Records Seen\": {\"count\": 1, \"max\": 6666786, \"sum\": 6666786.0, \"min\": 6666786}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 54, \"sum\": 54.0, \"min\": 54}}, \"EndTime\": 1528046480.352308, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 26}, \"StartTime\": 1528046476.006329}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:20 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=56813.0050553 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:20 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:20 INFO 139893340792640] # Starting training for epoch 28\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[06/03/2018 17:21:24 INFO 139893340792640] # Finished training epoch 28 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:24 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:24 INFO 139893340792640] Loss (name: value) total: 7.58106571387\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:24 INFO 139893340792640] Loss (name: value) kld: 0.0874590917783\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:24 INFO 139893340792640] Loss (name: value) recons: 7.49360664965\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:24 INFO 139893340792640] Loss (name: value) logppx: 7.58106571387\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:24 INFO 139893340792640] #quality_metric: host=algo-1, epoch=28, train total_loss <loss>=7.58106571387\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:24 INFO 139893340792640] patience losses:[7.5945136697331739, 7.5889624108460323, 7.5849436028929782] min patience loss:7.58494360289 current loss:7.58106571387 absolute loss difference:0.0038778890263\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:24 INFO 139893340792640] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 6776, \"sum\": 6776.0, \"min\": 6776}, \"Total Records Seen\": {\"count\": 1, \"max\": 6913704, \"sum\": 6913704.0, \"min\": 6913704}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 56, \"sum\": 56.0, \"min\": 56}}, \"EndTime\": 1528046484.780352, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 27}, \"StartTime\": 1528046480.354495}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:24 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=55784.6553302 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:24 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:24 INFO 139893340792640] # Starting training for epoch 29\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:29 INFO 139893340792640] # Finished training epoch 29 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:29 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:29 INFO 139893340792640] Loss (name: value) total: 7.57858962239\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:29 INFO 139893340792640] Loss (name: value) kld: 0.0903191770088\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:29 INFO 139893340792640] Loss (name: value) recons: 7.48827043126\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:29 INFO 139893340792640] Loss (name: value) logppx: 7.57858962239\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:29 INFO 139893340792640] #quality_metric: host=algo-1, epoch=29, train total_loss <loss>=7.57858962239\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:29 INFO 139893340792640] patience losses:[7.5889624108460323, 7.5849436028929782, 7.5810657138666828] min patience loss:7.58106571387 current loss:7.57858962239 absolute loss difference:0.00247609147356\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:29 INFO 139893340792640] #progress_metric: host=algo-1, completed 58 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 7018, \"sum\": 7018.0, \"min\": 7018}, \"Total Records Seen\": {\"count\": 1, \"max\": 7160622, \"sum\": 7160622.0, \"min\": 7160622}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 58, \"sum\": 58.0, \"min\": 58}}, \"EndTime\": 1528046489.30017, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 28}, \"StartTime\": 1528046484.780934}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:29 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=54635.2741039 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:29 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:29 INFO 139893340792640] # Starting training for epoch 30\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:33 INFO 139893340792640] # Finished training epoch 30 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:33 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:33 INFO 139893340792640] Loss (name: value) total: 7.57499467735\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:33 INFO 139893340792640] Loss (name: value) kld: 0.0931172591118\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:33 INFO 139893340792640] Loss (name: value) recons: 7.48187742302\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:33 INFO 139893340792640] Loss (name: value) logppx: 7.57499467735\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:33 INFO 139893340792640] #quality_metric: host=algo-1, epoch=30, train total_loss <loss>=7.57499467735\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:33 INFO 139893340792640] patience losses:[7.5849436028929782, 7.5810657138666828, 7.5785896223931273] min patience loss:7.57858962239 current loss:7.57499467735 absolute loss difference:0.00359494503865\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:33 INFO 139893340792640] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 7260, \"sum\": 7260.0, \"min\": 7260}, \"Total Records Seen\": {\"count\": 1, \"max\": 7407540, \"sum\": 7407540.0, \"min\": 7407540}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 60, \"sum\": 60.0, \"min\": 60}}, \"EndTime\": 1528046493.709664, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 29}, \"StartTime\": 1528046489.300537}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:33 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=55999.5697534 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:33 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:33 INFO 139893340792640] # Starting training for epoch 31\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:38 INFO 139893340792640] # Finished training epoch 31 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:38 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:38 INFO 139893340792640] Loss (name: value) total: 7.57222301605\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:38 INFO 139893340792640] Loss (name: value) kld: 0.0968454685947\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:38 INFO 139893340792640] Loss (name: value) recons: 7.47537753528\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:38 INFO 139893340792640] Loss (name: value) logppx: 7.57222301605\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:38 INFO 139893340792640] #quality_metric: host=algo-1, epoch=31, train total_loss <loss>=7.57222301605\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:38 INFO 139893340792640] patience losses:[7.5810657138666828, 7.5785896223931273, 7.5749946773544812] min patience loss:7.57499467735 current loss:7.57222301605 absolute loss difference:0.00277166130129\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:38 INFO 139893340792640] #progress_metric: host=algo-1, completed 62 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 7502, \"sum\": 7502.0, \"min\": 7502}, \"Total Records Seen\": {\"count\": 1, \"max\": 7654458, \"sum\": 7654458.0, \"min\": 7654458}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 62, \"sum\": 62.0, \"min\": 62}}, \"EndTime\": 1528046498.068778, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 30}, \"StartTime\": 1528046493.710013}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:38 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=56646.5620884 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:38 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:38 INFO 139893340792640] # Starting training for epoch 32\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:42 INFO 139893340792640] # Finished training epoch 32 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:42 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:42 INFO 139893340792640] Loss (name: value) total: 7.56802198315\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:42 INFO 139893340792640] Loss (name: value) kld: 0.100351464196\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:42 INFO 139893340792640] Loss (name: value) recons: 7.46767053796\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:42 INFO 139893340792640] Loss (name: value) logppx: 7.56802198315\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:42 INFO 139893340792640] #quality_metric: host=algo-1, epoch=32, train total_loss <loss>=7.56802198315\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:42 INFO 139893340792640] patience losses:[7.5785896223931273, 7.5749946773544812, 7.5722230160531918] min patience loss:7.57222301605 current loss:7.56802198315 absolute loss difference:0.00420103289864\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:42 INFO 139893340792640] #progress_metric: host=algo-1, completed 64 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 7744, \"sum\": 7744.0, \"min\": 7744}, \"Total Records Seen\": {\"count\": 1, \"max\": 7901376, \"sum\": 7901376.0, \"min\": 7901376}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 64, \"sum\": 64.0, \"min\": 64}}, \"EndTime\": 1528046502.520094, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 31}, \"StartTime\": 1528046498.069095}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:42 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=55472.8174441 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:42 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:42 INFO 139893340792640] # Starting training for epoch 33\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[06/03/2018 17:21:46 INFO 139893340792640] # Finished training epoch 33 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:46 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:46 INFO 139893340792640] Loss (name: value) total: 7.5643788682\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:46 INFO 139893340792640] Loss (name: value) kld: 0.103331442568\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:46 INFO 139893340792640] Loss (name: value) recons: 7.461047392\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:46 INFO 139893340792640] Loss (name: value) logppx: 7.5643788682\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:46 INFO 139893340792640] #quality_metric: host=algo-1, epoch=33, train total_loss <loss>=7.5643788682\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:46 INFO 139893340792640] patience losses:[7.5749946773544812, 7.5722230160531918, 7.5680219831545488] min patience loss:7.56802198315 current loss:7.5643788682 absolute loss difference:0.00364311495103\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:46 INFO 139893340792640] #progress_metric: host=algo-1, completed 66 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 7986, \"sum\": 7986.0, \"min\": 7986}, \"Total Records Seen\": {\"count\": 1, \"max\": 8148294, \"sum\": 8148294.0, \"min\": 8148294}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 66, \"sum\": 66.0, \"min\": 66}}, \"EndTime\": 1528046506.905925, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 32}, \"StartTime\": 1528046502.520723}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:46 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=56302.0269601 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:46 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:46 INFO 139893340792640] # Starting training for epoch 34\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:51 INFO 139893340792640] # Finished training epoch 34 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:51 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:51 INFO 139893340792640] Loss (name: value) total: 7.56334872162\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:51 INFO 139893340792640] Loss (name: value) kld: 0.105815392003\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:51 INFO 139893340792640] Loss (name: value) recons: 7.45753334081\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:51 INFO 139893340792640] Loss (name: value) logppx: 7.56334872162\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:51 INFO 139893340792640] #quality_metric: host=algo-1, epoch=34, train total_loss <loss>=7.56334872162\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:51 INFO 139893340792640] patience losses:[7.5722230160531918, 7.5680219831545488, 7.5643788682035176] min patience loss:7.5643788682 current loss:7.56334872162 absolute loss difference:0.00103014658305\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:51 INFO 139893340792640] #progress_metric: host=algo-1, completed 68 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 8228, \"sum\": 8228.0, \"min\": 8228}, \"Total Records Seen\": {\"count\": 1, \"max\": 8395212, \"sum\": 8395212.0, \"min\": 8395212}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 68, \"sum\": 68.0, \"min\": 68}}, \"EndTime\": 1528046511.316556, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 33}, \"StartTime\": 1528046506.906243}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:51 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=55984.5730291 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:51 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:51 INFO 139893340792640] # Starting training for epoch 35\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:55 INFO 139893340792640] # Finished training epoch 35 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:55 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:55 INFO 139893340792640] Loss (name: value) total: 7.56085359188\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:55 INFO 139893340792640] Loss (name: value) kld: 0.107381720839\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:55 INFO 139893340792640] Loss (name: value) recons: 7.45347188179\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:55 INFO 139893340792640] Loss (name: value) logppx: 7.56085359188\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:55 INFO 139893340792640] #quality_metric: host=algo-1, epoch=35, train total_loss <loss>=7.56085359188\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:55 INFO 139893340792640] patience losses:[7.5680219831545488, 7.5643788682035176, 7.5633487216204651] min patience loss:7.56334872162 current loss:7.56085359188 absolute loss difference:0.00249512973896\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:55 INFO 139893340792640] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 8470, \"sum\": 8470.0, \"min\": 8470}, \"Total Records Seen\": {\"count\": 1, \"max\": 8642130, \"sum\": 8642130.0, \"min\": 8642130}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 70, \"sum\": 70.0, \"min\": 70}}, \"EndTime\": 1528046515.721679, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 34}, \"StartTime\": 1528046511.323761}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:55 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=56142.2242873 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:55 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:21:55 INFO 139893340792640] # Starting training for epoch 36\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:00 INFO 139893340792640] # Finished training epoch 36 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:00 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:00 INFO 139893340792640] Loss (name: value) total: 7.55977645541\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:00 INFO 139893340792640] Loss (name: value) kld: 0.109251854703\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:00 INFO 139893340792640] Loss (name: value) recons: 7.45052461437\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:00 INFO 139893340792640] Loss (name: value) logppx: 7.55977645541\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:00 INFO 139893340792640] #quality_metric: host=algo-1, epoch=36, train total_loss <loss>=7.55977645541\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:00 INFO 139893340792640] patience losses:[7.5643788682035176, 7.5633487216204651, 7.5608535918815081] min patience loss:7.56085359188 current loss:7.55977645541 absolute loss difference:0.00107713647125\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:00 INFO 139893340792640] #progress_metric: host=algo-1, completed 72 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 8712, \"sum\": 8712.0, \"min\": 8712}, \"Total Records Seen\": {\"count\": 1, \"max\": 8889048, \"sum\": 8889048.0, \"min\": 8889048}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 72, \"sum\": 72.0, \"min\": 72}}, \"EndTime\": 1528046520.169476, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 35}, \"StartTime\": 1528046515.728845}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:00 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=55601.5055434 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:00 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:00 INFO 139893340792640] # Starting training for epoch 37\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:04 INFO 139893340792640] # Finished training epoch 37 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:04 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:04 INFO 139893340792640] Loss (name: value) total: 7.55857551098\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:04 INFO 139893340792640] Loss (name: value) kld: 0.110370726063\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:04 INFO 139893340792640] Loss (name: value) recons: 7.44820480825\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:04 INFO 139893340792640] Loss (name: value) logppx: 7.55857551098\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:04 INFO 139893340792640] #quality_metric: host=algo-1, epoch=37, train total_loss <loss>=7.55857551098\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:04 INFO 139893340792640] patience losses:[7.5633487216204651, 7.5608535918815081, 7.5597764554102556] min patience loss:7.55977645541 current loss:7.55857551098 absolute loss difference:0.00120094443156\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:04 INFO 139893340792640] #progress_metric: host=algo-1, completed 74 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 8954, \"sum\": 8954.0, \"min\": 8954}, \"Total Records Seen\": {\"count\": 1, \"max\": 9135966, \"sum\": 9135966.0, \"min\": 9135966}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 74, \"sum\": 74.0, \"min\": 74}}, \"EndTime\": 1528046524.688548, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 36}, \"StartTime\": 1528046520.176811}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:04 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=54725.8984084 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:04 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:04 INFO 139893340792640] # Starting training for epoch 38\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[06/03/2018 17:22:09 INFO 139893340792640] # Finished training epoch 38 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:09 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:09 INFO 139893340792640] Loss (name: value) total: 7.55659563187\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:09 INFO 139893340792640] Loss (name: value) kld: 0.111448525143\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:09 INFO 139893340792640] Loss (name: value) recons: 7.4451471045\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:09 INFO 139893340792640] Loss (name: value) logppx: 7.55659563187\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:09 INFO 139893340792640] #quality_metric: host=algo-1, epoch=38, train total_loss <loss>=7.55659563187\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:09 INFO 139893340792640] patience losses:[7.5608535918815081, 7.5597764554102556, 7.5585755109786987] min patience loss:7.55857551098 current loss:7.55659563187 absolute loss difference:0.0019798791113\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:09 INFO 139893340792640] #progress_metric: host=algo-1, completed 76 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 9196, \"sum\": 9196.0, \"min\": 9196}, \"Total Records Seen\": {\"count\": 1, \"max\": 9382884, \"sum\": 9382884.0, \"min\": 9382884}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 76, \"sum\": 76.0, \"min\": 76}}, \"EndTime\": 1528046529.092729, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 37}, \"StartTime\": 1528046524.689532}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:09 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=56075.0080295 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:09 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:09 INFO 139893340792640] # Starting training for epoch 39\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:13 INFO 139893340792640] # Finished training epoch 39 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:13 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:13 INFO 139893340792640] Loss (name: value) total: 7.55565895965\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:13 INFO 139893340792640] Loss (name: value) kld: 0.112276104252\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:13 INFO 139893340792640] Loss (name: value) recons: 7.44338284076\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:13 INFO 139893340792640] Loss (name: value) logppx: 7.55565895965\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:13 INFO 139893340792640] #quality_metric: host=algo-1, epoch=39, train total_loss <loss>=7.55565895965\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:13 INFO 139893340792640] patience losses:[7.5597764554102556, 7.5585755109786987, 7.5565956318674008] min patience loss:7.55659563187 current loss:7.55565895965 absolute loss difference:0.000936672214634\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:13 INFO 139893340792640] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:13 INFO 139893340792640] #progress_metric: host=algo-1, completed 78 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 9438, \"sum\": 9438.0, \"min\": 9438}, \"Total Records Seen\": {\"count\": 1, \"max\": 9629802, \"sum\": 9629802.0, \"min\": 9629802}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 78, \"sum\": 78.0, \"min\": 78}}, \"EndTime\": 1528046533.489546, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 38}, \"StartTime\": 1528046529.100108}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:13 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=56250.6340563 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:13 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:13 INFO 139893340792640] # Starting training for epoch 40\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:17 INFO 139893340792640] # Finished training epoch 40 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:17 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:17 INFO 139893340792640] Loss (name: value) total: 7.55508867522\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:17 INFO 139893340792640] Loss (name: value) kld: 0.113300754389\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:17 INFO 139893340792640] Loss (name: value) recons: 7.4417878882\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:17 INFO 139893340792640] Loss (name: value) logppx: 7.55508867522\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:17 INFO 139893340792640] #quality_metric: host=algo-1, epoch=40, train total_loss <loss>=7.55508867522\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:17 INFO 139893340792640] patience losses:[7.5585755109786987, 7.5565956318674008, 7.5556589596527663] min patience loss:7.55565895965 current loss:7.55508867522 absolute loss difference:0.000570284433601\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:17 INFO 139893340792640] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:17 INFO 139893340792640] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 9680, \"sum\": 9680.0, \"min\": 9680}, \"Total Records Seen\": {\"count\": 1, \"max\": 9876720, \"sum\": 9876720.0, \"min\": 9876720}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 80, \"sum\": 80.0, \"min\": 80}}, \"EndTime\": 1528046537.922099, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 39}, \"StartTime\": 1528046533.489845}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:17 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=55707.5295067 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:17 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:17 INFO 139893340792640] # Starting training for epoch 41\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:22 INFO 139893340792640] # Finished training epoch 41 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:22 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:22 INFO 139893340792640] Loss (name: value) total: 7.55371197516\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:22 INFO 139893340792640] Loss (name: value) kld: 0.114320332208\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:22 INFO 139893340792640] Loss (name: value) recons: 7.43939164724\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:22 INFO 139893340792640] Loss (name: value) logppx: 7.55371197516\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:22 INFO 139893340792640] #quality_metric: host=algo-1, epoch=41, train total_loss <loss>=7.55371197516\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:22 INFO 139893340792640] patience losses:[7.5565956318674008, 7.5556589596527663, 7.5550886752191655] min patience loss:7.55508867522 current loss:7.55371197516 absolute loss difference:0.00137670005649\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:22 INFO 139893340792640] #progress_metric: host=algo-1, completed 82 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 9922, \"sum\": 9922.0, \"min\": 9922}, \"Total Records Seen\": {\"count\": 1, \"max\": 10123638, \"sum\": 10123638.0, \"min\": 10123638}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 82, \"sum\": 82.0, \"min\": 82}}, \"EndTime\": 1528046542.302858, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 40}, \"StartTime\": 1528046537.922423}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:22 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=56366.4325128 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:22 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:22 INFO 139893340792640] # Starting training for epoch 42\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:26 INFO 139893340792640] # Finished training epoch 42 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:26 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:26 INFO 139893340792640] Loss (name: value) total: 7.55325371273\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:26 INFO 139893340792640] Loss (name: value) kld: 0.114928423927\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:26 INFO 139893340792640] Loss (name: value) recons: 7.43832529084\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:26 INFO 139893340792640] Loss (name: value) logppx: 7.55325371273\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:26 INFO 139893340792640] #quality_metric: host=algo-1, epoch=42, train total_loss <loss>=7.55325371273\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:26 INFO 139893340792640] patience losses:[7.5556589596527663, 7.5550886752191655, 7.5537119751626793] min patience loss:7.55371197516 current loss:7.55325371273 absolute loss difference:0.000458262429749\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:26 INFO 139893340792640] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:26 INFO 139893340792640] #progress_metric: host=algo-1, completed 84 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 10164, \"sum\": 10164.0, \"min\": 10164}, \"Total Records Seen\": {\"count\": 1, \"max\": 10370556, \"sum\": 10370556.0, \"min\": 10370556}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 84, \"sum\": 84.0, \"min\": 84}}, \"EndTime\": 1528046546.778777, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 41}, \"StartTime\": 1528046542.303172}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:26 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=55167.9610531 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:26 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:26 INFO 139893340792640] # Starting training for epoch 43\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:31 INFO 139893340792640] # Finished training epoch 43 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:31 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:31 INFO 139893340792640] Loss (name: value) total: 7.55311420412\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:31 INFO 139893340792640] Loss (name: value) kld: 0.115696720983\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:31 INFO 139893340792640] Loss (name: value) recons: 7.43741746259\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:31 INFO 139893340792640] Loss (name: value) logppx: 7.55311420412\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:31 INFO 139893340792640] #quality_metric: host=algo-1, epoch=43, train total_loss <loss>=7.55311420412\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:31 INFO 139893340792640] patience losses:[7.5550886752191655, 7.5537119751626793, 7.5532537127329302] min patience loss:7.55325371273 current loss:7.55311420412 absolute loss difference:0.0001395086119\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:31 INFO 139893340792640] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:31 INFO 139893340792640] #progress_metric: host=algo-1, completed 86 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 10406, \"sum\": 10406.0, \"min\": 10406}, \"Total Records Seen\": {\"count\": 1, \"max\": 10617474, \"sum\": 10617474.0, \"min\": 10617474}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 86, \"sum\": 86.0, \"min\": 86}}, \"EndTime\": 1528046551.164157, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 42}, \"StartTime\": 1528046546.78604}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:31 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=56396.0155826 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:31 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:31 INFO 139893340792640] # Starting training for epoch 44\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[06/03/2018 17:22:35 INFO 139893340792640] # Finished training epoch 44 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:35 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:35 INFO 139893340792640] Loss (name: value) total: 7.55175155672\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:35 INFO 139893340792640] Loss (name: value) kld: 0.116382418247\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:35 INFO 139893340792640] Loss (name: value) recons: 7.43536913567\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:35 INFO 139893340792640] Loss (name: value) logppx: 7.55175155672\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:35 INFO 139893340792640] #quality_metric: host=algo-1, epoch=44, train total_loss <loss>=7.55175155672\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:35 INFO 139893340792640] patience losses:[7.5537119751626793, 7.5532537127329302, 7.5531142041210302] min patience loss:7.55311420412 current loss:7.55175155672 absolute loss difference:0.00136264739943\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:35 INFO 139893340792640] #progress_metric: host=algo-1, completed 88 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 10648, \"sum\": 10648.0, \"min\": 10648}, \"Total Records Seen\": {\"count\": 1, \"max\": 10864392, \"sum\": 10864392.0, \"min\": 10864392}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 88, \"sum\": 88.0, \"min\": 88}}, \"EndTime\": 1528046555.619501, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 43}, \"StartTime\": 1528046551.16455}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:35 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=55423.0125522 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:35 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:35 INFO 139893340792640] # Starting training for epoch 45\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:39 INFO 139893340792640] # Finished training epoch 45 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:39 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:39 INFO 139893340792640] Loss (name: value) total: 7.55124838933\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:39 INFO 139893340792640] Loss (name: value) kld: 0.117382870117\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:39 INFO 139893340792640] Loss (name: value) recons: 7.43386553142\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:39 INFO 139893340792640] Loss (name: value) logppx: 7.55124838933\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:39 INFO 139893340792640] #quality_metric: host=algo-1, epoch=45, train total_loss <loss>=7.55124838933\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:39 INFO 139893340792640] patience losses:[7.5532537127329302, 7.5531142041210302, 7.5517515567216007] min patience loss:7.55175155672 current loss:7.55124838933 absolute loss difference:0.000503167386882\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:39 INFO 139893340792640] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:39 INFO 139893340792640] #progress_metric: host=algo-1, completed 90 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 10890, \"sum\": 10890.0, \"min\": 10890}, \"Total Records Seen\": {\"count\": 1, \"max\": 11111310, \"sum\": 11111310.0, \"min\": 11111310}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 90, \"sum\": 90.0, \"min\": 90}}, \"EndTime\": 1528046559.975841, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 44}, \"StartTime\": 1528046555.61984}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:39 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=56682.6315755 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:39 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:39 INFO 139893340792640] # Starting training for epoch 46\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:44 INFO 139893340792640] # Finished training epoch 46 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:44 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:44 INFO 139893340792640] Loss (name: value) total: 7.55057305935\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:44 INFO 139893340792640] Loss (name: value) kld: 0.118203915417\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:44 INFO 139893340792640] Loss (name: value) recons: 7.4323691642\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:44 INFO 139893340792640] Loss (name: value) logppx: 7.55057305935\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:44 INFO 139893340792640] #quality_metric: host=algo-1, epoch=46, train total_loss <loss>=7.55057305935\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:44 INFO 139893340792640] patience losses:[7.5531142041210302, 7.5517515567216007, 7.5512483893347184] min patience loss:7.55124838933 current loss:7.55057305935 absolute loss difference:0.000675329984713\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:44 INFO 139893340792640] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:44 INFO 139893340792640] #progress_metric: host=algo-1, completed 92 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 11132, \"sum\": 11132.0, \"min\": 11132}, \"Total Records Seen\": {\"count\": 1, \"max\": 11358228, \"sum\": 11358228.0, \"min\": 11358228}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 92, \"sum\": 92.0, \"min\": 92}}, \"EndTime\": 1528046564.423262, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 45}, \"StartTime\": 1528046559.976202}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:44 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=55521.9261029 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:44 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:44 INFO 139893340792640] # Starting training for epoch 47\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:48 INFO 139893340792640] # Finished training epoch 47 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:48 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:48 INFO 139893340792640] Loss (name: value) total: 7.550057657\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:48 INFO 139893340792640] Loss (name: value) kld: 0.119322192162\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:48 INFO 139893340792640] Loss (name: value) recons: 7.43073546246\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:48 INFO 139893340792640] Loss (name: value) logppx: 7.550057657\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:48 INFO 139893340792640] #quality_metric: host=algo-1, epoch=47, train total_loss <loss>=7.550057657\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:48 INFO 139893340792640] patience losses:[7.5517515567216007, 7.5512483893347184, 7.5505730593500058] min patience loss:7.55057305935 current loss:7.550057657 absolute loss difference:0.000515402348573\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:48 INFO 139893340792640] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:48 INFO 139893340792640] #progress_metric: host=algo-1, completed 94 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 11374, \"sum\": 11374.0, \"min\": 11374}, \"Total Records Seen\": {\"count\": 1, \"max\": 11605146, \"sum\": 11605146.0, \"min\": 11605146}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 94, \"sum\": 94.0, \"min\": 94}}, \"EndTime\": 1528046568.908381, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 46}, \"StartTime\": 1528046564.43044}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:48 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=55138.7272701 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:48 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:48 INFO 139893340792640] # Starting training for epoch 48\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:53 INFO 139893340792640] # Finished training epoch 48 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:53 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:53 INFO 139893340792640] Loss (name: value) total: 7.54814541562\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:53 INFO 139893340792640] Loss (name: value) kld: 0.120601630423\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:53 INFO 139893340792640] Loss (name: value) recons: 7.4275437958\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:53 INFO 139893340792640] Loss (name: value) logppx: 7.54814541562\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:53 INFO 139893340792640] #quality_metric: host=algo-1, epoch=48, train total_loss <loss>=7.54814541562\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:53 INFO 139893340792640] patience losses:[7.5512483893347184, 7.5505730593500058, 7.5500576570014326] min patience loss:7.550057657 current loss:7.54814541562 absolute loss difference:0.00191224138599\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:53 INFO 139893340792640] #progress_metric: host=algo-1, completed 96 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 11616, \"sum\": 11616.0, \"min\": 11616}, \"Total Records Seen\": {\"count\": 1, \"max\": 11852064, \"sum\": 11852064.0, \"min\": 11852064}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 96, \"sum\": 96.0, \"min\": 96}}, \"EndTime\": 1528046573.319244, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 47}, \"StartTime\": 1528046568.918512}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:53 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=56106.1886031 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:53 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:53 INFO 139893340792640] # Starting training for epoch 49\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[06/03/2018 17:22:57 INFO 139893340792640] # Finished training epoch 49 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:57 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:57 INFO 139893340792640] Loss (name: value) total: 7.54704588233\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:57 INFO 139893340792640] Loss (name: value) kld: 0.122145494954\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:57 INFO 139893340792640] Loss (name: value) recons: 7.42490036773\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:57 INFO 139893340792640] Loss (name: value) logppx: 7.54704588233\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:57 INFO 139893340792640] #quality_metric: host=algo-1, epoch=49, train total_loss <loss>=7.54704588233\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:57 INFO 139893340792640] patience losses:[7.5505730593500058, 7.5500576570014326, 7.5481454156154442] min patience loss:7.54814541562 current loss:7.54704588233 absolute loss difference:0.00109953328598\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:57 INFO 139893340792640] #progress_metric: host=algo-1, completed 98 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 11858, \"sum\": 11858.0, \"min\": 11858}, \"Total Records Seen\": {\"count\": 1, \"max\": 12098982, \"sum\": 12098982.0, \"min\": 12098982}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 98, \"sum\": 98.0, \"min\": 98}}, \"EndTime\": 1528046577.74531, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 48}, \"StartTime\": 1528046573.319625}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:57 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=55790.0735268 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:57 INFO 139893340792640] \u001b[0m\n",
      "\u001b[31m[06/03/2018 17:22:57 INFO 139893340792640] # Starting training for epoch 50\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:23:02 INFO 139893340792640] # Finished training epoch 50 on 246918 examples from 242 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:23:02 INFO 139893340792640] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:23:02 INFO 139893340792640] Loss (name: value) total: 7.54512319437\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:23:02 INFO 139893340792640] Loss (name: value) kld: 0.124603470949\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:23:02 INFO 139893340792640] Loss (name: value) recons: 7.4205197251\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:23:02 INFO 139893340792640] Loss (name: value) logppx: 7.54512319437\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:23:02 INFO 139893340792640] #quality_metric: host=algo-1, epoch=50, train total_loss <loss>=7.54512319437\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:23:02 INFO 139893340792640] patience losses:[7.5500576570014326, 7.5481454156154442, 7.5470458823294679] min patience loss:7.54704588233 current loss:7.54512319437 absolute loss difference:0.00192268796204\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:23:02 INFO 139893340792640] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 242, \"sum\": 242.0, \"min\": 242}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 12100, \"sum\": 12100.0, \"min\": 12100}, \"Total Records Seen\": {\"count\": 1, \"max\": 12345900, \"sum\": 12345900.0, \"min\": 12345900}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 100, \"sum\": 100.0, \"min\": 100}}, \"EndTime\": 1528046582.171487, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 49}, \"StartTime\": 1528046577.745707}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:23:02 INFO 139893340792640] #throughput_metric: host=algo-1, train throughput=55788.4356359 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:23:02 WARNING 139893340792640] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:23:02 INFO 139893340792640] Best model based on early stopping at epoch 50. Best loss: 7.54512319437\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:23:02 INFO 139893340792640] Topics from epoch:final (num_topics:20):\u001b[0m\n",
      "\u001b[31m956 1338 798 4284 3282 3040 574 1014 2381 4321 4417 3655 2520 4989 512 2055 584 2112 4199 1040\u001b[0m\n",
      "\u001b[31m1988 1920 3224 4847 3139 3534 4214 4553 1392 2599 1110 2556 2064 2799 3708 3442 1062 1619 4713 971\u001b[0m\n",
      "\u001b[31m1282 3619 4761 3875 3362 1079 3020 899 906 3750 4772 2103 1775 1948 1673 3019 1891 1473 1536 882\u001b[0m\n",
      "\u001b[31m2112 4772 1896 1040 956 3282 4945 3433 4450 2821 4284 388 985 798 3970 742 1516 175 2649 4651\u001b[0m\n",
      "\u001b[31m2103 1536 4462 3019 709 1282 2136 906 4350 446 3020 899 3673 1079 3362 3996 706 3428 375 4814\u001b[0m\n",
      "\u001b[31m1988 3019 615 882 3139 3003 4509 3479 3316 3442 1473 1673 2221 3846 1239 222 3750 3708 2893 1167\u001b[0m\n",
      "\u001b[31m1282 2103 4772 3864 2821 1516 3020 3569 2136 3019 1079 3685 2649 906 1463 899 3745 1461 1673 3074\u001b[0m\n",
      "\u001b[31m2112 1282 1079 175 3020 4772 1694 3074 956 4566 709 2952 1896 388 4945 4608 1040 2024 4284 3282\u001b[0m\n",
      "\u001b[31m584 512 574 2328 1338 1892 3167 1714 1838 1414 4428 2263 1014 798 2968 4354 1655 3347 2447 4513\u001b[0m\n",
      "\u001b[31m3875 4730 1473 2893 4265 882 4814 4829 1500 3316 1891 3132 1846 4426 1103 2162 4509 2221 3663 428\u001b[0m\n",
      "\u001b[31m956 1896 4989 2649 388 175 4772 4595 798 3149 2112 2024 1445 985 4284 2520 1905 3970 4651 3020\u001b[0m\n",
      "\u001b[31m948 1580 4854 3725 2607 4700 1519 2460 1752 2072 27 1449 3215 333 80 1265 3187 2465 153 1722\u001b[0m\n",
      "\u001b[31m3316 1536 4282 1392 882 3019 3132 3052 1988 3846 1532 1282 1673 3003 4829 4129 615 4265 767 375\u001b[0m\n",
      "\u001b[31m3020 2024 4284 4276 3495 3504 2289 1040 734 2649 446 4989 985 956 175 2112 4884 2821 1097 1896\u001b[0m\n",
      "\u001b[31m1896 956 985 4595 388 3020 4989 906 734 3019 4884 899 175 3592 1097 2024 3714 2821 2520 2649\u001b[0m\n",
      "\u001b[31m2153 990 2860 359 1020 4783 208 665 1067 1079 4193 1660 762 113 562 4232 2766 485 3321 3697\u001b[0m\n",
      "\u001b[31m4772 956 1896 2112 985 175 4284 3020 3282 1282 2821 2337 1097 1040 4945 4608 562 1412 1694 3040\u001b[0m\n",
      "\u001b[31m3282 956 798 4417 2649 4595 4334 1896 4513 2112 1338 734 2381 3040 400 1097 4199 2814 4284 4030\u001b[0m\n",
      "\u001b[31m956 3282 2112 3040 4199 4411 798 4933 2055 1896 2520 1338 3242 2263 4334 985 2024 2381 388 4651\u001b[0m\n",
      "\u001b[31m3020 2952 906 3019 3619 1079 1694 4284 4919 4276 2103 446 1602 1282 175 985 734 4565 1445 3499\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:23:02 INFO 139893340792640] Serializing model to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:23:02 INFO 139893340792640] Saved checkpoint to \"/tmp/tmpOrJGSW/state-0001.params\"\u001b[0m\n",
      "\u001b[31m[06/03/2018 17:23:02 INFO 139893340792640] Test data is not provided.\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 230434.1390132904, \"sum\": 230434.1390132904, \"min\": 230434.1390132904}, \"finalize.time\": {\"count\": 1, \"max\": 156.77809715270996, \"sum\": 156.77809715270996, \"min\": 156.77809715270996}, \"initialize.time\": {\"count\": 1, \"max\": 9099.241018295288, \"sum\": 9099.241018295288, \"min\": 9099.241018295288}, \"model.serialize.time\": {\"count\": 1, \"max\": 8.133888244628906, \"sum\": 8.133888244628906, \"min\": 8.133888244628906}, \"setuptime\": {\"count\": 1, \"max\": 15.017032623291016, \"sum\": 15.017032623291016, \"min\": 15.017032623291016}, \"early_stop.time\": {\"count\": 50, \"max\": 28.90801429748535, \"sum\": 293.654203414917, \"min\": 1.0750293731689453}, \"update.time\": {\"count\": 50, \"max\": 4519.068002700806, \"sum\": 220933.09903144836, \"min\": 4345.808029174805}, \"epochs\": {\"count\": 1, \"max\": 50, \"sum\": 50.0, \"min\": 50}}, \"EndTime\": 1528046582.339157, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1528046351.943176}\n",
      "\u001b[0m\n",
      "===== Job Complete =====\n",
      "Billable seconds: 338\n"
     ]
    }
   ],
   "source": [
    "ntm.fit({'train': training_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "A trained model does nothing on its own. We now want to use the model to perform inference. For this example, that means predicting the topic mixture representing a given document or comment.\n",
    "\n",
    "This is simplified by the deploy function provided by the Amazon SageMaker Python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: ntm-2018-06-03-17-25-37-392\n",
      "INFO:sagemaker:Creating endpoint with name ntm-nyt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------"
     ]
    }
   ],
   "source": [
    "ntm_predictor = ntm.deploy(initial_instance_count=1,\n",
    "                           instance_type='ml.m4.xlarge',\n",
    "                          endpoint_name='ntm-nyt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Exploration \n",
    "\n",
    "This next section is based on [\"An Introduction to SageMaker Neural Topic Model\"](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_applying_machine_learning/ntm_20newsgroups_topic_modeling/ntm_20newsgroups_topic_model.ipynb).  While this section is not required for model deployment, it does offer some explaination of the topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mxnet \n",
    "import mxnet as mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join('data/nytimes-model/sagemaker-ntm', ntm._current_job_name, 'output/model.tar.gz')\n",
    "boto3.resource('s3').Bucket(bucket).download_file(model_path, 'downloaded_model.tar.gz')\n",
    "!tar -xzvf 'downloaded_model.tar.gz'\n",
    "!unzip -o model_algo-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mx.ndarray.load('params')\n",
    "W = model['arg:projection_weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud\n",
    "import wordcloud as wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "obj = s3.Object('sagemaker-us-east-1-228889150161','data/nyt-features/vocab.json')\n",
    "obj.download_file('vocab.json')\n",
    "\n",
    "def load_vocab():\n",
    "    with open('vocab.json', 'r') as json_file:\n",
    "        return json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "word_to_id = load_vocab()\n",
    "\n",
    "limit = 24\n",
    "n_col = 4\n",
    "counter = 0\n",
    "\n",
    "plt.figure(figsize=(20,16))\n",
    "for ind in range(num_topics):\n",
    "\n",
    "    if counter >= limit:\n",
    "        break\n",
    "\n",
    "    title_str = 'Topic{}'.format(ind)\n",
    "\n",
    "    #pvals = mx.nd.softmax(W[:, ind]).asnumpy()\n",
    "    pvals = mx.nd.softmax(mx.nd.array(W[:, ind])).asnumpy()\n",
    "\n",
    "    word_freq = dict()\n",
    "    for k in word_to_id.keys():\n",
    "        i = word_to_id[k]\n",
    "        word_freq[k] =pvals[i]\n",
    "\n",
    "    wordcloud = wc.WordCloud(background_color='white').fit_words(word_freq)\n",
    "\n",
    "    plt.subplot(limit // n_col, n_col, counter+1)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title_str)\n",
    "    #plt.close()\n",
    "\n",
    "    counter +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
