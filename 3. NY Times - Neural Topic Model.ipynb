{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "***\n",
    "\n",
    "Amazon SageMaker NTM (Neural Topic Model) is an unsupervised learning algorithm that attempts to describe a set of observations as a mixture of distinct categories. NTM is most commonly used to discover a user-specified number of topics shared by documents within a text corpus. Here each observation is a document, the features are the presence (or occurrence count) of each word, and the categories are the topics. Since the method is unsupervised, the topics are not specified up front, and are not guaranteed to align with how a human may naturally categorize documents. The topics are learned as a probability distribution over the words that occur in each document. Each document, in turn, is described as a mixture of topics.\n",
    "\n",
    "In this notebook we will use the Amazon SageMaker NTM algorithm to train a model on some example synthetic data. We will then use this model to classify (perform inference on) the data. The main goals of this notebook are to,\n",
    "\n",
    "* create an AWS SageMaker training job on a data set to produce a NTM model,\n",
    "* use the model to perform inference with an Amazon SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Created S3 bucket: sagemaker-us-east-1-228889150161\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Once the data is preprocessed and available in a recommended format the next step is to train our model on the data. There are number of parameters required by the NTM algorithm to configure the model and define the computational environment in which training will take place. The first of these is to point to a container image which holds the algorithms training and hosting code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "containers = {'us-west-2': '174872318107.dkr.ecr.us-west-2.amazonaws.com/ntm:latest',\n",
    "              'us-east-1': '382416733822.dkr.ecr.us-east-1.amazonaws.com/ntm:latest',\n",
    "              'us-east-2': '404615174143.dkr.ecr.us-east-2.amazonaws.com/ntm:latest',\n",
    "              'eu-west-1': '438346466558.dkr.ecr.eu-west-1.amazonaws.com/ntm:latest'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An NTM model uses the following hyperparameters:\n",
    "\n",
    "- **num_topics** - The number of topics or categories in the NTM model. \n",
    "- **feature_dim** - The size of the \"vocabulary\". In this case, this has been set to 1000 by the nytimes pyspark data prep.\n",
    "\n",
    "In addition to these NTM model hyperparameters, we provide additional parameters defining things like the EC2 instance type on which training will run, the S3 bucket containing the data, and the AWS access role.\n",
    "\n",
    "> Note: Try adjusting the mini_batch_size if running on a GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics=20\n",
    "vocabulary_size=1000\n",
    "output = 's3://{}/data/nytimes-model/sagemaker-ntm'.format(sagemaker_session.default_bucket())\n",
    "\n",
    "ntm = sagemaker.estimator.Estimator(containers[boto3.Session().region_name],\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.c4.xlarge',\n",
    "                                    output_path=output,\n",
    "                                    sagemaker_session=sagemaker_session)\n",
    "\n",
    "ntm.set_hyperparameters(num_topics=num_topics,\n",
    "                        feature_dim=vocabulary_size,\n",
    "                        mini_batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll train against the bag-of-words extracted from the NY Times comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3_client = boto3.client('s3')\n",
    "objects = s3_client.list_objects(Bucket=bucket, Prefix='data/nyt-record-io/training.rec')\n",
    "training_key = objects['Contents'][0]['Key']\n",
    "training_input = 's3://{}/{}'.format(bucket, training_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: ntm-2018-06-03-13-46-51-763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................\n",
      "\u001b[31mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:49:58 INFO 140519950444352] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/default-input.json: {u'num_patience_epochs': u'3', u'clip_gradient': u'Inf', u'encoder_layers': u'auto', u'optimizer': u'adadelta', u'_kvstore': u'auto', u'rescale_gradient': u'1.0', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'learning_rate': u'0.01', u'_data_format': u'record', u'epochs': u'50', u'weight_decay': u'0.0', u'_num_kv_servers': u'auto', u'encoder_layers_activation': u'sigmoid', u'mini_batch_size': u'256', u'tolerance': u'0.001', u'batch_norm': u'false'}\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:49:58 INFO 140519950444352] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'feature_dim': u'1000', u'mini_batch_size': u'256', u'num_topics': u'20'}\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:49:58 INFO 140519950444352] Final configuration: {u'num_patience_epochs': u'3', u'clip_gradient': u'Inf', u'encoder_layers': u'auto', u'optimizer': u'adadelta', u'_kvstore': u'auto', u'rescale_gradient': u'1.0', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'learning_rate': u'0.01', u'_data_format': u'record', u'epochs': u'50', u'feature_dim': u'1000', u'weight_decay': u'0.0', u'num_topics': u'20', u'_num_kv_servers': u'auto', u'encoder_layers_activation': u'sigmoid', u'mini_batch_size': u'256', u'tolerance': u'0.001', u'batch_norm': u'false'}\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:49:58 INFO 140519950444352] Using default worker.\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:49:58 INFO 140519950444352] Initializing\u001b[0m\n",
      "\u001b[31m/opt/amazon/lib/python2.7/site-packages/ai_algorithms_sdk/config/config_helper.py:122: DeprecationWarning: deprecated\n",
      "  warnings.warn(\"deprecated\", DeprecationWarning)\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:49:58 INFO 140519950444352] nvidia-smi took: 0.0251910686493 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:49:58 INFO 140519950444352] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:49:58 INFO 140519950444352] Create Store: local\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}}, \"EndTime\": 1528033798.277521, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1528033798.277483}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:49:58 INFO 140519950444352] \u001b[0m\n",
      "\u001b[31m[06/03/2018 13:49:58 INFO 140519950444352] # Starting training for epoch 1\u001b[0m\n",
      "\u001b[31m[13:49:58] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.200341.0/RHEL5_64/generic-flavor/src/src/ndarray/./../operator/tensor/.././../common/utils.h:416: \u001b[0m\n",
      "\u001b[31mStorage fallback detected:\u001b[0m\n",
      "\u001b[31mCopy from csr storage type on cpu to default storage type on cpu.\u001b[0m\n",
      "\u001b[31mA temporary ndarray with default storage type will be generated in order to perform the copy. You can set environment variable MXNET_STORAGE_FALLBACK_LOG_VERBOSE to 0 to suppress this warning.\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:50:16 INFO 140519950444352] # Finished training epoch 1 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:50:16 INFO 140519950444352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:50:16 INFO 140519950444352] Loss (name: value) total: 6.47842557541\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:50:16 INFO 140519950444352] Loss (name: value) kld: 0.016863260346\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:50:16 INFO 140519950444352] Loss (name: value) recons: 6.46156231035\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:50:16 INFO 140519950444352] Loss (name: value) logppx: 6.47842557541\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:50:16 INFO 140519950444352] #quality_metric: host=algo-1, epoch=1, train total_loss <loss>=6.47842557541\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:50:16 INFO 140519950444352] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Total Records Seen\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1528033816.967318, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 0}, \"StartTime\": 1528033798.283756}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:50:16 INFO 140519950444352] #throughput_metric: host=algo-1, train throughput=13215.6673497 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:50:16 INFO 140519950444352] \u001b[0m\n",
      "\u001b[31m[06/03/2018 13:50:16 INFO 140519950444352] # Starting training for epoch 2\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:50:35 INFO 140519950444352] # Finished training epoch 2 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:50:35 INFO 140519950444352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:50:35 INFO 140519950444352] Loss (name: value) total: 6.45633927232\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:50:35 INFO 140519950444352] Loss (name: value) kld: 0.0233360874689\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:50:35 INFO 140519950444352] Loss (name: value) recons: 6.43300319089\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:50:35 INFO 140519950444352] Loss (name: value) logppx: 6.45633927232\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:50:35 INFO 140519950444352] #quality_metric: host=algo-1, epoch=2, train total_loss <loss>=6.45633927232\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:50:35 INFO 140519950444352] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1930, \"sum\": 1930.0, \"min\": 1930}, \"Total Records Seen\": {\"count\": 1, \"max\": 493836, \"sum\": 493836.0, \"min\": 493836}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 4, \"sum\": 4.0, \"min\": 4}}, \"EndTime\": 1528033835.61498, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 1}, \"StartTime\": 1528033816.971472}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:50:35 INFO 140519950444352] #throughput_metric: host=algo-1, train throughput=13244.0739492 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:50:35 INFO 140519950444352] \u001b[0m\n",
      "\u001b[31m[06/03/2018 13:50:35 INFO 140519950444352] # Starting training for epoch 3\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:50:54 INFO 140519950444352] # Finished training epoch 3 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:50:54 INFO 140519950444352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:50:54 INFO 140519950444352] Loss (name: value) total: 6.44584073427\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:50:54 INFO 140519950444352] Loss (name: value) kld: 0.033955251715\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:50:54 INFO 140519950444352] Loss (name: value) recons: 6.41188548513\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:50:54 INFO 140519950444352] Loss (name: value) logppx: 6.44584073427\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:50:54 INFO 140519950444352] #quality_metric: host=algo-1, epoch=3, train total_loss <loss>=6.44584073427\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:50:54 INFO 140519950444352] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2895, \"sum\": 2895.0, \"min\": 2895}, \"Total Records Seen\": {\"count\": 1, \"max\": 740754, \"sum\": 740754.0, \"min\": 740754}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 6, \"sum\": 6.0, \"min\": 6}}, \"EndTime\": 1528033854.231373, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 2}, \"StartTime\": 1528033835.615363}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:50:54 INFO 140519950444352] #throughput_metric: host=algo-1, train throughput=13263.6020014 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:50:54 INFO 140519950444352] \u001b[0m\n",
      "\u001b[31m[06/03/2018 13:50:54 INFO 140519950444352] # Starting training for epoch 4\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[06/03/2018 13:51:12 INFO 140519950444352] # Finished training epoch 4 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:51:12 INFO 140519950444352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:51:12 INFO 140519950444352] Loss (name: value) total: 6.43215225521\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:51:12 INFO 140519950444352] Loss (name: value) kld: 0.0502889504685\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:51:12 INFO 140519950444352] Loss (name: value) recons: 6.38186329387\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:51:12 INFO 140519950444352] Loss (name: value) logppx: 6.43215225521\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:51:12 INFO 140519950444352] #quality_metric: host=algo-1, epoch=4, train total_loss <loss>=6.43215225521\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:51:12 INFO 140519950444352] patience losses:[6.4784255754144695, 6.4563392723162556, 6.4458407342742765] min patience loss:6.44584073427 current loss:6.43215225521 absolute loss difference:0.0136884790628\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:51:12 INFO 140519950444352] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3860, \"sum\": 3860.0, \"min\": 3860}, \"Total Records Seen\": {\"count\": 1, \"max\": 987672, \"sum\": 987672.0, \"min\": 987672}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}}, \"EndTime\": 1528033872.70763, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 3}, \"StartTime\": 1528033854.235483}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:51:12 INFO 140519950444352] #throughput_metric: host=algo-1, train throughput=13366.9331538 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:51:12 INFO 140519950444352] \u001b[0m\n",
      "\u001b[31m[06/03/2018 13:51:12 INFO 140519950444352] # Starting training for epoch 5\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:51:31 INFO 140519950444352] # Finished training epoch 5 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:51:31 INFO 140519950444352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:51:31 INFO 140519950444352] Loss (name: value) total: 6.42514469438\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:51:31 INFO 140519950444352] Loss (name: value) kld: 0.0599974445709\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:51:31 INFO 140519950444352] Loss (name: value) recons: 6.36514725364\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:51:31 INFO 140519950444352] Loss (name: value) logppx: 6.42514469438\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:51:31 INFO 140519950444352] #quality_metric: host=algo-1, epoch=5, train total_loss <loss>=6.42514469438\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:51:31 INFO 140519950444352] patience losses:[6.4563392723162556, 6.4458407342742765, 6.4321522552114692] min patience loss:6.43215225521 current loss:6.42514469438 absolute loss difference:0.00700756082881\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:51:31 INFO 140519950444352] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 4825, \"sum\": 4825.0, \"min\": 4825}, \"Total Records Seen\": {\"count\": 1, \"max\": 1234590, \"sum\": 1234590.0, \"min\": 1234590}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}}, \"EndTime\": 1528033891.12152, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 4}, \"StartTime\": 1528033872.708016}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:51:31 INFO 140519950444352] #throughput_metric: host=algo-1, train throughput=13409.4489539 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:51:31 INFO 140519950444352] \u001b[0m\n",
      "\u001b[31m[06/03/2018 13:51:31 INFO 140519950444352] # Starting training for epoch 6\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:51:49 INFO 140519950444352] # Finished training epoch 6 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:51:49 INFO 140519950444352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:51:49 INFO 140519950444352] Loss (name: value) total: 6.41731798439\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:51:49 INFO 140519950444352] Loss (name: value) kld: 0.0691370298092\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:51:49 INFO 140519950444352] Loss (name: value) recons: 6.34818095815\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:51:49 INFO 140519950444352] Loss (name: value) logppx: 6.41731798439\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:51:49 INFO 140519950444352] #quality_metric: host=algo-1, epoch=6, train total_loss <loss>=6.41731798439\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:51:49 INFO 140519950444352] patience losses:[6.4458407342742765, 6.4321522552114692, 6.4251446943826629] min patience loss:6.42514469438 current loss:6.41731798439 absolute loss difference:0.00782670999438\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:51:49 INFO 140519950444352] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 5790, \"sum\": 5790.0, \"min\": 5790}, \"Total Records Seen\": {\"count\": 1, \"max\": 1481508, \"sum\": 1481508.0, \"min\": 1481508}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 12, \"sum\": 12.0, \"min\": 12}}, \"EndTime\": 1528033909.641668, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 5}, \"StartTime\": 1528033891.123448}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:51:49 INFO 140519950444352] #throughput_metric: host=algo-1, train throughput=13333.671102 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:51:49 INFO 140519950444352] \u001b[0m\n",
      "\u001b[31m[06/03/2018 13:51:49 INFO 140519950444352] # Starting training for epoch 7\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:52:08 INFO 140519950444352] # Finished training epoch 7 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:52:08 INFO 140519950444352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:52:08 INFO 140519950444352] Loss (name: value) total: 6.41033125961\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:52:08 INFO 140519950444352] Loss (name: value) kld: 0.0768526902278\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:52:08 INFO 140519950444352] Loss (name: value) recons: 6.33347856195\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:52:08 INFO 140519950444352] Loss (name: value) logppx: 6.41033125961\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:52:08 INFO 140519950444352] #quality_metric: host=algo-1, epoch=7, train total_loss <loss>=6.41033125961\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:52:08 INFO 140519950444352] patience losses:[6.4321522552114692, 6.4251446943826629, 6.4173179843882826] min patience loss:6.41731798439 current loss:6.41033125961 absolute loss difference:0.00698672477446\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:52:08 INFO 140519950444352] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 6755, \"sum\": 6755.0, \"min\": 6755}, \"Total Records Seen\": {\"count\": 1, \"max\": 1728426, \"sum\": 1728426.0, \"min\": 1728426}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 14, \"sum\": 14.0, \"min\": 14}}, \"EndTime\": 1528033928.056787, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 6}, \"StartTime\": 1528033909.641986}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:52:08 INFO 140519950444352] #throughput_metric: host=algo-1, train throughput=13408.5628367 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:52:08 INFO 140519950444352] \u001b[0m\n",
      "\u001b[31m[06/03/2018 13:52:08 INFO 140519950444352] # Starting training for epoch 8\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[06/03/2018 13:52:26 INFO 140519950444352] # Finished training epoch 8 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:52:26 INFO 140519950444352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:52:26 INFO 140519950444352] Loss (name: value) total: 6.40626482865\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:52:26 INFO 140519950444352] Loss (name: value) kld: 0.0818534008773\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:52:26 INFO 140519950444352] Loss (name: value) recons: 6.32441143174\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:52:26 INFO 140519950444352] Loss (name: value) logppx: 6.40626482865\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:52:26 INFO 140519950444352] #quality_metric: host=algo-1, epoch=8, train total_loss <loss>=6.40626482865\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:52:26 INFO 140519950444352] patience losses:[6.4251446943826629, 6.4173179843882826, 6.4103312596138275] min patience loss:6.41033125961 current loss:6.40626482865 absolute loss difference:0.00406643096647\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:52:26 INFO 140519950444352] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 7720, \"sum\": 7720.0, \"min\": 7720}, \"Total Records Seen\": {\"count\": 1, \"max\": 1975344, \"sum\": 1975344.0, \"min\": 1975344}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 16, \"sum\": 16.0, \"min\": 16}}, \"EndTime\": 1528033946.397947, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 7}, \"StartTime\": 1528033928.057109}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:52:26 INFO 140519950444352] #throughput_metric: host=algo-1, train throughput=13462.6299119 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:52:26 INFO 140519950444352] \u001b[0m\n",
      "\u001b[31m[06/03/2018 13:52:26 INFO 140519950444352] # Starting training for epoch 9\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:52:44 INFO 140519950444352] # Finished training epoch 9 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:52:44 INFO 140519950444352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:52:44 INFO 140519950444352] Loss (name: value) total: 6.4026931103\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:52:44 INFO 140519950444352] Loss (name: value) kld: 0.0859225561891\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:52:44 INFO 140519950444352] Loss (name: value) recons: 6.31677055062\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:52:44 INFO 140519950444352] Loss (name: value) logppx: 6.4026931103\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:52:44 INFO 140519950444352] #quality_metric: host=algo-1, epoch=9, train total_loss <loss>=6.4026931103\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:52:44 INFO 140519950444352] patience losses:[6.4173179843882826, 6.4103312596138275, 6.4062648286473562] min patience loss:6.40626482865 current loss:6.4026931103 absolute loss difference:0.00357171834442\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:52:44 INFO 140519950444352] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 8685, \"sum\": 8685.0, \"min\": 8685}, \"Total Records Seen\": {\"count\": 1, \"max\": 2222262, \"sum\": 2222262.0, \"min\": 2222262}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 18, \"sum\": 18.0, \"min\": 18}}, \"EndTime\": 1528033964.870677, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 8}, \"StartTime\": 1528033946.398246}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:52:44 INFO 140519950444352] #throughput_metric: host=algo-1, train throughput=13366.7378588 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:52:44 INFO 140519950444352] \u001b[0m\n",
      "\u001b[31m[06/03/2018 13:52:44 INFO 140519950444352] # Starting training for epoch 10\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:53:03 INFO 140519950444352] # Finished training epoch 10 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:53:03 INFO 140519950444352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:53:03 INFO 140519950444352] Loss (name: value) total: 6.39886213189\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:53:03 INFO 140519950444352] Loss (name: value) kld: 0.0906406539092\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:53:03 INFO 140519950444352] Loss (name: value) recons: 6.30822147829\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:53:03 INFO 140519950444352] Loss (name: value) logppx: 6.39886213189\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:53:03 INFO 140519950444352] #quality_metric: host=algo-1, epoch=10, train total_loss <loss>=6.39886213189\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:53:03 INFO 140519950444352] patience losses:[6.4103312596138275, 6.4062648286473562, 6.4026931103029403] min patience loss:6.4026931103 current loss:6.39886213189 absolute loss difference:0.00383097841332\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:53:03 INFO 140519950444352] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 9650, \"sum\": 9650.0, \"min\": 9650}, \"Total Records Seen\": {\"count\": 1, \"max\": 2469180, \"sum\": 2469180.0, \"min\": 2469180}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 20, \"sum\": 20.0, \"min\": 20}}, \"EndTime\": 1528033983.315801, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 9}, \"StartTime\": 1528033964.87097}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:53:03 INFO 140519950444352] #throughput_metric: host=algo-1, train throughput=13386.7382533 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:53:03 INFO 140519950444352] \u001b[0m\n",
      "\u001b[31m[06/03/2018 13:53:03 INFO 140519950444352] # Starting training for epoch 11\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:53:21 INFO 140519950444352] # Finished training epoch 11 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:53:21 INFO 140519950444352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:53:21 INFO 140519950444352] Loss (name: value) total: 6.39379471522\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:53:21 INFO 140519950444352] Loss (name: value) kld: 0.0984249478718\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:53:21 INFO 140519950444352] Loss (name: value) recons: 6.2953697595\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:53:21 INFO 140519950444352] Loss (name: value) logppx: 6.39379471522\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:53:21 INFO 140519950444352] #quality_metric: host=algo-1, epoch=11, train total_loss <loss>=6.39379471522\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:53:21 INFO 140519950444352] patience losses:[6.4062648286473562, 6.4026931103029403, 6.3988621318896204] min patience loss:6.39886213189 current loss:6.39379471522 absolute loss difference:0.00506741667041\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:53:21 INFO 140519950444352] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 10615, \"sum\": 10615.0, \"min\": 10615}, \"Total Records Seen\": {\"count\": 1, \"max\": 2716098, \"sum\": 2716098.0, \"min\": 2716098}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 22, \"sum\": 22.0, \"min\": 22}}, \"EndTime\": 1528034001.808561, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 10}, \"StartTime\": 1528033983.316154}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:53:21 INFO 140519950444352] #throughput_metric: host=algo-1, train throughput=13352.2944502 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:53:21 INFO 140519950444352] \u001b[0m\n",
      "\u001b[31m[06/03/2018 13:53:21 INFO 140519950444352] # Starting training for epoch 12\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[06/03/2018 13:53:40 INFO 140519950444352] # Finished training epoch 12 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:53:40 INFO 140519950444352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:53:40 INFO 140519950444352] Loss (name: value) total: 6.3874256554\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:53:40 INFO 140519950444352] Loss (name: value) kld: 0.105909884999\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:53:40 INFO 140519950444352] Loss (name: value) recons: 6.28151576778\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:53:40 INFO 140519950444352] Loss (name: value) logppx: 6.3874256554\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:53:40 INFO 140519950444352] #quality_metric: host=algo-1, epoch=12, train total_loss <loss>=6.3874256554\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:53:40 INFO 140519950444352] patience losses:[6.4026931103029403, 6.3988621318896204, 6.3937947152192107] min patience loss:6.39379471522 current loss:6.3874256554 absolute loss difference:0.00636905981469\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:53:40 INFO 140519950444352] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 11580, \"sum\": 11580.0, \"min\": 11580}, \"Total Records Seen\": {\"count\": 1, \"max\": 2963016, \"sum\": 2963016.0, \"min\": 2963016}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 24, \"sum\": 24.0, \"min\": 24}}, \"EndTime\": 1528034020.226951, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 11}, \"StartTime\": 1528034001.811496}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:53:40 INFO 140519950444352] #throughput_metric: host=algo-1, train throughput=13407.4954501 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:53:40 INFO 140519950444352] \u001b[0m\n",
      "\u001b[31m[06/03/2018 13:53:40 INFO 140519950444352] # Starting training for epoch 13\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:53:58 INFO 140519950444352] # Finished training epoch 13 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:53:58 INFO 140519950444352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:53:58 INFO 140519950444352] Loss (name: value) total: 6.38411743307\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:53:58 INFO 140519950444352] Loss (name: value) kld: 0.111718005795\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:53:58 INFO 140519950444352] Loss (name: value) recons: 6.27239942551\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:53:58 INFO 140519950444352] Loss (name: value) logppx: 6.38411743307\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:53:58 INFO 140519950444352] #quality_metric: host=algo-1, epoch=13, train total_loss <loss>=6.38411743307\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:53:58 INFO 140519950444352] patience losses:[6.3988621318896204, 6.3937947152192107, 6.3874256554045203] min patience loss:6.3874256554 current loss:6.38411743307 absolute loss difference:0.00330822233091\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:53:58 INFO 140519950444352] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 12545, \"sum\": 12545.0, \"min\": 12545}, \"Total Records Seen\": {\"count\": 1, \"max\": 3209934, \"sum\": 3209934.0, \"min\": 3209934}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 26, \"sum\": 26.0, \"min\": 26}}, \"EndTime\": 1528034038.575084, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 12}, \"StartTime\": 1528034020.231517}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:53:58 INFO 140519950444352] #throughput_metric: host=algo-1, train throughput=13460.6190701 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:53:58 INFO 140519950444352] \u001b[0m\n",
      "\u001b[31m[06/03/2018 13:53:58 INFO 140519950444352] # Starting training for epoch 14\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:54:16 INFO 140519950444352] # Finished training epoch 14 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:54:16 INFO 140519950444352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:54:16 INFO 140519950444352] Loss (name: value) total: 6.38138470106\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:54:16 INFO 140519950444352] Loss (name: value) kld: 0.116495107693\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:54:16 INFO 140519950444352] Loss (name: value) recons: 6.26488959728\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:54:16 INFO 140519950444352] Loss (name: value) logppx: 6.38138470106\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:54:16 INFO 140519950444352] #quality_metric: host=algo-1, epoch=14, train total_loss <loss>=6.38138470106\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:54:16 INFO 140519950444352] patience losses:[6.3937947152192107, 6.3874256554045203, 6.384117433073607] min patience loss:6.38411743307 current loss:6.38138470106 absolute loss difference:0.00273273201186\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:54:16 INFO 140519950444352] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 13510, \"sum\": 13510.0, \"min\": 13510}, \"Total Records Seen\": {\"count\": 1, \"max\": 3456852, \"sum\": 3456852.0, \"min\": 3456852}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 28, \"sum\": 28.0, \"min\": 28}}, \"EndTime\": 1528034056.971076, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 13}, \"StartTime\": 1528034038.57541}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:54:16 INFO 140519950444352] #throughput_metric: host=algo-1, train throughput=13422.4999214 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:54:16 INFO 140519950444352] \u001b[0m\n",
      "\u001b[31m[06/03/2018 13:54:16 INFO 140519950444352] # Starting training for epoch 15\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:54:35 INFO 140519950444352] # Finished training epoch 15 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:54:35 INFO 140519950444352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:54:35 INFO 140519950444352] Loss (name: value) total: 6.37778967477\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:54:35 INFO 140519950444352] Loss (name: value) kld: 0.122226034425\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:54:35 INFO 140519950444352] Loss (name: value) recons: 6.25556363121\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:54:35 INFO 140519950444352] Loss (name: value) logppx: 6.37778967477\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:54:35 INFO 140519950444352] #quality_metric: host=algo-1, epoch=15, train total_loss <loss>=6.37778967477\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:54:35 INFO 140519950444352] patience losses:[6.3874256554045203, 6.384117433073607, 6.3813847010617426] min patience loss:6.38138470106 current loss:6.37778967477 absolute loss difference:0.00359502629295\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:54:35 INFO 140519950444352] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 14475, \"sum\": 14475.0, \"min\": 14475}, \"Total Records Seen\": {\"count\": 1, \"max\": 3703770, \"sum\": 3703770.0, \"min\": 3703770}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 30, \"sum\": 30.0, \"min\": 30}}, \"EndTime\": 1528034075.38801, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 14}, \"StartTime\": 1528034056.971389}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:54:35 INFO 140519950444352] #throughput_metric: host=algo-1, train throughput=13407.2427323 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:54:35 INFO 140519950444352] \u001b[0m\n",
      "\u001b[31m[06/03/2018 13:54:35 INFO 140519950444352] # Starting training for epoch 16\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[06/03/2018 13:54:53 INFO 140519950444352] # Finished training epoch 16 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:54:53 INFO 140519950444352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:54:53 INFO 140519950444352] Loss (name: value) total: 6.37473626532\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:54:53 INFO 140519950444352] Loss (name: value) kld: 0.127532463857\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:54:53 INFO 140519950444352] Loss (name: value) recons: 6.24720378984\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:54:53 INFO 140519950444352] Loss (name: value) logppx: 6.37473626532\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:54:53 INFO 140519950444352] #quality_metric: host=algo-1, epoch=16, train total_loss <loss>=6.37473626532\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:54:53 INFO 140519950444352] patience losses:[6.384117433073607, 6.3813847010617426, 6.3777896747687937] min patience loss:6.37778967477 current loss:6.37473626532 absolute loss difference:0.00305340944794\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:54:53 INFO 140519950444352] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 15440, \"sum\": 15440.0, \"min\": 15440}, \"Total Records Seen\": {\"count\": 1, \"max\": 3950688, \"sum\": 3950688.0, \"min\": 3950688}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 32, \"sum\": 32.0, \"min\": 32}}, \"EndTime\": 1528034093.764564, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 15}, \"StartTime\": 1528034075.388368}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:54:53 INFO 140519950444352] #throughput_metric: host=algo-1, train throughput=13436.6928086 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:54:53 INFO 140519950444352] \u001b[0m\n",
      "\u001b[31m[06/03/2018 13:54:53 INFO 140519950444352] # Starting training for epoch 17\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:55:12 INFO 140519950444352] # Finished training epoch 17 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:55:12 INFO 140519950444352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:55:12 INFO 140519950444352] Loss (name: value) total: 6.37284097202\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:55:12 INFO 140519950444352] Loss (name: value) kld: 0.130872206251\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:55:12 INFO 140519950444352] Loss (name: value) recons: 6.24196876343\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:55:12 INFO 140519950444352] Loss (name: value) logppx: 6.37284097202\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:55:12 INFO 140519950444352] #quality_metric: host=algo-1, epoch=17, train total_loss <loss>=6.37284097202\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:55:12 INFO 140519950444352] patience losses:[6.3813847010617426, 6.3777896747687937, 6.3747362653208519] min patience loss:6.37473626532 current loss:6.37284097202 absolute loss difference:0.00189529330002\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:55:12 INFO 140519950444352] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 16405, \"sum\": 16405.0, \"min\": 16405}, \"Total Records Seen\": {\"count\": 1, \"max\": 4197606, \"sum\": 4197606.0, \"min\": 4197606}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 34, \"sum\": 34.0, \"min\": 34}}, \"EndTime\": 1528034112.30497, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 16}, \"StartTime\": 1528034093.764924}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:55:12 INFO 140519950444352] #throughput_metric: host=algo-1, train throughput=13317.9592756 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:55:12 INFO 140519950444352] \u001b[0m\n",
      "\u001b[31m[06/03/2018 13:55:12 INFO 140519950444352] # Starting training for epoch 18\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:55:30 INFO 140519950444352] # Finished training epoch 18 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:55:30 INFO 140519950444352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:55:30 INFO 140519950444352] Loss (name: value) total: 6.37044192966\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:55:30 INFO 140519950444352] Loss (name: value) kld: 0.134170647128\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:55:30 INFO 140519950444352] Loss (name: value) recons: 6.23627128576\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:55:30 INFO 140519950444352] Loss (name: value) logppx: 6.37044192966\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:55:30 INFO 140519950444352] #quality_metric: host=algo-1, epoch=18, train total_loss <loss>=6.37044192966\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:55:30 INFO 140519950444352] patience losses:[6.3777896747687937, 6.3747362653208519, 6.3728409720208363] min patience loss:6.37284097202 current loss:6.37044192966 absolute loss difference:0.00239904235682\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:55:30 INFO 140519950444352] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 17370, \"sum\": 17370.0, \"min\": 17370}, \"Total Records Seen\": {\"count\": 1, \"max\": 4444524, \"sum\": 4444524.0, \"min\": 4444524}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}}, \"EndTime\": 1528034130.889942, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 17}, \"StartTime\": 1528034112.305282}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:55:30 INFO 140519950444352] #throughput_metric: host=algo-1, train throughput=13285.9978845 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:55:30 INFO 140519950444352] \u001b[0m\n",
      "\u001b[31m[06/03/2018 13:55:30 INFO 140519950444352] # Starting training for epoch 19\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:55:49 INFO 140519950444352] # Finished training epoch 19 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:55:49 INFO 140519950444352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:55:49 INFO 140519950444352] Loss (name: value) total: 6.36932412182\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:55:49 INFO 140519950444352] Loss (name: value) kld: 0.136458242148\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:55:49 INFO 140519950444352] Loss (name: value) recons: 6.23286588106\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:55:49 INFO 140519950444352] Loss (name: value) logppx: 6.36932412182\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:55:49 INFO 140519950444352] #quality_metric: host=algo-1, epoch=19, train total_loss <loss>=6.36932412182\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:55:49 INFO 140519950444352] patience losses:[6.3747362653208519, 6.3728409720208363, 6.3704419296640191] min patience loss:6.37044192966 current loss:6.36932412182 absolute loss difference:0.00111780784291\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:55:49 INFO 140519950444352] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 18335, \"sum\": 18335.0, \"min\": 18335}, \"Total Records Seen\": {\"count\": 1, \"max\": 4691442, \"sum\": 4691442.0, \"min\": 4691442}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 38, \"sum\": 38.0, \"min\": 38}}, \"EndTime\": 1528034149.456957, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 18}, \"StartTime\": 1528034130.896522}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:55:49 INFO 140519950444352] #throughput_metric: host=algo-1, train throughput=13303.3387223 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:55:49 INFO 140519950444352] \u001b[0m\n",
      "\u001b[31m[06/03/2018 13:55:49 INFO 140519950444352] # Starting training for epoch 20\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[06/03/2018 13:56:07 INFO 140519950444352] # Finished training epoch 20 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:56:07 INFO 140519950444352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:56:07 INFO 140519950444352] Loss (name: value) total: 6.36803244457\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:56:07 INFO 140519950444352] Loss (name: value) kld: 0.137846018135\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:56:07 INFO 140519950444352] Loss (name: value) recons: 6.23018641818\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:56:07 INFO 140519950444352] Loss (name: value) logppx: 6.36803244457\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:56:07 INFO 140519950444352] #quality_metric: host=algo-1, epoch=20, train total_loss <loss>=6.36803244457\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:56:07 INFO 140519950444352] patience losses:[6.3728409720208363, 6.3704419296640191, 6.3693241218211121] min patience loss:6.36932412182 current loss:6.36803244457 absolute loss difference:0.00129167724767\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:56:07 INFO 140519950444352] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 19300, \"sum\": 19300.0, \"min\": 19300}, \"Total Records Seen\": {\"count\": 1, \"max\": 4938360, \"sum\": 4938360.0, \"min\": 4938360}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 40, \"sum\": 40.0, \"min\": 40}}, \"EndTime\": 1528034167.867186, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 19}, \"StartTime\": 1528034149.45733}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:56:07 INFO 140519950444352] #throughput_metric: host=algo-1, train throughput=13412.1724487 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:56:07 INFO 140519950444352] \u001b[0m\n",
      "\u001b[31m[06/03/2018 13:56:07 INFO 140519950444352] # Starting training for epoch 21\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:56:26 INFO 140519950444352] # Finished training epoch 21 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:56:26 INFO 140519950444352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:56:26 INFO 140519950444352] Loss (name: value) total: 6.36699019985\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:56:26 INFO 140519950444352] Loss (name: value) kld: 0.139684631371\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:56:26 INFO 140519950444352] Loss (name: value) recons: 6.22730556621\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:56:26 INFO 140519950444352] Loss (name: value) logppx: 6.36699019985\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:56:26 INFO 140519950444352] #quality_metric: host=algo-1, epoch=21, train total_loss <loss>=6.36699019985\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:56:26 INFO 140519950444352] patience losses:[6.3704419296640191, 6.3693241218211121, 6.3680324445734371] min patience loss:6.36803244457 current loss:6.36699019985 absolute loss difference:0.00104224471848\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:56:26 INFO 140519950444352] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 20265, \"sum\": 20265.0, \"min\": 20265}, \"Total Records Seen\": {\"count\": 1, \"max\": 5185278, \"sum\": 5185278.0, \"min\": 5185278}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 42, \"sum\": 42.0, \"min\": 42}}, \"EndTime\": 1528034186.402978, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 20}, \"StartTime\": 1528034167.867498}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:56:26 INFO 140519950444352] #throughput_metric: host=algo-1, train throughput=13321.2709488 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:56:26 INFO 140519950444352] \u001b[0m\n",
      "\u001b[31m[06/03/2018 13:56:26 INFO 140519950444352] # Starting training for epoch 22\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:56:44 INFO 140519950444352] # Finished training epoch 22 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:56:44 INFO 140519950444352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:56:44 INFO 140519950444352] Loss (name: value) total: 6.36507054561\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:56:44 INFO 140519950444352] Loss (name: value) kld: 0.142238939951\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:56:44 INFO 140519950444352] Loss (name: value) recons: 6.22283160748\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:56:44 INFO 140519950444352] Loss (name: value) logppx: 6.36507054561\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:56:44 INFO 140519950444352] #quality_metric: host=algo-1, epoch=22, train total_loss <loss>=6.36507054561\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:56:44 INFO 140519950444352] patience losses:[6.3693241218211121, 6.3680324445734371, 6.3669901998549543] min patience loss:6.36699019985 current loss:6.36507054561 absolute loss difference:0.00191965424335\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:56:44 INFO 140519950444352] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 21230, \"sum\": 21230.0, \"min\": 21230}, \"Total Records Seen\": {\"count\": 1, \"max\": 5432196, \"sum\": 5432196.0, \"min\": 5432196}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 44, \"sum\": 44.0, \"min\": 44}}, \"EndTime\": 1528034204.816198, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 21}, \"StartTime\": 1528034186.403278}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:56:44 INFO 140519950444352] #throughput_metric: host=algo-1, train throughput=13409.9450154 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:56:44 INFO 140519950444352] \u001b[0m\n",
      "\u001b[31m[06/03/2018 13:56:44 INFO 140519950444352] # Starting training for epoch 23\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:57:03 INFO 140519950444352] # Finished training epoch 23 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:57:03 INFO 140519950444352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:57:03 INFO 140519950444352] Loss (name: value) total: 6.36380197001\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:57:03 INFO 140519950444352] Loss (name: value) kld: 0.146093621194\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:57:03 INFO 140519950444352] Loss (name: value) recons: 6.21770835269\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:57:03 INFO 140519950444352] Loss (name: value) logppx: 6.36380197001\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:57:03 INFO 140519950444352] #quality_metric: host=algo-1, epoch=23, train total_loss <loss>=6.36380197001\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:57:03 INFO 140519950444352] patience losses:[6.3680324445734371, 6.3669901998549543, 6.3650705456116041] min patience loss:6.36507054561 current loss:6.36380197001 absolute loss difference:0.00126857559916\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:57:03 INFO 140519950444352] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 22195, \"sum\": 22195.0, \"min\": 22195}, \"Total Records Seen\": {\"count\": 1, \"max\": 5679114, \"sum\": 5679114.0, \"min\": 5679114}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 46, \"sum\": 46.0, \"min\": 46}}, \"EndTime\": 1528034223.279719, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 22}, \"StartTime\": 1528034204.816517}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:57:03 INFO 140519950444352] #throughput_metric: host=algo-1, train throughput=13373.4163187 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:57:03 INFO 140519950444352] \u001b[0m\n",
      "\u001b[31m[06/03/2018 13:57:03 INFO 140519950444352] # Starting training for epoch 24\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[06/03/2018 13:57:21 INFO 140519950444352] # Finished training epoch 24 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:57:21 INFO 140519950444352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:57:21 INFO 140519950444352] Loss (name: value) total: 6.36277991601\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:57:21 INFO 140519950444352] Loss (name: value) kld: 0.148876987166\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:57:21 INFO 140519950444352] Loss (name: value) recons: 6.21390292484\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:57:21 INFO 140519950444352] Loss (name: value) logppx: 6.36277991601\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:57:21 INFO 140519950444352] #quality_metric: host=algo-1, epoch=24, train total_loss <loss>=6.36277991601\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:57:21 INFO 140519950444352] patience losses:[6.3669901998549543, 6.3650705456116041, 6.3638019700124477] min patience loss:6.36380197001 current loss:6.36277991601 absolute loss difference:0.00102205400022\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:57:21 INFO 140519950444352] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 23160, \"sum\": 23160.0, \"min\": 23160}, \"Total Records Seen\": {\"count\": 1, \"max\": 5926032, \"sum\": 5926032.0, \"min\": 5926032}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 48, \"sum\": 48.0, \"min\": 48}}, \"EndTime\": 1528034241.753485, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 23}, \"StartTime\": 1528034223.280029}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:57:21 INFO 140519950444352] #throughput_metric: host=algo-1, train throughput=13365.9962386 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:57:21 INFO 140519950444352] \u001b[0m\n",
      "\u001b[31m[06/03/2018 13:57:21 INFO 140519950444352] # Starting training for epoch 25\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:57:41 INFO 140519950444352] # Finished training epoch 25 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:57:41 INFO 140519950444352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:57:41 INFO 140519950444352] Loss (name: value) total: 6.36144160063\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:57:41 INFO 140519950444352] Loss (name: value) kld: 0.150155247841\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:57:41 INFO 140519950444352] Loss (name: value) recons: 6.21128635382\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:57:41 INFO 140519950444352] Loss (name: value) logppx: 6.36144160063\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:57:41 INFO 140519950444352] #quality_metric: host=algo-1, epoch=25, train total_loss <loss>=6.36144160063\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:57:41 INFO 140519950444352] patience losses:[6.3650705456116041, 6.3638019700124477, 6.3627799160122258] min patience loss:6.36277991601 current loss:6.36144160063 absolute loss difference:0.00133831538067\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:57:41 INFO 140519950444352] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 24125, \"sum\": 24125.0, \"min\": 24125}, \"Total Records Seen\": {\"count\": 1, \"max\": 6172950, \"sum\": 6172950.0, \"min\": 6172950}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 50, \"sum\": 50.0, \"min\": 50}}, \"EndTime\": 1528034261.248707, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 24}, \"StartTime\": 1528034241.753794}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:57:41 INFO 140519950444352] #throughput_metric: host=algo-1, train throughput=12665.6838273 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:57:41 INFO 140519950444352] \u001b[0m\n",
      "\u001b[31m[06/03/2018 13:57:41 INFO 140519950444352] # Starting training for epoch 26\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:00 INFO 140519950444352] # Finished training epoch 26 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:00 INFO 140519950444352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:00 INFO 140519950444352] Loss (name: value) total: 6.36028005051\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:00 INFO 140519950444352] Loss (name: value) kld: 0.151325439828\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:00 INFO 140519950444352] Loss (name: value) recons: 6.20895461814\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:00 INFO 140519950444352] Loss (name: value) logppx: 6.36028005051\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:00 INFO 140519950444352] #quality_metric: host=algo-1, epoch=26, train total_loss <loss>=6.36028005051\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:00 INFO 140519950444352] patience losses:[6.3638019700124477, 6.3627799160122258, 6.3614416006315562] min patience loss:6.36144160063 current loss:6.36028005051 absolute loss difference:0.00116155011666\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:00 INFO 140519950444352] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 25090, \"sum\": 25090.0, \"min\": 25090}, \"Total Records Seen\": {\"count\": 1, \"max\": 6419868, \"sum\": 6419868.0, \"min\": 6419868}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 52, \"sum\": 52.0, \"min\": 52}}, \"EndTime\": 1528034280.057009, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 25}, \"StartTime\": 1528034261.251491}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:00 INFO 140519950444352] #throughput_metric: host=algo-1, train throughput=13129.957399 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:00 INFO 140519950444352] \u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:00 INFO 140519950444352] # Starting training for epoch 27\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:18 INFO 140519950444352] # Finished training epoch 27 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:18 INFO 140519950444352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:18 INFO 140519950444352] Loss (name: value) total: 6.35932197596\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:18 INFO 140519950444352] Loss (name: value) kld: 0.153312753627\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:18 INFO 140519950444352] Loss (name: value) recons: 6.20600922194\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:18 INFO 140519950444352] Loss (name: value) logppx: 6.35932197596\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:18 INFO 140519950444352] #quality_metric: host=algo-1, epoch=27, train total_loss <loss>=6.35932197596\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:18 INFO 140519950444352] patience losses:[6.3627799160122258, 6.3614416006315562, 6.3602800505148931] min patience loss:6.36028005051 current loss:6.35932197596 absolute loss difference:0.000958074559819\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:18 INFO 140519950444352] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:18 INFO 140519950444352] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 26055, \"sum\": 26055.0, \"min\": 26055}, \"Total Records Seen\": {\"count\": 1, \"max\": 6666786, \"sum\": 6666786.0, \"min\": 6666786}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 54, \"sum\": 54.0, \"min\": 54}}, \"EndTime\": 1528034298.52924, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 26}, \"StartTime\": 1528034280.057318}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:18 INFO 140519950444352] #throughput_metric: host=algo-1, train throughput=13367.0915332 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:18 INFO 140519950444352] \u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:18 INFO 140519950444352] # Starting training for epoch 28\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[06/03/2018 13:58:36 INFO 140519950444352] # Finished training epoch 28 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:36 INFO 140519950444352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:36 INFO 140519950444352] Loss (name: value) total: 6.35857422982\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:36 INFO 140519950444352] Loss (name: value) kld: 0.1539169775\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:36 INFO 140519950444352] Loss (name: value) recons: 6.20465724703\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:36 INFO 140519950444352] Loss (name: value) logppx: 6.35857422982\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:36 INFO 140519950444352] #quality_metric: host=algo-1, epoch=28, train total_loss <loss>=6.35857422982\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:36 INFO 140519950444352] patience losses:[6.3614416006315562, 6.3602800505148931, 6.3593219759550736] min patience loss:6.35932197596 current loss:6.35857422982 absolute loss difference:0.000747746136522\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:36 INFO 140519950444352] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:36 INFO 140519950444352] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 27020, \"sum\": 27020.0, \"min\": 27020}, \"Total Records Seen\": {\"count\": 1, \"max\": 6913704, \"sum\": 6913704.0, \"min\": 6913704}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 56, \"sum\": 56.0, \"min\": 56}}, \"EndTime\": 1528034316.951925, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 27}, \"StartTime\": 1528034298.529625}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:36 INFO 140519950444352] #throughput_metric: host=algo-1, train throughput=13403.1198833 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:36 INFO 140519950444352] \u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:36 INFO 140519950444352] # Starting training for epoch 29\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:55 INFO 140519950444352] # Finished training epoch 29 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:55 INFO 140519950444352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:55 INFO 140519950444352] Loss (name: value) total: 6.3570873305\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:55 INFO 140519950444352] Loss (name: value) kld: 0.154899536297\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:55 INFO 140519950444352] Loss (name: value) recons: 6.20218778744\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:55 INFO 140519950444352] Loss (name: value) logppx: 6.3570873305\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:55 INFO 140519950444352] #quality_metric: host=algo-1, epoch=29, train total_loss <loss>=6.3570873305\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:55 INFO 140519950444352] patience losses:[6.3602800505148931, 6.3593219759550736, 6.3585742298185517] min patience loss:6.35857422982 current loss:6.3570873305 absolute loss difference:0.00148689932156\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:55 INFO 140519950444352] #progress_metric: host=algo-1, completed 58 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 27985, \"sum\": 27985.0, \"min\": 27985}, \"Total Records Seen\": {\"count\": 1, \"max\": 7160622, \"sum\": 7160622.0, \"min\": 7160622}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 58, \"sum\": 58.0, \"min\": 58}}, \"EndTime\": 1528034335.474994, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 28}, \"StartTime\": 1528034316.955531}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:55 INFO 140519950444352] #throughput_metric: host=algo-1, train throughput=13332.7630454 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:55 INFO 140519950444352] \u001b[0m\n",
      "\u001b[31m[06/03/2018 13:58:55 INFO 140519950444352] # Starting training for epoch 30\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:59:13 INFO 140519950444352] # Finished training epoch 30 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:59:13 INFO 140519950444352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:59:13 INFO 140519950444352] Loss (name: value) total: 6.35613761467\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:59:13 INFO 140519950444352] Loss (name: value) kld: 0.157266817632\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:59:13 INFO 140519950444352] Loss (name: value) recons: 6.19887079797\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:59:13 INFO 140519950444352] Loss (name: value) logppx: 6.35613761467\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:59:13 INFO 140519950444352] #quality_metric: host=algo-1, epoch=30, train total_loss <loss>=6.35613761467\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:59:13 INFO 140519950444352] patience losses:[6.3593219759550736, 6.3585742298185517, 6.3570873304969906] min patience loss:6.3570873305 current loss:6.35613761467 absolute loss difference:0.000949715826795\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:59:13 INFO 140519950444352] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:59:13 INFO 140519950444352] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 28950, \"sum\": 28950.0, \"min\": 28950}, \"Total Records Seen\": {\"count\": 1, \"max\": 7407540, \"sum\": 7407540.0, \"min\": 7407540}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 60, \"sum\": 60.0, \"min\": 60}}, \"EndTime\": 1528034353.896779, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 29}, \"StartTime\": 1528034335.475346}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:59:13 INFO 140519950444352] #throughput_metric: host=algo-1, train throughput=13403.7029074 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:59:13 INFO 140519950444352] \u001b[0m\n",
      "\u001b[31m[06/03/2018 13:59:13 INFO 140519950444352] # Starting training for epoch 31\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:59:32 INFO 140519950444352] # Finished training epoch 31 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:59:32 INFO 140519950444352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:59:32 INFO 140519950444352] Loss (name: value) total: 6.35359024853\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:59:32 INFO 140519950444352] Loss (name: value) kld: 0.15900035038\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:59:32 INFO 140519950444352] Loss (name: value) recons: 6.19458990023\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:59:32 INFO 140519950444352] Loss (name: value) logppx: 6.35359024853\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:59:32 INFO 140519950444352] #quality_metric: host=algo-1, epoch=31, train total_loss <loss>=6.35359024853\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:59:32 INFO 140519950444352] patience losses:[6.3585742298185517, 6.3570873304969906, 6.3561376146701951] min patience loss:6.35613761467 current loss:6.35359024853 absolute loss difference:0.00254736613733\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:59:32 INFO 140519950444352] #progress_metric: host=algo-1, completed 62 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 29915, \"sum\": 29915.0, \"min\": 29915}, \"Total Records Seen\": {\"count\": 1, \"max\": 7654458, \"sum\": 7654458.0, \"min\": 7654458}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 62, \"sum\": 62.0, \"min\": 62}}, \"EndTime\": 1528034372.290277, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 30}, \"StartTime\": 1528034353.89708}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:59:32 INFO 140519950444352] #throughput_metric: host=algo-1, train throughput=13424.3177216 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:59:32 INFO 140519950444352] \u001b[0m\n",
      "\u001b[31m[06/03/2018 13:59:32 INFO 140519950444352] # Starting training for epoch 32\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[06/03/2018 13:59:50 INFO 140519950444352] # Finished training epoch 32 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:59:50 INFO 140519950444352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:59:50 INFO 140519950444352] Loss (name: value) total: 6.3520228549\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:59:50 INFO 140519950444352] Loss (name: value) kld: 0.16312203669\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:59:50 INFO 140519950444352] Loss (name: value) recons: 6.18890082256\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:59:50 INFO 140519950444352] Loss (name: value) logppx: 6.3520228549\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:59:50 INFO 140519950444352] #quality_metric: host=algo-1, epoch=32, train total_loss <loss>=6.3520228549\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:59:50 INFO 140519950444352] patience losses:[6.3570873304969906, 6.3561376146701951, 6.3535902485328632] min patience loss:6.35359024853 current loss:6.3520228549 absolute loss difference:0.00156739363399\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:59:50 INFO 140519950444352] #progress_metric: host=algo-1, completed 64 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 30880, \"sum\": 30880.0, \"min\": 30880}, \"Total Records Seen\": {\"count\": 1, \"max\": 7901376, \"sum\": 7901376.0, \"min\": 7901376}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 64, \"sum\": 64.0, \"min\": 64}}, \"EndTime\": 1528034390.637313, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 31}, \"StartTime\": 1528034372.290571}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:59:50 INFO 140519950444352] #throughput_metric: host=algo-1, train throughput=13458.3057355 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 13:59:50 INFO 140519950444352] \u001b[0m\n",
      "\u001b[31m[06/03/2018 13:59:50 INFO 140519950444352] # Starting training for epoch 33\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:00:09 INFO 140519950444352] # Finished training epoch 33 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:00:09 INFO 140519950444352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:00:09 INFO 140519950444352] Loss (name: value) total: 6.3505619437\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:00:09 INFO 140519950444352] Loss (name: value) kld: 0.167161561016\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:00:09 INFO 140519950444352] Loss (name: value) recons: 6.18340039451\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:00:09 INFO 140519950444352] Loss (name: value) logppx: 6.3505619437\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:00:09 INFO 140519950444352] #quality_metric: host=algo-1, epoch=33, train total_loss <loss>=6.3505619437\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:00:09 INFO 140519950444352] patience losses:[6.3561376146701951, 6.3535902485328632, 6.3520228548988777] min patience loss:6.3520228549 current loss:6.3505619437 absolute loss difference:0.00146091120231\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:00:09 INFO 140519950444352] #progress_metric: host=algo-1, completed 66 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 31845, \"sum\": 31845.0, \"min\": 31845}, \"Total Records Seen\": {\"count\": 1, \"max\": 8148294, \"sum\": 8148294.0, \"min\": 8148294}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 66, \"sum\": 66.0, \"min\": 66}}, \"EndTime\": 1528034409.124083, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 32}, \"StartTime\": 1528034390.637639}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:00:09 INFO 140519950444352] #throughput_metric: host=algo-1, train throughput=13356.5984761 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:00:09 INFO 140519950444352] \u001b[0m\n",
      "\u001b[31m[06/03/2018 14:00:09 INFO 140519950444352] # Starting training for epoch 34\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:00:27 INFO 140519950444352] # Finished training epoch 34 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:00:27 INFO 140519950444352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:00:27 INFO 140519950444352] Loss (name: value) total: 6.34936797236\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:00:27 INFO 140519950444352] Loss (name: value) kld: 0.168900130434\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:00:27 INFO 140519950444352] Loss (name: value) recons: 6.18046785117\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:00:27 INFO 140519950444352] Loss (name: value) logppx: 6.34936797236\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:00:27 INFO 140519950444352] #quality_metric: host=algo-1, epoch=34, train total_loss <loss>=6.34936797236\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:00:27 INFO 140519950444352] patience losses:[6.3535902485328632, 6.3520228548988777, 6.3505619436965706] min patience loss:6.3505619437 current loss:6.34936797236 absolute loss difference:0.00119397133743\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:00:27 INFO 140519950444352] #progress_metric: host=algo-1, completed 68 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 32810, \"sum\": 32810.0, \"min\": 32810}, \"Total Records Seen\": {\"count\": 1, \"max\": 8395212, \"sum\": 8395212.0, \"min\": 8395212}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 68, \"sum\": 68.0, \"min\": 68}}, \"EndTime\": 1528034427.649986, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 33}, \"StartTime\": 1528034409.124403}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:00:27 INFO 140519950444352] #throughput_metric: host=algo-1, train throughput=13328.3702449 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:00:27 INFO 140519950444352] \u001b[0m\n",
      "\u001b[31m[06/03/2018 14:00:27 INFO 140519950444352] # Starting training for epoch 35\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:00:46 INFO 140519950444352] # Finished training epoch 35 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:00:46 INFO 140519950444352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:00:46 INFO 140519950444352] Loss (name: value) total: 6.34838841542\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:00:46 INFO 140519950444352] Loss (name: value) kld: 0.169694474803\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:00:46 INFO 140519950444352] Loss (name: value) recons: 6.17869393788\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:00:46 INFO 140519950444352] Loss (name: value) logppx: 6.34838841542\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:00:46 INFO 140519950444352] #quality_metric: host=algo-1, epoch=35, train total_loss <loss>=6.34838841542\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:00:46 INFO 140519950444352] patience losses:[6.3520228548988777, 6.3505619436965706, 6.3493679723591381] min patience loss:6.34936797236 current loss:6.34838841542 absolute loss difference:0.000979556938526\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:00:46 INFO 140519950444352] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:00:46 INFO 140519950444352] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 33775, \"sum\": 33775.0, \"min\": 33775}, \"Total Records Seen\": {\"count\": 1, \"max\": 8642130, \"sum\": 8642130.0, \"min\": 8642130}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 70, \"sum\": 70.0, \"min\": 70}}, \"EndTime\": 1528034446.271078, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 34}, \"StartTime\": 1528034427.651437}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:00:46 INFO 140519950444352] #throughput_metric: host=algo-1, train throughput=13261.0397054 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:00:46 INFO 140519950444352] \u001b[0m\n",
      "\u001b[31m[06/03/2018 14:00:46 INFO 140519950444352] # Starting training for epoch 36\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[06/03/2018 14:01:04 INFO 140519950444352] # Finished training epoch 36 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:01:04 INFO 140519950444352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:01:04 INFO 140519950444352] Loss (name: value) total: 6.34755487738\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:01:04 INFO 140519950444352] Loss (name: value) kld: 0.17104971046\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:01:04 INFO 140519950444352] Loss (name: value) recons: 6.17650516268\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:01:04 INFO 140519950444352] Loss (name: value) logppx: 6.34755487738\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:01:04 INFO 140519950444352] #quality_metric: host=algo-1, epoch=36, train total_loss <loss>=6.34755487738\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:01:04 INFO 140519950444352] patience losses:[6.3505619436965706, 6.3493679723591381, 6.3483884154206116] min patience loss:6.34838841542 current loss:6.34755487738 absolute loss difference:0.000833538035655\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:01:04 INFO 140519950444352] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:01:04 INFO 140519950444352] #progress_metric: host=algo-1, completed 72 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 34740, \"sum\": 34740.0, \"min\": 34740}, \"Total Records Seen\": {\"count\": 1, \"max\": 8889048, \"sum\": 8889048.0, \"min\": 8889048}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 72, \"sum\": 72.0, \"min\": 72}}, \"EndTime\": 1528034464.82462, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 35}, \"StartTime\": 1528034446.275471}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:01:04 INFO 140519950444352] #throughput_metric: host=algo-1, train throughput=13311.445747 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:01:04 INFO 140519950444352] \u001b[0m\n",
      "\u001b[31m[06/03/2018 14:01:04 INFO 140519950444352] # Starting training for epoch 37\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:01:23 INFO 140519950444352] # Finished training epoch 37 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:01:23 INFO 140519950444352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:01:23 INFO 140519950444352] Loss (name: value) total: 6.34659217281\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:01:23 INFO 140519950444352] Loss (name: value) kld: 0.171627715299\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:01:23 INFO 140519950444352] Loss (name: value) recons: 6.17496445389\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:01:23 INFO 140519950444352] Loss (name: value) logppx: 6.34659217281\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:01:23 INFO 140519950444352] #quality_metric: host=algo-1, epoch=37, train total_loss <loss>=6.34659217281\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:01:23 INFO 140519950444352] patience losses:[6.3493679723591381, 6.3483884154206116, 6.3475548773849564] min patience loss:6.34755487738 current loss:6.34659217281 absolute loss difference:0.000962704574506\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:01:23 INFO 140519950444352] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:01:23 INFO 140519950444352] #progress_metric: host=algo-1, completed 74 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 35705, \"sum\": 35705.0, \"min\": 35705}, \"Total Records Seen\": {\"count\": 1, \"max\": 9135966, \"sum\": 9135966.0, \"min\": 9135966}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 74, \"sum\": 74.0, \"min\": 74}}, \"EndTime\": 1528034483.341323, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 36}, \"StartTime\": 1528034464.827457}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:01:23 INFO 140519950444352] #throughput_metric: host=algo-1, train throughput=13336.8203901 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:01:23 INFO 140519950444352] \u001b[0m\n",
      "\u001b[31m[06/03/2018 14:01:23 INFO 140519950444352] # Starting training for epoch 38\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:01:41 INFO 140519950444352] # Finished training epoch 38 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:01:41 INFO 140519950444352] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:01:41 INFO 140519950444352] Loss (name: value) total: 6.34628967374\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:01:41 INFO 140519950444352] Loss (name: value) kld: 0.172514126454\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:01:41 INFO 140519950444352] Loss (name: value) recons: 6.17377554864\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:01:41 INFO 140519950444352] Loss (name: value) logppx: 6.34628967374\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:01:41 INFO 140519950444352] #quality_metric: host=algo-1, epoch=38, train total_loss <loss>=6.34628967374\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:01:41 INFO 140519950444352] patience losses:[6.3483884154206116, 6.3475548773849564, 6.3465921728104506] min patience loss:6.34659217281 current loss:6.34628967374 absolute loss difference:0.000302499069451\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:01:41 INFO 140519950444352] Bad epoch: loss has not improved (enough). Bad count:4\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:01:41 INFO 140519950444352] Bad epochs exceeded patience. Stopping training early!\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:01:41 INFO 140519950444352] Early stop condition met. Stopping training.\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:01:41 INFO 140519950444352] #progress_metric: host=algo-1, completed 100 % epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 36670, \"sum\": 36670.0, \"min\": 36670}, \"Total Records Seen\": {\"count\": 1, \"max\": 9382884, \"sum\": 9382884.0, \"min\": 9382884}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 76, \"sum\": 76.0, \"min\": 76}}, \"EndTime\": 1528034501.779396, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 37}, \"StartTime\": 1528034483.341639}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:01:41 INFO 140519950444352] #throughput_metric: host=algo-1, train throughput=13391.8780159 records/second\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:01:41 WARNING 140519950444352] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:01:41 INFO 140519950444352] Best model based on early stopping at epoch 38. Best loss: 6.34628967374\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:01:41 INFO 140519950444352] Topics from epoch:final (num_topics:20):\u001b[0m\n",
      "\u001b[31m405 957 994 597 327 566 401 225 981 181 264 341 448 201 269 513 496 872 784 103\u001b[0m\n",
      "\u001b[31m405 863 957 265 269 994 913 401 597 576 981 495 335 81 452 225 402 27 775 566\u001b[0m\n",
      "\u001b[31m957 405 597 994 327 448 130 27 833 74 147 659 211 269 566 726 858 572 371 274\u001b[0m\n",
      "\u001b[31m405 81 994 269 265 784 533 335 576 495 950 957 863 566 401 76 181 27 496 39\u001b[0m\n",
      "\u001b[31m726 448 909 957 943 788 930 397 677 449 128 383 674 981 550 295 927 348 865 249\u001b[0m\n",
      "\u001b[31m405 994 981 622 702 327 912 957 554 909 562 566 585 838 591 624 492 937 358 680\u001b[0m\n",
      "\u001b[31m981 405 588 957 642 994 726 891 382 828 167 225 567 622 900 448 286 130 912 563\u001b[0m\n",
      "\u001b[31m150 906 119 333 332 117 469 691 281 297 250 658 286 878 145 921 532 401 546 608\u001b[0m\n",
      "\u001b[31m520 945 937 239 761 280 227 604 891 342 106 382 730 906 989 64 392 838 827 565\u001b[0m\n",
      "\u001b[31m957 405 994 327 566 597 756 448 726 803 591 472 74 235 891 130 415 858 570 788\u001b[0m\n",
      "\u001b[31m957 405 994 597 726 335 448 492 863 167 103 981 437 433 566 269 289 265 183 397\u001b[0m\n",
      "\u001b[31m405 957 994 533 597 335 282 495 401 737 341 950 289 293 981 235 94 872 437 81\u001b[0m\n",
      "\u001b[31m405 994 957 726 863 448 784 181 566 805 950 225 533 597 341 327 496 335 865 201\u001b[0m\n",
      "\u001b[31m891 742 213 67 113 672 567 51 368 572 280 460 758 382 620 767 927 716 133 173\u001b[0m\n",
      "\u001b[31m957 405 622 994 327 563 981 382 891 570 937 496 130 402 858 157 702 120 437 919\u001b[0m\n",
      "\u001b[31m994 405 597 957 740 566 415 225 437 448 260 269 265 950 75 726 103 896 496 456\u001b[0m\n",
      "\u001b[31m957 405 994 878 520 702 280 642 496 382 937 604 327 906 981 891 764 622 448 858\u001b[0m\n",
      "\u001b[31m743 954 588 710 544 52 156 577 856 804 358 421 820 304 217 594 800 596 191 454\u001b[0m\n",
      "\u001b[31m691 119 333 332 150 935 469 906 27 117 863 269 81 250 281 201 533 805 423 316\u001b[0m\n",
      "\u001b[31m957 405 327 994 225 496 803 597 448 516 554 737 678 437 865 264 981 784 120 335\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:01:41 INFO 140519950444352] Serializing model to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:01:41 INFO 140519950444352] Saved checkpoint to \"/tmp/tmpobPvvD/state-0001.params\"\u001b[0m\n",
      "\u001b[31m[06/03/2018 14:01:41 INFO 140519950444352] Test data is not provided.\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 703681.2651157379, \"sum\": 703681.2651157379, \"min\": 703681.2651157379}, \"finalize.time\": {\"count\": 1, \"max\": 71.13003730773926, \"sum\": 71.13003730773926, \"min\": 71.13003730773926}, \"initialize.time\": {\"count\": 1, \"max\": 51.34105682373047, \"sum\": 51.34105682373047, \"min\": 51.34105682373047}, \"model.serialize.time\": {\"count\": 1, \"max\": 2.560138702392578, \"sum\": 2.560138702392578, \"min\": 2.560138702392578}, \"setuptime\": {\"count\": 1, \"max\": 14.142990112304688, \"sum\": 14.142990112304688, \"min\": 14.142990112304688}, \"early_stop.time\": {\"count\": 38, \"max\": 4.84919548034668, \"sum\": 154.16812896728516, \"min\": 0.7529258728027344}, \"update.time\": {\"count\": 38, \"max\": 19494.675159454346, \"sum\": 703441.1916732788, \"min\": 18340.6720161438}, \"epochs\": {\"count\": 1, \"max\": 50, \"sum\": 50.0, \"min\": 50}}, \"EndTime\": 1528034501.855469, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1528033798.21166}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Job Complete =====\n",
      "Billable seconds: 808\n"
     ]
    }
   ],
   "source": [
    "ntm.fit({'train': training_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "A trained model does nothing on its own. We now want to use the model to perform inference. For this example, that means predicting the topic mixture representing a given document or comment.\n",
    "\n",
    "This is simplified by the deploy function provided by the Amazon SageMaker Python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: ntm-2018-06-03-14-03-37-919\n",
      "INFO:sagemaker:Creating endpoint with name ntm-nyt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "ntm_predictor = ntm.deploy(initial_instance_count=1,\n",
    "                           instance_type='ml.m4.xlarge',\n",
    "                          endpoint_name='ntm-nyt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Inference\n",
    "\n",
    "With this real-time endpoint at our fingertips we can finally perform inference on our training and test data.  We should first discuss the meaning of the SageMaker NTM inference output.\n",
    "\n",
    "For each document we wish to compute its corresponding `topic_weights`. Each set of topic weights is a probability distribution over the number of topics, which is 5 in this example. Of the 5 topics discovered during NTM training each element of the topic weights is the proportion to which the input document is represented by the corresponding topic.\n",
    "\n",
    "For example, if the topic weights of an input document $\\mathbf{w}$ is,\n",
    "\n",
    "$$\\theta = \\left[ 0.3, 0.2, 0, 0.5, 0 \\right]$$\n",
    "\n",
    "then $\\mathbf{w}$ is 30% generated from Topic #1, 20% from Topic #2, and 50% from Topic #4. Below, we compute the topic mixtures for the first ten traning documents.\n",
    "\n",
    "First, we setup our serializes and deserializers which allow us to convert NumPy arrays to CSV strings which we can pass into our HTTP POST request to our hosted endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "ntm_predictor.content_type = 'text/csv'\n",
    "ntm_predictor.serializer = csv_serializer\n",
    "ntm_predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "test_comments = np.load('test_comments.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03881462 0.0330111  0.02953554 0.44098815 0.03035911 0.0249091\n",
      " 0.01516498 0.02849843 0.02895007 0.02496313 0.02657804 0.0277223\n",
      " 0.02970661 0.0321646  0.03495577 0.03026009 0.0334294  0.02988204\n",
      " 0.03124016 0.02886663]\n"
     ]
    }
   ],
   "source": [
    "results = ntm_predictor.predict(test_comments)\n",
    "predictions = np.array([prediction['topic_weights'] for prediction in results['predictions']])\n",
    "print(predictions[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 0, 3, 3, 3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint with name: ntm-2018-05-22-22-03-44-121\n"
     ]
    }
   ],
   "source": [
    "sagemaker.Session().delete_endpoint(ntm_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
