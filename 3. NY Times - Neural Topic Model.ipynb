{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "***\n",
    "\n",
    "Amazon SageMaker NTM (Neural Topic Model) is an unsupervised learning algorithm that attempts to describe a set of observations as a mixture of distinct categories. NTM is most commonly used to discover a user-specified number of topics shared by documents within a text corpus. Here each observation is a document, the features are the presence (or occurrence count) of each word, and the categories are the topics. Since the method is unsupervised, the topics are not specified up front, and are not guaranteed to align with how a human may naturally categorize documents. The topics are learned as a probability distribution over the words that occur in each document. Each document, in turn, is described as a mixture of topics.\n",
    "\n",
    "In this notebook we will use the Amazon SageMaker NTM algorithm to train a model on some example synthetic data. We will then use this model to classify (perform inference on) the data. The main goals of this notebook are to,\n",
    "\n",
    "* create an AWS SageMaker training job on a data set to produce a NTM model,\n",
    "* use the model to perform inference with an Amazon SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Created S3 bucket: sagemaker-us-east-1-228889150161\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Once the data is preprocessed and available in a recommended format the next step is to train our model on the data. There are number of parameters required by the NTM algorithm to configure the model and define the computational environment in which training will take place. The first of these is to point to a container image which holds the algorithms training and hosting code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "containers = {'us-west-2': '174872318107.dkr.ecr.us-west-2.amazonaws.com/ntm:latest',\n",
    "              'us-east-1': '382416733822.dkr.ecr.us-east-1.amazonaws.com/ntm:latest',\n",
    "              'us-east-2': '404615174143.dkr.ecr.us-east-2.amazonaws.com/ntm:latest',\n",
    "              'eu-west-1': '438346466558.dkr.ecr.eu-west-1.amazonaws.com/ntm:latest'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An NTM model uses the following hyperparameters:\n",
    "\n",
    "- **num_topics** - The number of topics or categories in the NTM model. \n",
    "- **feature_dim** - The size of the \"vocabulary\". In this case, this has been set to 1000 by the nytimes pyspark data prep.\n",
    "\n",
    "In addition to these NTM model hyperparameters, we provide additional parameters defining things like the EC2 instance type on which training will run, the S3 bucket containing the data, and the AWS access role.\n",
    "\n",
    "> Note: Try adjusting the mini_batch_size if running on a GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Created S3 bucket: sagemaker-us-east-1-228889150161\n"
     ]
    }
   ],
   "source": [
    "num_topics=20\n",
    "vocabulary_size=1000\n",
    "output = 's3://{}/data/nytimes-model/sagemaker-ntm'.format(sagemaker_session.default_bucket())\n",
    "\n",
    "ntm = sagemaker.estimator.Estimator(containers[boto3.Session().region_name],\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.c4.xlarge',\n",
    "                                    output_path=output,\n",
    "                                    sagemaker_session=sagemaker_session)\n",
    "\n",
    "ntm.set_hyperparameters(num_topics=num_topics,\n",
    "                        feature_dim=vocabulary_size,\n",
    "                        mini_batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll train against the bag-of-words extracted from the NY Times comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3_client = boto3.client('s3')\n",
    "objects = s3_client.list_objects(Bucket=bucket, Prefix='data/nyt-record-io/training.rec')\n",
    "training_key = objects['Contents'][0]['Key']\n",
    "training_input = 's3://{}/{}'.format(bucket, training_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: ntm-2018-06-01-21-13-08-814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....................\n",
      "\u001b[31mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:16:30 INFO 140428234315584] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/default-input.json: {u'num_patience_epochs': u'3', u'clip_gradient': u'Inf', u'encoder_layers': u'auto', u'optimizer': u'adadelta', u'_kvstore': u'auto', u'rescale_gradient': u'1.0', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'learning_rate': u'0.01', u'_data_format': u'record', u'epochs': u'50', u'weight_decay': u'0.0', u'_num_kv_servers': u'auto', u'encoder_layers_activation': u'sigmoid', u'mini_batch_size': u'256', u'tolerance': u'0.001', u'batch_norm': u'false'}\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:16:30 INFO 140428234315584] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'feature_dim': u'1000', u'mini_batch_size': u'256', u'num_topics': u'20'}\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:16:30 INFO 140428234315584] Final configuration: {u'num_patience_epochs': u'3', u'clip_gradient': u'Inf', u'encoder_layers': u'auto', u'optimizer': u'adadelta', u'_kvstore': u'auto', u'rescale_gradient': u'1.0', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'learning_rate': u'0.01', u'_data_format': u'record', u'epochs': u'50', u'feature_dim': u'1000', u'weight_decay': u'0.0', u'num_topics': u'20', u'_num_kv_servers': u'auto', u'encoder_layers_activation': u'sigmoid', u'mini_batch_size': u'256', u'tolerance': u'0.001', u'batch_norm': u'false'}\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:16:30 INFO 140428234315584] Using default worker.\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:16:30 INFO 140428234315584] Initializing\u001b[0m\n",
      "\u001b[31m/opt/amazon/lib/python2.7/site-packages/ai_algorithms_sdk/config/config_helper.py:122: DeprecationWarning: deprecated\n",
      "  warnings.warn(\"deprecated\", DeprecationWarning)\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:16:30 INFO 140428234315584] nvidia-smi took: 0.0251891613007 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:16:30 INFO 140428234315584] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:16:30 INFO 140428234315584] Create Store: local\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}}, \"EndTime\": 1527887790.33569, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1527887790.335655}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:16:30 INFO 140428234315584] \u001b[0m\n",
      "\u001b[31m[06/01/2018 21:16:30 INFO 140428234315584] # Starting training for epoch 1\u001b[0m\n",
      "\u001b[31m[21:16:30] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.200341.0/RHEL5_64/generic-flavor/src/src/ndarray/./../operator/tensor/.././../common/utils.h:416: \u001b[0m\n",
      "\u001b[31mStorage fallback detected:\u001b[0m\n",
      "\u001b[31mCopy from csr storage type on cpu to default storage type on cpu.\u001b[0m\n",
      "\u001b[31mA temporary ndarray with default storage type will be generated in order to perform the copy. You can set environment variable MXNET_STORAGE_FALLBACK_LOG_VERBOSE to 0 to suppress this warning.\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:16:49 INFO 140428234315584] # Finished training epoch 1 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:16:49 INFO 140428234315584] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:16:49 INFO 140428234315584] Loss (name: value) total: 6.47831054055\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:16:49 INFO 140428234315584] Loss (name: value) kld: 0.0168512276821\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:16:49 INFO 140428234315584] Loss (name: value) recons: 6.46145930611\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:16:49 INFO 140428234315584] Loss (name: value) logppx: 6.47831054055\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:16:49 INFO 140428234315584] #quality_metric: host=algo-1, epoch=1, train total_loss <loss>=6.47831054055\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:16:49 INFO 140428234315584] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Total Records Seen\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1527887809.01725, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 0}, \"StartTime\": 1527887790.337002}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:16:49 INFO 140428234315584] #throughput_metric: host=algo-1, train throughput=13218.0380371 records/second\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:16:49 INFO 140428234315584] \u001b[0m\n",
      "\u001b[31m[06/01/2018 21:16:49 INFO 140428234315584] # Starting training for epoch 2\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:17:07 INFO 140428234315584] # Finished training epoch 2 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:17:07 INFO 140428234315584] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:17:07 INFO 140428234315584] Loss (name: value) total: 6.45618780289\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:17:07 INFO 140428234315584] Loss (name: value) kld: 0.0232898235673\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:17:07 INFO 140428234315584] Loss (name: value) recons: 6.43289798529\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:17:07 INFO 140428234315584] Loss (name: value) logppx: 6.45618780289\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:17:07 INFO 140428234315584] #quality_metric: host=algo-1, epoch=2, train total_loss <loss>=6.45618780289\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:17:07 INFO 140428234315584] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1930, \"sum\": 1930.0, \"min\": 1930}, \"Total Records Seen\": {\"count\": 1, \"max\": 493836, \"sum\": 493836.0, \"min\": 493836}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 4, \"sum\": 4.0, \"min\": 4}}, \"EndTime\": 1527887827.486981, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 1}, \"StartTime\": 1527887809.019459}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:17:07 INFO 140428234315584] #throughput_metric: host=algo-1, train throughput=13370.2980568 records/second\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:17:07 INFO 140428234315584] \u001b[0m\n",
      "\u001b[31m[06/01/2018 21:17:07 INFO 140428234315584] # Starting training for epoch 3\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:17:26 INFO 140428234315584] # Finished training epoch 3 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:17:26 INFO 140428234315584] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:17:26 INFO 140428234315584] Loss (name: value) total: 6.44560309667\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:17:26 INFO 140428234315584] Loss (name: value) kld: 0.0341875246789\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:17:26 INFO 140428234315584] Loss (name: value) recons: 6.41141557249\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:17:26 INFO 140428234315584] Loss (name: value) logppx: 6.44560309667\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:17:26 INFO 140428234315584] #quality_metric: host=algo-1, epoch=3, train total_loss <loss>=6.44560309667\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:17:26 INFO 140428234315584] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2895, \"sum\": 2895.0, \"min\": 2895}, \"Total Records Seen\": {\"count\": 1, \"max\": 740754, \"sum\": 740754.0, \"min\": 740754}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 6, \"sum\": 6.0, \"min\": 6}}, \"EndTime\": 1527887846.009969, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 2}, \"StartTime\": 1527887827.492972}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:17:26 INFO 140428234315584] #throughput_metric: host=algo-1, train throughput=13334.5617699 records/second\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:17:26 INFO 140428234315584] \u001b[0m\n",
      "\u001b[31m[06/01/2018 21:17:26 INFO 140428234315584] # Starting training for epoch 4\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[06/01/2018 21:17:44 INFO 140428234315584] # Finished training epoch 4 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:17:44 INFO 140428234315584] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:17:44 INFO 140428234315584] Loss (name: value) total: 6.43195738619\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:17:44 INFO 140428234315584] Loss (name: value) kld: 0.0505995121278\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:17:44 INFO 140428234315584] Loss (name: value) recons: 6.38135787267\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:17:44 INFO 140428234315584] Loss (name: value) logppx: 6.43195738619\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:17:44 INFO 140428234315584] #quality_metric: host=algo-1, epoch=4, train total_loss <loss>=6.43195738619\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:17:44 INFO 140428234315584] patience losses:[6.4783105405501136, 6.4561878028928925, 6.4456030966704372] min patience loss:6.44560309667 current loss:6.43195738619 absolute loss difference:0.0136457104757\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:17:44 INFO 140428234315584] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3860, \"sum\": 3860.0, \"min\": 3860}, \"Total Records Seen\": {\"count\": 1, \"max\": 987672, \"sum\": 987672.0, \"min\": 987672}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}}, \"EndTime\": 1527887864.381861, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 3}, \"StartTime\": 1527887846.01028}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:17:44 INFO 140428234315584] #throughput_metric: host=algo-1, train throughput=13440.0979875 records/second\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:17:44 INFO 140428234315584] \u001b[0m\n",
      "\u001b[31m[06/01/2018 21:17:44 INFO 140428234315584] # Starting training for epoch 5\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:18:02 INFO 140428234315584] # Finished training epoch 5 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:18:02 INFO 140428234315584] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:18:02 INFO 140428234315584] Loss (name: value) total: 6.42510351542\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:18:02 INFO 140428234315584] Loss (name: value) kld: 0.0600351444508\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:18:02 INFO 140428234315584] Loss (name: value) recons: 6.36506836427\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:18:02 INFO 140428234315584] Loss (name: value) logppx: 6.42510351542\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:18:02 INFO 140428234315584] #quality_metric: host=algo-1, epoch=5, train total_loss <loss>=6.42510351542\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:18:02 INFO 140428234315584] patience losses:[6.4561878028928925, 6.4456030966704372, 6.4319573861947328] min patience loss:6.43195738619 current loss:6.42510351542 absolute loss difference:0.00685387077727\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:18:02 INFO 140428234315584] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 4825, \"sum\": 4825.0, \"min\": 4825}, \"Total Records Seen\": {\"count\": 1, \"max\": 1234590, \"sum\": 1234590.0, \"min\": 1234590}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}}, \"EndTime\": 1527887882.825398, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 4}, \"StartTime\": 1527887864.382162}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:18:02 INFO 140428234315584] #throughput_metric: host=algo-1, train throughput=13387.89735 records/second\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:18:02 INFO 140428234315584] \u001b[0m\n",
      "\u001b[31m[06/01/2018 21:18:02 INFO 140428234315584] # Starting training for epoch 6\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:18:21 INFO 140428234315584] # Finished training epoch 6 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:18:21 INFO 140428234315584] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:18:21 INFO 140428234315584] Loss (name: value) total: 6.4178449648\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:18:21 INFO 140428234315584] Loss (name: value) kld: 0.0684053747611\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:18:21 INFO 140428234315584] Loss (name: value) recons: 6.34943960121\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:18:21 INFO 140428234315584] Loss (name: value) logppx: 6.4178449648\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:18:21 INFO 140428234315584] #quality_metric: host=algo-1, epoch=6, train total_loss <loss>=6.4178449648\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:18:21 INFO 140428234315584] patience losses:[6.4456030966704372, 6.4319573861947328, 6.4251035154174643] min patience loss:6.42510351542 current loss:6.4178449648 absolute loss difference:0.00725855061427\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:18:21 INFO 140428234315584] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 5790, \"sum\": 5790.0, \"min\": 5790}, \"Total Records Seen\": {\"count\": 1, \"max\": 1481508, \"sum\": 1481508.0, \"min\": 1481508}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 12, \"sum\": 12.0, \"min\": 12}}, \"EndTime\": 1527887901.180399, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 5}, \"StartTime\": 1527887882.825753}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:18:21 INFO 140428234315584] #throughput_metric: host=algo-1, train throughput=13452.4999273 records/second\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:18:21 INFO 140428234315584] \u001b[0m\n",
      "\u001b[31m[06/01/2018 21:18:21 INFO 140428234315584] # Starting training for epoch 7\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:18:39 INFO 140428234315584] # Finished training epoch 7 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:18:39 INFO 140428234315584] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:18:39 INFO 140428234315584] Loss (name: value) total: 6.41095480029\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:18:39 INFO 140428234315584] Loss (name: value) kld: 0.0761636576736\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:18:39 INFO 140428234315584] Loss (name: value) recons: 6.33479114221\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:18:39 INFO 140428234315584] Loss (name: value) logppx: 6.41095480029\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:18:39 INFO 140428234315584] #quality_metric: host=algo-1, epoch=7, train total_loss <loss>=6.41095480029\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:18:39 INFO 140428234315584] patience losses:[6.4319573861947328, 6.4251035154174643, 6.4178449648031917] min patience loss:6.4178449648 current loss:6.41095480029 absolute loss difference:0.00689016450872\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:18:39 INFO 140428234315584] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 6755, \"sum\": 6755.0, \"min\": 6755}, \"Total Records Seen\": {\"count\": 1, \"max\": 1728426, \"sum\": 1728426.0, \"min\": 1728426}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 14, \"sum\": 14.0, \"min\": 14}}, \"EndTime\": 1527887919.64211, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 6}, \"StartTime\": 1527887901.180711}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:18:39 INFO 140428234315584] #throughput_metric: host=algo-1, train throughput=13374.7266608 records/second\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:18:39 INFO 140428234315584] \u001b[0m\n",
      "\u001b[31m[06/01/2018 21:18:39 INFO 140428234315584] # Starting training for epoch 8\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[06/01/2018 21:18:58 INFO 140428234315584] # Finished training epoch 8 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:18:58 INFO 140428234315584] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:18:58 INFO 140428234315584] Loss (name: value) total: 6.40672978515\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:18:58 INFO 140428234315584] Loss (name: value) kld: 0.0810080907004\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:18:58 INFO 140428234315584] Loss (name: value) recons: 6.32572168563\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:18:58 INFO 140428234315584] Loss (name: value) logppx: 6.40672978515\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:18:58 INFO 140428234315584] #quality_metric: host=algo-1, epoch=8, train total_loss <loss>=6.40672978515\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:18:58 INFO 140428234315584] patience losses:[6.4251035154174643, 6.4178449648031917, 6.410954800294471] min patience loss:6.41095480029 current loss:6.40672978515 absolute loss difference:0.00422501514613\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:18:58 INFO 140428234315584] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 7720, \"sum\": 7720.0, \"min\": 7720}, \"Total Records Seen\": {\"count\": 1, \"max\": 1975344, \"sum\": 1975344.0, \"min\": 1975344}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 16, \"sum\": 16.0, \"min\": 16}}, \"EndTime\": 1527887938.06336, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 7}, \"StartTime\": 1527887919.642521}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:18:58 INFO 140428234315584] #throughput_metric: host=algo-1, train throughput=13404.1576009 records/second\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:18:58 INFO 140428234315584] \u001b[0m\n",
      "\u001b[31m[06/01/2018 21:18:58 INFO 140428234315584] # Starting training for epoch 9\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:19:16 INFO 140428234315584] # Finished training epoch 9 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:19:16 INFO 140428234315584] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:19:16 INFO 140428234315584] Loss (name: value) total: 6.40300980034\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:19:16 INFO 140428234315584] Loss (name: value) kld: 0.0852636697302\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:19:16 INFO 140428234315584] Loss (name: value) recons: 6.31774613153\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:19:16 INFO 140428234315584] Loss (name: value) logppx: 6.40300980034\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:19:16 INFO 140428234315584] #quality_metric: host=algo-1, epoch=9, train total_loss <loss>=6.40300980034\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:19:16 INFO 140428234315584] patience losses:[6.4178449648031917, 6.410954800294471, 6.406729785148344] min patience loss:6.40672978515 current loss:6.40300980034 absolute loss difference:0.00371998480565\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:19:16 INFO 140428234315584] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 8685, \"sum\": 8685.0, \"min\": 8685}, \"Total Records Seen\": {\"count\": 1, \"max\": 2222262, \"sum\": 2222262.0, \"min\": 2222262}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 18, \"sum\": 18.0, \"min\": 18}}, \"EndTime\": 1527887956.526195, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 8}, \"StartTime\": 1527887938.068563}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:19:16 INFO 140428234315584] #throughput_metric: host=algo-1, train throughput=13377.4144686 records/second\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:19:16 INFO 140428234315584] \u001b[0m\n",
      "\u001b[31m[06/01/2018 21:19:16 INFO 140428234315584] # Starting training for epoch 10\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:19:34 INFO 140428234315584] # Finished training epoch 10 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:19:34 INFO 140428234315584] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:19:34 INFO 140428234315584] Loss (name: value) total: 6.39864210341\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:19:34 INFO 140428234315584] Loss (name: value) kld: 0.0910801348136\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:19:34 INFO 140428234315584] Loss (name: value) recons: 6.3075619747\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:19:34 INFO 140428234315584] Loss (name: value) logppx: 6.39864210341\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:19:34 INFO 140428234315584] #quality_metric: host=algo-1, epoch=10, train total_loss <loss>=6.39864210341\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:19:34 INFO 140428234315584] patience losses:[6.410954800294471, 6.406729785148344, 6.4030098003426978] min patience loss:6.40300980034 current loss:6.39864210341 absolute loss difference:0.00436769693009\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:19:34 INFO 140428234315584] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 9650, \"sum\": 9650.0, \"min\": 9650}, \"Total Records Seen\": {\"count\": 1, \"max\": 2469180, \"sum\": 2469180.0, \"min\": 2469180}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 20, \"sum\": 20.0, \"min\": 20}}, \"EndTime\": 1527887974.961396, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 9}, \"StartTime\": 1527887956.526495}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:19:34 INFO 140428234315584] #throughput_metric: host=algo-1, train throughput=13393.9454541 records/second\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:19:34 INFO 140428234315584] \u001b[0m\n",
      "\u001b[31m[06/01/2018 21:19:34 INFO 140428234315584] # Starting training for epoch 11\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:19:53 INFO 140428234315584] # Finished training epoch 11 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:19:53 INFO 140428234315584] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:19:53 INFO 140428234315584] Loss (name: value) total: 6.39321902873\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:19:53 INFO 140428234315584] Loss (name: value) kld: 0.10022138151\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:19:53 INFO 140428234315584] Loss (name: value) recons: 6.29299764164\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:19:53 INFO 140428234315584] Loss (name: value) logppx: 6.39321902873\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:19:53 INFO 140428234315584] #quality_metric: host=algo-1, epoch=11, train total_loss <loss>=6.39321902873\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:19:53 INFO 140428234315584] patience losses:[6.406729785148344, 6.4030098003426978, 6.398642103412608] min patience loss:6.39864210341 current loss:6.39321902873 absolute loss difference:0.00542307468276\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:19:53 INFO 140428234315584] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 10615, \"sum\": 10615.0, \"min\": 10615}, \"Total Records Seen\": {\"count\": 1, \"max\": 2716098, \"sum\": 2716098.0, \"min\": 2716098}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 22, \"sum\": 22.0, \"min\": 22}}, \"EndTime\": 1527887993.443776, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 10}, \"StartTime\": 1527887974.963517}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:19:53 INFO 140428234315584] #throughput_metric: host=algo-1, train throughput=13361.0614459 records/second\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:19:53 INFO 140428234315584] \u001b[0m\n",
      "\u001b[31m[06/01/2018 21:19:53 INFO 140428234315584] # Starting training for epoch 12\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[06/01/2018 21:20:11 INFO 140428234315584] # Finished training epoch 12 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:20:11 INFO 140428234315584] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:20:11 INFO 140428234315584] Loss (name: value) total: 6.38788782797\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:20:11 INFO 140428234315584] Loss (name: value) kld: 0.106702757824\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:20:11 INFO 140428234315584] Loss (name: value) recons: 6.28118507109\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:20:11 INFO 140428234315584] Loss (name: value) logppx: 6.38788782797\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:20:11 INFO 140428234315584] #quality_metric: host=algo-1, epoch=12, train total_loss <loss>=6.38788782797\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:20:11 INFO 140428234315584] patience losses:[6.4030098003426978, 6.398642103412608, 6.3932190287298489] min patience loss:6.39321902873 current loss:6.38788782797 absolute loss difference:0.00533120076273\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:20:11 INFO 140428234315584] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 11580, \"sum\": 11580.0, \"min\": 11580}, \"Total Records Seen\": {\"count\": 1, \"max\": 2963016, \"sum\": 2963016.0, \"min\": 2963016}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 24, \"sum\": 24.0, \"min\": 24}}, \"EndTime\": 1527888011.928186, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 11}, \"StartTime\": 1527887993.444081}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:20:11 INFO 140428234315584] #throughput_metric: host=algo-1, train throughput=13358.2900904 records/second\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:20:11 INFO 140428234315584] \u001b[0m\n",
      "\u001b[31m[06/01/2018 21:20:11 INFO 140428234315584] # Starting training for epoch 13\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:20:30 INFO 140428234315584] # Finished training epoch 13 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:20:30 INFO 140428234315584] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:20:30 INFO 140428234315584] Loss (name: value) total: 6.38549128443\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:20:30 INFO 140428234315584] Loss (name: value) kld: 0.111658027934\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:20:30 INFO 140428234315584] Loss (name: value) recons: 6.27383324964\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:20:30 INFO 140428234315584] Loss (name: value) logppx: 6.38549128443\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:20:30 INFO 140428234315584] #quality_metric: host=algo-1, epoch=13, train total_loss <loss>=6.38549128443\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:20:30 INFO 140428234315584] patience losses:[6.398642103412608, 6.3932190287298489, 6.3878878279671154] min patience loss:6.38788782797 current loss:6.38549128443 absolute loss difference:0.00239654353246\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:20:30 INFO 140428234315584] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 12545, \"sum\": 12545.0, \"min\": 12545}, \"Total Records Seen\": {\"count\": 1, \"max\": 3209934, \"sum\": 3209934.0, \"min\": 3209934}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 26, \"sum\": 26.0, \"min\": 26}}, \"EndTime\": 1527888030.341944, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 12}, \"StartTime\": 1527888011.928555}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:20:30 INFO 140428234315584] #throughput_metric: host=algo-1, train throughput=13409.5602477 records/second\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:20:30 INFO 140428234315584] \u001b[0m\n",
      "\u001b[31m[06/01/2018 21:20:30 INFO 140428234315584] # Starting training for epoch 14\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:20:48 INFO 140428234315584] # Finished training epoch 14 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:20:48 INFO 140428234315584] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:20:48 INFO 140428234315584] Loss (name: value) total: 6.38309943145\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:20:48 INFO 140428234315584] Loss (name: value) kld: 0.11598199394\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:20:48 INFO 140428234315584] Loss (name: value) recons: 6.2671174294\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:20:48 INFO 140428234315584] Loss (name: value) logppx: 6.38309943145\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:20:48 INFO 140428234315584] #quality_metric: host=algo-1, epoch=14, train total_loss <loss>=6.38309943145\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:20:48 INFO 140428234315584] patience losses:[6.3932190287298489, 6.3878878279671154, 6.3854912844346599] min patience loss:6.38549128443 current loss:6.38309943145 absolute loss difference:0.00239185298663\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:20:48 INFO 140428234315584] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 13510, \"sum\": 13510.0, \"min\": 13510}, \"Total Records Seen\": {\"count\": 1, \"max\": 3456852, \"sum\": 3456852.0, \"min\": 3456852}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 28, \"sum\": 28.0, \"min\": 28}}, \"EndTime\": 1527888048.720656, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 13}, \"StartTime\": 1527888030.343462}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:20:48 INFO 140428234315584] #throughput_metric: host=algo-1, train throughput=13435.9899468 records/second\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:20:48 INFO 140428234315584] \u001b[0m\n",
      "\u001b[31m[06/01/2018 21:20:48 INFO 140428234315584] # Starting training for epoch 15\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:21:07 INFO 140428234315584] # Finished training epoch 15 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:21:07 INFO 140428234315584] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:21:07 INFO 140428234315584] Loss (name: value) total: 6.37975286177\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:21:07 INFO 140428234315584] Loss (name: value) kld: 0.120842387423\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:21:07 INFO 140428234315584] Loss (name: value) recons: 6.25891045511\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:21:07 INFO 140428234315584] Loss (name: value) logppx: 6.37975286177\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:21:07 INFO 140428234315584] #quality_metric: host=algo-1, epoch=15, train total_loss <loss>=6.37975286177\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:21:07 INFO 140428234315584] patience losses:[6.3878878279671154, 6.3854912844346599, 6.3830994314480325] min patience loss:6.38309943145 current loss:6.37975286177 absolute loss difference:0.003346569674\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:21:07 INFO 140428234315584] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 14475, \"sum\": 14475.0, \"min\": 14475}, \"Total Records Seen\": {\"count\": 1, \"max\": 3703770, \"sum\": 3703770.0, \"min\": 3703770}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 30, \"sum\": 30.0, \"min\": 30}}, \"EndTime\": 1527888067.122264, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 14}, \"StartTime\": 1527888048.723477}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:21:07 INFO 140428234315584] #throughput_metric: host=algo-1, train throughput=13420.2391453 records/second\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:21:07 INFO 140428234315584] \u001b[0m\n",
      "\u001b[31m[06/01/2018 21:21:07 INFO 140428234315584] # Starting training for epoch 16\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[06/01/2018 21:21:25 INFO 140428234315584] # Finished training epoch 16 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:21:25 INFO 140428234315584] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:21:25 INFO 140428234315584] Loss (name: value) total: 6.37562277626\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:21:25 INFO 140428234315584] Loss (name: value) kld: 0.126761106521\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:21:25 INFO 140428234315584] Loss (name: value) recons: 6.24886167136\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:21:25 INFO 140428234315584] Loss (name: value) logppx: 6.37562277626\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:21:25 INFO 140428234315584] #quality_metric: host=algo-1, epoch=16, train total_loss <loss>=6.37562277626\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:21:25 INFO 140428234315584] patience losses:[6.3854912844346599, 6.3830994314480325, 6.3797528617740298] min patience loss:6.37975286177 current loss:6.37562277626 absolute loss difference:0.00413008551523\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:21:25 INFO 140428234315584] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 15440, \"sum\": 15440.0, \"min\": 15440}, \"Total Records Seen\": {\"count\": 1, \"max\": 3950688, \"sum\": 3950688.0, \"min\": 3950688}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 32, \"sum\": 32.0, \"min\": 32}}, \"EndTime\": 1527888085.538288, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 15}, \"StartTime\": 1527888067.122571}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:21:25 INFO 140428234315584] #throughput_metric: host=algo-1, train throughput=13407.885654 records/second\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:21:25 INFO 140428234315584] \u001b[0m\n",
      "\u001b[31m[06/01/2018 21:21:25 INFO 140428234315584] # Starting training for epoch 17\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:21:43 INFO 140428234315584] # Finished training epoch 17 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:21:43 INFO 140428234315584] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:21:43 INFO 140428234315584] Loss (name: value) total: 6.37261156813\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:21:43 INFO 140428234315584] Loss (name: value) kld: 0.131163615189\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:21:43 INFO 140428234315584] Loss (name: value) recons: 6.24144795472\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:21:43 INFO 140428234315584] Loss (name: value) logppx: 6.37261156813\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:21:43 INFO 140428234315584] #quality_metric: host=algo-1, epoch=17, train total_loss <loss>=6.37261156813\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:21:43 INFO 140428234315584] patience losses:[6.3830994314480325, 6.3797528617740298, 6.3756227762587949] min patience loss:6.37562277626 current loss:6.37261156813 absolute loss difference:0.00301120812411\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:21:43 INFO 140428234315584] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 16405, \"sum\": 16405.0, \"min\": 16405}, \"Total Records Seen\": {\"count\": 1, \"max\": 4197606, \"sum\": 4197606.0, \"min\": 4197606}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 34, \"sum\": 34.0, \"min\": 34}}, \"EndTime\": 1527888103.897698, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 16}, \"StartTime\": 1527888085.5386}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:21:43 INFO 140428234315584] #throughput_metric: host=algo-1, train throughput=13449.252462 records/second\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:21:43 INFO 140428234315584] \u001b[0m\n",
      "\u001b[31m[06/01/2018 21:21:43 INFO 140428234315584] # Starting training for epoch 18\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:22:02 INFO 140428234315584] # Finished training epoch 18 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:22:02 INFO 140428234315584] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:22:02 INFO 140428234315584] Loss (name: value) total: 6.37008446412\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:22:02 INFO 140428234315584] Loss (name: value) kld: 0.134858862562\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:22:02 INFO 140428234315584] Loss (name: value) recons: 6.23522559818\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:22:02 INFO 140428234315584] Loss (name: value) logppx: 6.37008446412\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:22:02 INFO 140428234315584] #quality_metric: host=algo-1, epoch=18, train total_loss <loss>=6.37008446412\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:22:02 INFO 140428234315584] patience losses:[6.3797528617740298, 6.3756227762587949, 6.3726115681346833] min patience loss:6.37261156813 current loss:6.37008446412 absolute loss difference:0.00252710401703\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:22:02 INFO 140428234315584] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 17370, \"sum\": 17370.0, \"min\": 17370}, \"Total Records Seen\": {\"count\": 1, \"max\": 4444524, \"sum\": 4444524.0, \"min\": 4444524}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}}, \"EndTime\": 1527888122.363088, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 17}, \"StartTime\": 1527888103.899466}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:22:02 INFO 140428234315584] #throughput_metric: host=algo-1, train throughput=13373.1244757 records/second\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:22:02 INFO 140428234315584] \u001b[0m\n",
      "\u001b[31m[06/01/2018 21:22:02 INFO 140428234315584] # Starting training for epoch 19\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:22:20 INFO 140428234315584] # Finished training epoch 19 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:22:20 INFO 140428234315584] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:22:20 INFO 140428234315584] Loss (name: value) total: 6.36918151687\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:22:20 INFO 140428234315584] Loss (name: value) kld: 0.138229017794\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:22:20 INFO 140428234315584] Loss (name: value) recons: 6.23095249438\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:22:20 INFO 140428234315584] Loss (name: value) logppx: 6.36918151687\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:22:20 INFO 140428234315584] #quality_metric: host=algo-1, epoch=19, train total_loss <loss>=6.36918151687\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:22:20 INFO 140428234315584] patience losses:[6.3756227762587949, 6.3726115681346833, 6.3700844641176531] min patience loss:6.37008446412 current loss:6.36918151687 absolute loss difference:0.000902947243014\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:22:20 INFO 140428234315584] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:22:20 INFO 140428234315584] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 18335, \"sum\": 18335.0, \"min\": 18335}, \"Total Records Seen\": {\"count\": 1, \"max\": 4691442, \"sum\": 4691442.0, \"min\": 4691442}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 38, \"sum\": 38.0, \"min\": 38}}, \"EndTime\": 1527888140.78943, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 18}, \"StartTime\": 1527888122.367556}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:22:20 INFO 140428234315584] #throughput_metric: host=algo-1, train throughput=13403.4015881 records/second\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:22:20 INFO 140428234315584] \u001b[0m\n",
      "\u001b[31m[06/01/2018 21:22:20 INFO 140428234315584] # Starting training for epoch 20\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[06/01/2018 21:22:39 INFO 140428234315584] # Finished training epoch 20 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:22:39 INFO 140428234315584] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:22:39 INFO 140428234315584] Loss (name: value) total: 6.36674192458\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:22:39 INFO 140428234315584] Loss (name: value) kld: 0.141850165776\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:22:39 INFO 140428234315584] Loss (name: value) recons: 6.22489174882\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:22:39 INFO 140428234315584] Loss (name: value) logppx: 6.36674192458\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:22:39 INFO 140428234315584] #quality_metric: host=algo-1, epoch=20, train total_loss <loss>=6.36674192458\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:22:39 INFO 140428234315584] patience losses:[6.3726115681346833, 6.3700844641176531, 6.3691815168746393] min patience loss:6.36918151687 current loss:6.36674192458 absolute loss difference:0.00243959229227\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:22:39 INFO 140428234315584] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 19300, \"sum\": 19300.0, \"min\": 19300}, \"Total Records Seen\": {\"count\": 1, \"max\": 4938360, \"sum\": 4938360.0, \"min\": 4938360}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 40, \"sum\": 40.0, \"min\": 40}}, \"EndTime\": 1527888159.158395, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 19}, \"StartTime\": 1527888140.789721}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:22:39 INFO 140428234315584] #throughput_metric: host=algo-1, train throughput=13442.2220412 records/second\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:22:39 INFO 140428234315584] \u001b[0m\n",
      "\u001b[31m[06/01/2018 21:22:39 INFO 140428234315584] # Starting training for epoch 21\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:22:57 INFO 140428234315584] # Finished training epoch 21 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:22:57 INFO 140428234315584] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:22:57 INFO 140428234315584] Loss (name: value) total: 6.36379859929\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:22:57 INFO 140428234315584] Loss (name: value) kld: 0.146595558401\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:22:57 INFO 140428234315584] Loss (name: value) recons: 6.21720304761\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:22:57 INFO 140428234315584] Loss (name: value) logppx: 6.36379859929\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:22:57 INFO 140428234315584] #quality_metric: host=algo-1, epoch=21, train total_loss <loss>=6.36379859929\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:22:57 INFO 140428234315584] patience losses:[6.3700844641176531, 6.3691815168746393, 6.3667419245823673] min patience loss:6.36674192458 current loss:6.36379859929 absolute loss difference:0.00294332528979\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:22:57 INFO 140428234315584] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 20265, \"sum\": 20265.0, \"min\": 20265}, \"Total Records Seen\": {\"count\": 1, \"max\": 5185278, \"sum\": 5185278.0, \"min\": 5185278}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 42, \"sum\": 42.0, \"min\": 42}}, \"EndTime\": 1527888177.566509, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 20}, \"StartTime\": 1527888159.159493}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:22:57 INFO 140428234315584] #throughput_metric: host=algo-1, train throughput=13414.2258298 records/second\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:22:57 INFO 140428234315584] \u001b[0m\n",
      "\u001b[31m[06/01/2018 21:22:57 INFO 140428234315584] # Starting training for epoch 22\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:23:16 INFO 140428234315584] # Finished training epoch 22 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:23:16 INFO 140428234315584] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:23:16 INFO 140428234315584] Loss (name: value) total: 6.36209183382\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:23:16 INFO 140428234315584] Loss (name: value) kld: 0.14930252842\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:23:16 INFO 140428234315584] Loss (name: value) recons: 6.21278930032\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:23:16 INFO 140428234315584] Loss (name: value) logppx: 6.36209183382\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:23:16 INFO 140428234315584] #quality_metric: host=algo-1, epoch=22, train total_loss <loss>=6.36209183382\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:23:16 INFO 140428234315584] patience losses:[6.3691815168746393, 6.3667419245823673, 6.3637985992925774] min patience loss:6.36379859929 current loss:6.36209183382 absolute loss difference:0.00170676547629\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:23:16 INFO 140428234315584] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 21230, \"sum\": 21230.0, \"min\": 21230}, \"Total Records Seen\": {\"count\": 1, \"max\": 5432196, \"sum\": 5432196.0, \"min\": 5432196}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 44, \"sum\": 44.0, \"min\": 44}}, \"EndTime\": 1527888196.041534, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 21}, \"StartTime\": 1527888177.566877}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:23:16 INFO 140428234315584] #throughput_metric: host=algo-1, train throughput=13365.1301704 records/second\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:23:16 INFO 140428234315584] \u001b[0m\n",
      "\u001b[31m[06/01/2018 21:23:16 INFO 140428234315584] # Starting training for epoch 23\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:23:34 INFO 140428234315584] # Finished training epoch 23 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:23:34 INFO 140428234315584] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:23:34 INFO 140428234315584] Loss (name: value) total: 6.3610591923\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:23:34 INFO 140428234315584] Loss (name: value) kld: 0.151472609455\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:23:34 INFO 140428234315584] Loss (name: value) recons: 6.20958656425\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:23:34 INFO 140428234315584] Loss (name: value) logppx: 6.3610591923\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:23:34 INFO 140428234315584] #quality_metric: host=algo-1, epoch=23, train total_loss <loss>=6.3610591923\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:23:34 INFO 140428234315584] patience losses:[6.3667419245823673, 6.3637985992925774, 6.3620918338162911] min patience loss:6.36209183382 current loss:6.3610591923 absolute loss difference:0.0010326415146\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:23:34 INFO 140428234315584] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 22195, \"sum\": 22195.0, \"min\": 22195}, \"Total Records Seen\": {\"count\": 1, \"max\": 5679114, \"sum\": 5679114.0, \"min\": 5679114}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 46, \"sum\": 46.0, \"min\": 46}}, \"EndTime\": 1527888214.405002, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 22}, \"StartTime\": 1527888196.041913}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:23:34 INFO 140428234315584] #throughput_metric: host=algo-1, train throughput=13446.3288314 records/second\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:23:34 INFO 140428234315584] \u001b[0m\n",
      "\u001b[31m[06/01/2018 21:23:34 INFO 140428234315584] # Starting training for epoch 24\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[06/01/2018 21:23:52 INFO 140428234315584] # Finished training epoch 24 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:23:52 INFO 140428234315584] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:23:52 INFO 140428234315584] Loss (name: value) total: 6.36119400454\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:23:52 INFO 140428234315584] Loss (name: value) kld: 0.152351276129\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:23:52 INFO 140428234315584] Loss (name: value) recons: 6.20884273139\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:23:52 INFO 140428234315584] Loss (name: value) logppx: 6.36119400454\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:23:52 INFO 140428234315584] #quality_metric: host=algo-1, epoch=24, train total_loss <loss>=6.36119400454\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:23:52 INFO 140428234315584] patience losses:[6.3637985992925774, 6.3620918338162911, 6.3610591923016955] min patience loss:6.3610591923 current loss:6.36119400454 absolute loss difference:0.000134812241392\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:23:52 INFO 140428234315584] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:23:52 INFO 140428234315584] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 23160, \"sum\": 23160.0, \"min\": 23160}, \"Total Records Seen\": {\"count\": 1, \"max\": 5926032, \"sum\": 5926032.0, \"min\": 5926032}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 48, \"sum\": 48.0, \"min\": 48}}, \"EndTime\": 1527888232.814711, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 23}, \"StartTime\": 1527888214.405337}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:23:52 INFO 140428234315584] #throughput_metric: host=algo-1, train throughput=13412.4988282 records/second\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:23:52 INFO 140428234315584] \u001b[0m\n",
      "\u001b[31m[06/01/2018 21:23:52 INFO 140428234315584] # Starting training for epoch 25\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:24:11 INFO 140428234315584] # Finished training epoch 25 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:24:11 INFO 140428234315584] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:24:11 INFO 140428234315584] Loss (name: value) total: 6.36029328435\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:24:11 INFO 140428234315584] Loss (name: value) kld: 0.152448614521\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:24:11 INFO 140428234315584] Loss (name: value) recons: 6.20784465859\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:24:11 INFO 140428234315584] Loss (name: value) logppx: 6.36029328435\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:24:11 INFO 140428234315584] #quality_metric: host=algo-1, epoch=25, train total_loss <loss>=6.36029328435\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:24:11 INFO 140428234315584] patience losses:[6.3620918338162911, 6.3610591923016955, 6.361194004543087] min patience loss:6.3610591923 current loss:6.36029328435 absolute loss difference:0.000765907949734\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:24:11 INFO 140428234315584] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:24:11 INFO 140428234315584] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 24125, \"sum\": 24125.0, \"min\": 24125}, \"Total Records Seen\": {\"count\": 1, \"max\": 6172950, \"sum\": 6172950.0, \"min\": 6172950}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 50, \"sum\": 50.0, \"min\": 50}}, \"EndTime\": 1527888251.726338, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 24}, \"StartTime\": 1527888232.815032}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:24:11 INFO 140428234315584] #throughput_metric: host=algo-1, train throughput=13056.5344314 records/second\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:24:11 INFO 140428234315584] \u001b[0m\n",
      "\u001b[31m[06/01/2018 21:24:11 INFO 140428234315584] # Starting training for epoch 26\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:24:31 INFO 140428234315584] # Finished training epoch 26 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:24:31 INFO 140428234315584] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:24:31 INFO 140428234315584] Loss (name: value) total: 6.35949846105\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:24:31 INFO 140428234315584] Loss (name: value) kld: 0.152618754219\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:24:31 INFO 140428234315584] Loss (name: value) recons: 6.20687970399\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:24:31 INFO 140428234315584] Loss (name: value) logppx: 6.35949846105\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:24:31 INFO 140428234315584] #quality_metric: host=algo-1, epoch=26, train total_loss <loss>=6.35949846105\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:24:31 INFO 140428234315584] patience losses:[6.3610591923016955, 6.361194004543087, 6.360293284351962] min patience loss:6.36029328435 current loss:6.35949846105 absolute loss difference:0.000794823305595\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:24:31 INFO 140428234315584] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:24:31 INFO 140428234315584] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 25090, \"sum\": 25090.0, \"min\": 25090}, \"Total Records Seen\": {\"count\": 1, \"max\": 6419868, \"sum\": 6419868.0, \"min\": 6419868}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 52, \"sum\": 52.0, \"min\": 52}}, \"EndTime\": 1527888271.099217, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 25}, \"StartTime\": 1527888251.727583}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:24:31 INFO 140428234315584] #throughput_metric: host=algo-1, train throughput=12746.2763781 records/second\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:24:31 INFO 140428234315584] \u001b[0m\n",
      "\u001b[31m[06/01/2018 21:24:31 INFO 140428234315584] # Starting training for epoch 27\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:24:49 INFO 140428234315584] # Finished training epoch 27 on 246918 examples from 965 batches, each of size 256.\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:24:49 INFO 140428234315584] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:24:49 INFO 140428234315584] Loss (name: value) total: 6.35894299468\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:24:49 INFO 140428234315584] Loss (name: value) kld: 0.154033871559\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:24:49 INFO 140428234315584] Loss (name: value) recons: 6.20490912008\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:24:49 INFO 140428234315584] Loss (name: value) logppx: 6.35894299468\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:24:49 INFO 140428234315584] #quality_metric: host=algo-1, epoch=27, train total_loss <loss>=6.35894299468\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:24:49 INFO 140428234315584] patience losses:[6.361194004543087, 6.360293284351962, 6.3594984610463667] min patience loss:6.35949846105 current loss:6.35894299468 absolute loss difference:0.000555466370261\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:24:49 INFO 140428234315584] Bad epoch: loss has not improved (enough). Bad count:4\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:24:49 INFO 140428234315584] Bad epochs exceeded patience. Stopping training early!\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:24:49 INFO 140428234315584] Early stop condition met. Stopping training.\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:24:49 INFO 140428234315584] #progress_metric: host=algo-1, completed 100 % epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 965, \"sum\": 965.0, \"min\": 965}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Total Batches Seen\": {\"count\": 1, \"max\": 26055, \"sum\": 26055.0, \"min\": 26055}, \"Total Records Seen\": {\"count\": 1, \"max\": 6666786, \"sum\": 6666786.0, \"min\": 6666786}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 246918, \"sum\": 246918.0, \"min\": 246918}, \"Reset Count\": {\"count\": 1, \"max\": 54, \"sum\": 54.0, \"min\": 54}}, \"EndTime\": 1527888289.522629, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 26}, \"StartTime\": 1527888271.099586}\n",
      "\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:24:49 INFO 140428234315584] #throughput_metric: host=algo-1, train throughput=13402.556856 records/second\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:24:49 WARNING 140428234315584] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:24:49 INFO 140428234315584] Best model based on early stopping at epoch 27. Best loss: 6.35894299468\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:24:49 INFO 140428234315584] Topics from epoch:final (num_topics:20):\u001b[0m\n",
      "\u001b[31m726 957 448 994 677 789 909 405 559 324 180 982 912 718 943 348 397 283 865 181\u001b[0m\n",
      "\u001b[31m957 448 726 909 789 982 994 397 930 943 241 324 87 181 415 677 550 597 495 708\u001b[0m\n",
      "\u001b[31m957 726 909 448 994 677 72 181 982 449 912 335 789 734 496 405 128 397 264 920\u001b[0m\n",
      "\u001b[31m957 448 382 726 544 566 994 621 589 264 891 405 449 937 460 245 714 496 912 211\u001b[0m\n",
      "\u001b[31m957 726 128 397 324 181 405 865 448 269 87 930 789 862 485 180 241 295 982 72\u001b[0m\n",
      "\u001b[31m891 280 604 838 382 989 567 342 589 448 937 460 642 106 957 157 438 562 45 113\u001b[0m\n",
      "\u001b[31m957 726 448 994 982 382 912 496 677 718 909 674 377 472 167 419 913 559 900 943\u001b[0m\n",
      "\u001b[31m150 117 119 333 332 904 469 692 281 296 250 878 286 921 658 405 145 585 401 608\u001b[0m\n",
      "\u001b[31m891 621 520 702 411 280 157 994 382 567 496 838 912 714 937 612 642 604 113 562\u001b[0m\n",
      "\u001b[31m642 937 891 702 239 64 604 382 157 498 946 386 162 989 878 280 227 106 411 520\u001b[0m\n",
      "\u001b[31m957 726 448 994 909 789 912 405 496 449 657 900 982 74 181 437 426 235 249 865\u001b[0m\n",
      "\u001b[31m957 128 397 943 865 982 994 677 726 448 426 789 181 269 324 718 415 485 930 241\u001b[0m\n",
      "\u001b[31m405 957 982 726 448 994 677 566 789 397 496 865 746 72 943 737 181 235 912 87\u001b[0m\n",
      "\u001b[31m742 368 213 67 280 113 51 716 926 108 767 382 133 392 672 572 891 460 76 567\u001b[0m\n",
      "\u001b[31m957 382 891 937 642 411 989 761 838 994 702 106 405 113 280 912 460 621 162 358\u001b[0m\n",
      "\u001b[31m726 405 957 994 448 677 982 789 909 496 900 912 72 566 718 358 397 449 415 128\u001b[0m\n",
      "\u001b[31m957 280 382 726 327 912 106 411 642 994 937 621 838 604 157 702 544 982 468 496\u001b[0m\n",
      "\u001b[31m743 710 954 589 217 156 544 592 820 804 856 546 739 596 585 52 800 304 862 892\u001b[0m\n",
      "\u001b[31m692 119 333 332 150 117 469 904 935 27 81 250 862 281 269 201 982 805 185 316\u001b[0m\n",
      "\u001b[31m957 909 448 726 912 677 865 994 789 397 496 72 405 449 181 982 324 225 283 415\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:24:49 INFO 140428234315584] Serializing model to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:24:49 INFO 140428234315584] Saved checkpoint to \"/tmp/tmpy43Yck/state-0001.params\"\u001b[0m\n",
      "\u001b[31m[06/01/2018 21:24:49 INFO 140428234315584] Test data is not provided.\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 499354.8469543457, \"sum\": 499354.8469543457, \"min\": 499354.8469543457}, \"finalize.time\": {\"count\": 1, \"max\": 70.25790214538574, \"sum\": 70.25790214538574, \"min\": 70.25790214538574}, \"initialize.time\": {\"count\": 1, \"max\": 44.83294486999512, \"sum\": 44.83294486999512, \"min\": 44.83294486999512}, \"model.serialize.time\": {\"count\": 1, \"max\": 2.5169849395751953, \"sum\": 2.5169849395751953, \"min\": 2.5169849395751953}, \"setuptime\": {\"count\": 1, \"max\": 14.004945755004883, \"sum\": 14.004945755004883, \"min\": 14.004945755004883}, \"early_stop.time\": {\"count\": 27, \"max\": 4.5299530029296875, \"sum\": 99.97773170471191, \"min\": 0.22292137145996094}, \"update.time\": {\"count\": 27, \"max\": 19371.48690223694, \"sum\": 499147.2840309143, \"min\": 18354.482889175415}, \"epochs\": {\"count\": 1, \"max\": 50, \"sum\": 50.0, \"min\": 50}}, \"EndTime\": 1527888289.597838, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1527887790.279425}\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Job Complete =====\n",
      "Billable seconds: 593\n"
     ]
    }
   ],
   "source": [
    "ntm.fit({'train': training_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "A trained model does nothing on its own. We now want to use the model to perform inference. For this example, that means predicting the topic mixture representing a given document or comment.\n",
    "\n",
    "This is simplified by the deploy function provided by the Amazon SageMaker Python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: ntm-2018-05-23-14-49-44-899\n",
      "INFO:sagemaker:Creating endpoint with name ntm-2018-05-23-14-34-25-654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "ntm_predictor = ntm.deploy(initial_instance_count=1,\n",
    "                           instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Inference\n",
    "\n",
    "With this real-time endpoint at our fingertips we can finally perform inference on our training and test data.  We should first discuss the meaning of the SageMaker NTM inference output.\n",
    "\n",
    "For each document we wish to compute its corresponding `topic_weights`. Each set of topic weights is a probability distribution over the number of topics, which is 5 in this example. Of the 5 topics discovered during NTM training each element of the topic weights is the proportion to which the input document is represented by the corresponding topic.\n",
    "\n",
    "For example, if the topic weights of an input document $\\mathbf{w}$ is,\n",
    "\n",
    "$$\\theta = \\left[ 0.3, 0.2, 0, 0.5, 0 \\right]$$\n",
    "\n",
    "then $\\mathbf{w}$ is 30% generated from Topic #1, 20% from Topic #2, and 50% from Topic #4. Below, we compute the topic mixtures for the first ten traning documents.\n",
    "\n",
    "First, we setup our serializes and deserializers which allow us to convert NumPy arrays to CSV strings which we can pass into our HTTP POST request to our hosted endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "ntm_predictor.content_type = 'text/csv'\n",
    "ntm_predictor.serializer = csv_serializer\n",
    "ntm_predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "test_comments = np.load('test_comments.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03881462 0.0330111  0.02953554 0.44098815 0.03035911 0.0249091\n",
      " 0.01516498 0.02849843 0.02895007 0.02496313 0.02657804 0.0277223\n",
      " 0.02970661 0.0321646  0.03495577 0.03026009 0.0334294  0.02988204\n",
      " 0.03124016 0.02886663]\n"
     ]
    }
   ],
   "source": [
    "results = ntm_predictor.predict(test_comments)\n",
    "predictions = np.array([prediction['topic_weights'] for prediction in results['predictions']])\n",
    "print(predictions[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 0, 3, 3, 3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint with name: ntm-2018-05-22-22-03-44-121\n"
     ]
    }
   ],
   "source": [
    "sagemaker.Session().delete_endpoint(ntm_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
