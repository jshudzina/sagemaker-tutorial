{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "***\n",
    "\n",
    "Amazon SageMaker NTM (Neural Topic Model) is an unsupervised learning algorithm that attempts to describe a set of observations as a mixture of distinct categories. NTM is most commonly used to discover a user-specified number of topics shared by documents within a text corpus. Here each observation is a document, the features are the presence (or occurrence count) of each word, and the categories are the topics. Since the method is unsupervised, the topics are not specified up front, and are not guaranteed to align with how a human may naturally categorize documents. The topics are learned as a probability distribution over the words that occur in each document. Each document, in turn, is described as a mixture of topics.\n",
    "\n",
    "In this notebook we will use the Amazon SageMaker NTM algorithm to train a model on some example synthetic data. We will then use this model to classify (perform inference on) the data. The main goals of this notebook are to,\n",
    "\n",
    "* create an AWS SageMaker training job on a data set to produce a NTM model,\n",
    "* use the model to perform inference with an Amazon SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "sagemaker_session._default_bucket = 'scw-use1-cors-test-3'\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Once the data is preprocessed and available in a recommended format the next step is to train our model on the data. There are number of parameters required by the NTM algorithm to configure the model and define the computational environment in which training will take place. The first of these is to point to a container image which holds the algorithms training and hosting code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "containers = {'us-west-2': '174872318107.dkr.ecr.us-west-2.amazonaws.com/ntm:latest',\n",
    "              'us-east-1': '382416733822.dkr.ecr.us-east-1.amazonaws.com/ntm:latest',\n",
    "              'us-east-2': '404615174143.dkr.ecr.us-east-2.amazonaws.com/ntm:latest',\n",
    "              'eu-west-1': '438346466558.dkr.ecr.eu-west-1.amazonaws.com/ntm:latest'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An NTM model uses the following hyperparameters:\n",
    "\n",
    "- **num_topics** - The number of topics or categories in the NTM model. \n",
    "- **feature_dim** - The size of the \"vocabulary\". In this case, this has been set to 1000 by the nytimes pyspark data prep.\n",
    "\n",
    "In addition to these NTM model hyperparameters, we provide additional parameters defining things like the EC2 instance type on which training will run, the S3 bucket containing the data, and the AWS access role.\n",
    "\n",
    "> Note: Try adjusting the mini_batch_size if running on a GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics=20\n",
    "vocabulary_size=1000\n",
    "\n",
    "ntm = sagemaker.estimator.Estimator(containers[boto3.Session().region_name],\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.p3.2xlarge',\n",
    "                                    output_path='s3://{}/data/nytimes-model/sagemaker-ntm'.format('scw-use1-cors-test-3'),\n",
    "                                    sagemaker_session=sagemaker_session)\n",
    "\n",
    "ntm.set_hyperparameters(num_topics=num_topics,\n",
    "                        feature_dim=vocabulary_size,\n",
    "                        mini_batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll train against the bag-of-words extracted from the NY Times comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: ntm-2018-05-23-14-34-25-654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......................\n",
      "\u001b[31mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:07 INFO 140609680480064] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/default-input.json: {u'num_patience_epochs': u'3', u'clip_gradient': u'Inf', u'encoder_layers': u'auto', u'optimizer': u'adadelta', u'_kvstore': u'auto', u'rescale_gradient': u'1.0', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'learning_rate': u'0.01', u'_data_format': u'record', u'epochs': u'50', u'weight_decay': u'0.0', u'_num_kv_servers': u'auto', u'encoder_layers_activation': u'sigmoid', u'mini_batch_size': u'256', u'tolerance': u'0.001', u'batch_norm': u'false'}\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:07 INFO 140609680480064] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'feature_dim': u'1000', u'mini_batch_size': u'1024', u'num_topics': u'20'}\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:07 INFO 140609680480064] Final configuration: {u'num_patience_epochs': u'3', u'clip_gradient': u'Inf', u'encoder_layers': u'auto', u'optimizer': u'adadelta', u'_kvstore': u'auto', u'rescale_gradient': u'1.0', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'learning_rate': u'0.01', u'_data_format': u'record', u'epochs': u'50', u'feature_dim': u'1000', u'weight_decay': u'0.0', u'num_topics': u'20', u'_num_kv_servers': u'auto', u'encoder_layers_activation': u'sigmoid', u'mini_batch_size': u'1024', u'tolerance': u'0.001', u'batch_norm': u'false'}\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:07 INFO 140609680480064] Using default worker.\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:07 INFO 140609680480064] Initializing\u001b[0m\n",
      "\u001b[31m/opt/amazon/lib/python2.7/site-packages/ai_algorithms_sdk/config/config_helper.py:122: DeprecationWarning: deprecated\n",
      "  warnings.warn(\"deprecated\", DeprecationWarning)\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:07 INFO 140609680480064] nvidia-smi took: 0.0503599643707 secs to identify 1 gpus\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:07 INFO 140609680480064] Number of GPUs being used: 1\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:18 INFO 140609680480064] Create Store: local\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}}, \"EndTime\": 1527086298.081651, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1527086298.081608}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:18 INFO 140609680480064] \u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:18 INFO 140609680480064] # Starting training for epoch 1\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:22 INFO 140609680480064] # Finished training epoch 1 on 260977 examples from 255 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:22 INFO 140609680480064] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:22 INFO 140609680480064] Loss (name: value) total: 6.36253313551\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:22 INFO 140609680480064] Loss (name: value) kld: 0.0121664107672\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:22 INFO 140609680480064] Loss (name: value) recons: 6.35036671769\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:22 INFO 140609680480064] Loss (name: value) logppx: 6.36253313551\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:22 INFO 140609680480064] #quality_metric: host=algo-1, epoch=1, train total_loss <loss>=6.36253313551\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:22 INFO 140609680480064] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Total Batches Seen\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Total Records Seen\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1527086302.152211, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 0}, \"StartTime\": 1527086302.152164}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:22 INFO 140609680480064] #throughput_metric: host=algo-1, train throughput=64116.3149612 records/second\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:22 INFO 140609680480064] \u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:22 INFO 140609680480064] # Starting training for epoch 2\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:26 INFO 140609680480064] # Finished training epoch 2 on 260977 examples from 255 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:26 INFO 140609680480064] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:26 INFO 140609680480064] Loss (name: value) total: 6.32032002468\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:26 INFO 140609680480064] Loss (name: value) kld: 0.0163957058543\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:26 INFO 140609680480064] Loss (name: value) recons: 6.30392433241\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:26 INFO 140609680480064] Loss (name: value) logppx: 6.32032002468\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:26 INFO 140609680480064] #quality_metric: host=algo-1, epoch=2, train total_loss <loss>=6.32032002468\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:26 INFO 140609680480064] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Total Batches Seen\": {\"count\": 1, \"max\": 510, \"sum\": 510.0, \"min\": 510}, \"Total Records Seen\": {\"count\": 1, \"max\": 521954, \"sum\": 521954.0, \"min\": 521954}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Reset Count\": {\"count\": 1, \"max\": 4, \"sum\": 4.0, \"min\": 4}}, \"EndTime\": 1527086306.33, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 1}, \"StartTime\": 1527086306.32995}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:26 INFO 140609680480064] #throughput_metric: host=algo-1, train throughput=62470.3197795 records/second\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:26 INFO 140609680480064] \u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:26 INFO 140609680480064] # Starting training for epoch 3\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:30 INFO 140609680480064] # Finished training epoch 3 on 260977 examples from 255 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:30 INFO 140609680480064] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:30 INFO 140609680480064] Loss (name: value) total: 6.30131544786\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:30 INFO 140609680480064] Loss (name: value) kld: 0.0278681081409\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:30 INFO 140609680480064] Loss (name: value) recons: 6.27344731537\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:30 INFO 140609680480064] Loss (name: value) logppx: 6.30131544786\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:30 INFO 140609680480064] #quality_metric: host=algo-1, epoch=3, train total_loss <loss>=6.30131544786\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:30 INFO 140609680480064] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Total Batches Seen\": {\"count\": 1, \"max\": 765, \"sum\": 765.0, \"min\": 765}, \"Total Records Seen\": {\"count\": 1, \"max\": 782931, \"sum\": 782931.0, \"min\": 782931}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Reset Count\": {\"count\": 1, \"max\": 6, \"sum\": 6.0, \"min\": 6}}, \"EndTime\": 1527086310.427911, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 2}, \"StartTime\": 1527086310.427848}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:30 INFO 140609680480064] #throughput_metric: host=algo-1, train throughput=63687.7967724 records/second\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:30 INFO 140609680480064] \u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:30 INFO 140609680480064] # Starting training for epoch 4\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[05/23/2018 14:38:34 INFO 140609680480064] # Finished training epoch 4 on 260977 examples from 255 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:34 INFO 140609680480064] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:34 INFO 140609680480064] Loss (name: value) total: 6.29366907793\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:34 INFO 140609680480064] Loss (name: value) kld: 0.036844597377\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:34 INFO 140609680480064] Loss (name: value) recons: 6.25682444853\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:34 INFO 140609680480064] Loss (name: value) logppx: 6.29366907793\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:34 INFO 140609680480064] #quality_metric: host=algo-1, epoch=4, train total_loss <loss>=6.29366907793\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:34 INFO 140609680480064] patience losses:[6.3625331355076211, 6.3203200246773514, 6.3013154478634101] min patience loss:6.30131544786 current loss:6.29366907793 absolute loss difference:0.00764636993408\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:34 INFO 140609680480064] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1020, \"sum\": 1020.0, \"min\": 1020}, \"Total Records Seen\": {\"count\": 1, \"max\": 1043908, \"sum\": 1043908.0, \"min\": 1043908}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Reset Count\": {\"count\": 1, \"max\": 8, \"sum\": 8.0, \"min\": 8}}, \"EndTime\": 1527086314.522541, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 3}, \"StartTime\": 1527086314.522483}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:34 INFO 140609680480064] #throughput_metric: host=algo-1, train throughput=63739.1486156 records/second\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:34 INFO 140609680480064] \u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:34 INFO 140609680480064] # Starting training for epoch 5\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:38 INFO 140609680480064] # Finished training epoch 5 on 260977 examples from 255 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:38 INFO 140609680480064] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:38 INFO 140609680480064] Loss (name: value) total: 6.29039756738\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:38 INFO 140609680480064] Loss (name: value) kld: 0.0394759045585\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:38 INFO 140609680480064] Loss (name: value) recons: 6.25092164395\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:38 INFO 140609680480064] Loss (name: value) logppx: 6.29039756738\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:38 INFO 140609680480064] #quality_metric: host=algo-1, epoch=5, train total_loss <loss>=6.29039756738\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:38 INFO 140609680480064] patience losses:[6.3203200246773514, 6.3013154478634101, 6.2936690779293283] min patience loss:6.29366907793 current loss:6.29039756738 absolute loss difference:0.00327151055429\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:38 INFO 140609680480064] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1275, \"sum\": 1275.0, \"min\": 1275}, \"Total Records Seen\": {\"count\": 1, \"max\": 1304885, \"sum\": 1304885.0, \"min\": 1304885}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Reset Count\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}}, \"EndTime\": 1527086318.527523, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 4}, \"StartTime\": 1527086318.52747}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:38 INFO 140609680480064] #throughput_metric: host=algo-1, train throughput=65165.7118984 records/second\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:38 INFO 140609680480064] \u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:38 INFO 140609680480064] # Starting training for epoch 6\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:42 INFO 140609680480064] # Finished training epoch 6 on 260977 examples from 255 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:42 INFO 140609680480064] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:42 INFO 140609680480064] Loss (name: value) total: 6.28683999753\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:42 INFO 140609680480064] Loss (name: value) kld: 0.0419280487387\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:42 INFO 140609680480064] Loss (name: value) recons: 6.24491191752\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:42 INFO 140609680480064] Loss (name: value) logppx: 6.28683999753\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:42 INFO 140609680480064] #quality_metric: host=algo-1, epoch=6, train total_loss <loss>=6.28683999753\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:42 INFO 140609680480064] patience losses:[6.3013154478634101, 6.2936690779293283, 6.2903975673750336] min patience loss:6.29039756738 current loss:6.28683999753 absolute loss difference:0.00355756984038\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:42 INFO 140609680480064] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1530, \"sum\": 1530.0, \"min\": 1530}, \"Total Records Seen\": {\"count\": 1, \"max\": 1565862, \"sum\": 1565862.0, \"min\": 1565862}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Reset Count\": {\"count\": 1, \"max\": 12, \"sum\": 12.0, \"min\": 12}}, \"EndTime\": 1527086322.641725, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 5}, \"StartTime\": 1527086322.641664}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:42 INFO 140609680480064] #throughput_metric: host=algo-1, train throughput=63435.1704412 records/second\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:42 INFO 140609680480064] \u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:42 INFO 140609680480064] # Starting training for epoch 7\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:46 INFO 140609680480064] # Finished training epoch 7 on 260977 examples from 255 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:46 INFO 140609680480064] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:46 INFO 140609680480064] Loss (name: value) total: 6.2806985051\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:46 INFO 140609680480064] Loss (name: value) kld: 0.0453369193217\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:46 INFO 140609680480064] Loss (name: value) recons: 6.2353615686\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:46 INFO 140609680480064] Loss (name: value) logppx: 6.2806985051\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:46 INFO 140609680480064] #quality_metric: host=algo-1, epoch=7, train total_loss <loss>=6.2806985051\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:46 INFO 140609680480064] patience losses:[6.2936690779293283, 6.2903975673750336, 6.2868399975346581] min patience loss:6.28683999753 current loss:6.2806985051 absolute loss difference:0.00614149243224\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:46 INFO 140609680480064] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1785, \"sum\": 1785.0, \"min\": 1785}, \"Total Records Seen\": {\"count\": 1, \"max\": 1826839, \"sum\": 1826839.0, \"min\": 1826839}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Reset Count\": {\"count\": 1, \"max\": 14, \"sum\": 14.0, \"min\": 14}}, \"EndTime\": 1527086326.719727, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 6}, \"StartTime\": 1527086326.719682}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:46 INFO 140609680480064] #throughput_metric: host=algo-1, train throughput=63999.5085812 records/second\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:46 INFO 140609680480064] \u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:46 INFO 140609680480064] # Starting training for epoch 8\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[05/23/2018 14:38:50 INFO 140609680480064] # Finished training epoch 8 on 260977 examples from 255 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:50 INFO 140609680480064] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:50 INFO 140609680480064] Loss (name: value) total: 6.27092050852\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:50 INFO 140609680480064] Loss (name: value) kld: 0.0512845783841\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:50 INFO 140609680480064] Loss (name: value) recons: 6.21963592043\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:50 INFO 140609680480064] Loss (name: value) logppx: 6.27092050852\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:50 INFO 140609680480064] #quality_metric: host=algo-1, epoch=8, train total_loss <loss>=6.27092050852\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:50 INFO 140609680480064] patience losses:[6.2903975673750336, 6.2868399975346581, 6.2806985051024196] min patience loss:6.2806985051 current loss:6.27092050852 absolute loss difference:0.00977799658682\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:50 INFO 140609680480064] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2040, \"sum\": 2040.0, \"min\": 2040}, \"Total Records Seen\": {\"count\": 1, \"max\": 2087816, \"sum\": 2087816.0, \"min\": 2087816}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Reset Count\": {\"count\": 1, \"max\": 16, \"sum\": 16.0, \"min\": 16}}, \"EndTime\": 1527086330.744994, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 7}, \"StartTime\": 1527086330.744951}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:50 INFO 140609680480064] #throughput_metric: host=algo-1, train throughput=64837.7833549 records/second\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:50 INFO 140609680480064] \u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:50 INFO 140609680480064] # Starting training for epoch 9\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:54 INFO 140609680480064] # Finished training epoch 9 on 260977 examples from 255 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:54 INFO 140609680480064] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:54 INFO 140609680480064] Loss (name: value) total: 6.25827541912\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:54 INFO 140609680480064] Loss (name: value) kld: 0.062684951635\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:54 INFO 140609680480064] Loss (name: value) recons: 6.19559044183\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:54 INFO 140609680480064] Loss (name: value) logppx: 6.25827541912\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:54 INFO 140609680480064] #quality_metric: host=algo-1, epoch=9, train total_loss <loss>=6.25827541912\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:54 INFO 140609680480064] patience losses:[6.2868399975346581, 6.2806985051024196, 6.2709205085156015] min patience loss:6.27092050852 current loss:6.25827541912 absolute loss difference:0.0126450893926\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:54 INFO 140609680480064] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2295, \"sum\": 2295.0, \"min\": 2295}, \"Total Records Seen\": {\"count\": 1, \"max\": 2348793, \"sum\": 2348793.0, \"min\": 2348793}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Reset Count\": {\"count\": 1, \"max\": 18, \"sum\": 18.0, \"min\": 18}}, \"EndTime\": 1527086334.903905, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 8}, \"StartTime\": 1527086334.903849}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:54 INFO 140609680480064] #throughput_metric: host=algo-1, train throughput=62754.5098739 records/second\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:54 INFO 140609680480064] \u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:54 INFO 140609680480064] # Starting training for epoch 10\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:59 INFO 140609680480064] # Finished training epoch 10 on 260977 examples from 255 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:59 INFO 140609680480064] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:59 INFO 140609680480064] Loss (name: value) total: 6.24983245251\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:59 INFO 140609680480064] Loss (name: value) kld: 0.0700775601992\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:59 INFO 140609680480064] Loss (name: value) recons: 6.17975490794\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:59 INFO 140609680480064] Loss (name: value) logppx: 6.24983245251\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:59 INFO 140609680480064] #quality_metric: host=algo-1, epoch=10, train total_loss <loss>=6.24983245251\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:59 INFO 140609680480064] patience losses:[6.2806985051024196, 6.2709205085156015, 6.2582754191230325] min patience loss:6.25827541912 current loss:6.24983245251 absolute loss difference:0.00844296661078\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:59 INFO 140609680480064] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2550, \"sum\": 2550.0, \"min\": 2550}, \"Total Records Seen\": {\"count\": 1, \"max\": 2609770, \"sum\": 2609770.0, \"min\": 2609770}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Reset Count\": {\"count\": 1, \"max\": 20, \"sum\": 20.0, \"min\": 20}}, \"EndTime\": 1527086339.011849, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 9}, \"StartTime\": 1527086339.011788}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:59 INFO 140609680480064] #throughput_metric: host=algo-1, train throughput=63532.245223 records/second\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:59 INFO 140609680480064] \u001b[0m\n",
      "\u001b[31m[05/23/2018 14:38:59 INFO 140609680480064] # Starting training for epoch 11\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:03 INFO 140609680480064] # Finished training epoch 11 on 260977 examples from 255 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:03 INFO 140609680480064] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:03 INFO 140609680480064] Loss (name: value) total: 6.24512306662\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:03 INFO 140609680480064] Loss (name: value) kld: 0.0748007547622\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:03 INFO 140609680480064] Loss (name: value) recons: 6.17032232098\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:03 INFO 140609680480064] Loss (name: value) logppx: 6.24512306662\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:03 INFO 140609680480064] #quality_metric: host=algo-1, epoch=11, train total_loss <loss>=6.24512306662\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:03 INFO 140609680480064] patience losses:[6.2709205085156015, 6.2582754191230325, 6.2498324525122548] min patience loss:6.24983245251 current loss:6.24512306662 absolute loss difference:0.00470938589059\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:03 INFO 140609680480064] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2805, \"sum\": 2805.0, \"min\": 2805}, \"Total Records Seen\": {\"count\": 1, \"max\": 2870747, \"sum\": 2870747.0, \"min\": 2870747}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Reset Count\": {\"count\": 1, \"max\": 22, \"sum\": 22.0, \"min\": 22}}, \"EndTime\": 1527086343.074943, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 10}, \"StartTime\": 1527086343.074904}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:03 INFO 140609680480064] #throughput_metric: host=algo-1, train throughput=64234.0724289 records/second\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:03 INFO 140609680480064] \u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:03 INFO 140609680480064] # Starting training for epoch 12\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:07 INFO 140609680480064] # Finished training epoch 12 on 260977 examples from 255 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:07 INFO 140609680480064] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:07 INFO 140609680480064] Loss (name: value) total: 6.24232428682\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:07 INFO 140609680480064] Loss (name: value) kld: 0.0776756572665\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:07 INFO 140609680480064] Loss (name: value) recons: 6.16464864693\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:07 INFO 140609680480064] Loss (name: value) logppx: 6.24232428682\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:07 INFO 140609680480064] #quality_metric: host=algo-1, epoch=12, train total_loss <loss>=6.24232428682\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:07 INFO 140609680480064] patience losses:[6.2582754191230325, 6.2498324525122548, 6.2451230666216686] min patience loss:6.24512306662 current loss:6.24232428682 absolute loss difference:0.0027987798055\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:07 INFO 140609680480064] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3060, \"sum\": 3060.0, \"min\": 3060}, \"Total Records Seen\": {\"count\": 1, \"max\": 3131724, \"sum\": 3131724.0, \"min\": 3131724}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Reset Count\": {\"count\": 1, \"max\": 24, \"sum\": 24.0, \"min\": 24}}, \"EndTime\": 1527086347.134796, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 11}, \"StartTime\": 1527086347.134736}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:07 INFO 140609680480064] #throughput_metric: host=algo-1, train throughput=64284.6708153 records/second\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:07 INFO 140609680480064] \u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:07 INFO 140609680480064] # Starting training for epoch 13\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:11 INFO 140609680480064] # Finished training epoch 13 on 260977 examples from 255 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:11 INFO 140609680480064] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:11 INFO 140609680480064] Loss (name: value) total: 6.23998605317\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:11 INFO 140609680480064] Loss (name: value) kld: 0.0802632138279\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:11 INFO 140609680480064] Loss (name: value) recons: 6.15972286299\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:11 INFO 140609680480064] Loss (name: value) logppx: 6.23998605317\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:11 INFO 140609680480064] #quality_metric: host=algo-1, epoch=13, train total_loss <loss>=6.23998605317\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:11 INFO 140609680480064] patience losses:[6.2498324525122548, 6.2451230666216686, 6.2423242868161672] min patience loss:6.24232428682 current loss:6.23998605317 absolute loss difference:0.00233823364856\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:11 INFO 140609680480064] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3315, \"sum\": 3315.0, \"min\": 3315}, \"Total Records Seen\": {\"count\": 1, \"max\": 3392701, \"sum\": 3392701.0, \"min\": 3392701}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Reset Count\": {\"count\": 1, \"max\": 26, \"sum\": 26.0, \"min\": 26}}, \"EndTime\": 1527086351.272328, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 12}, \"StartTime\": 1527086351.272268}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:11 INFO 140609680480064] #throughput_metric: host=algo-1, train throughput=63078.0868288 records/second\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:11 INFO 140609680480064] \u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:11 INFO 140609680480064] # Starting training for epoch 14\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[05/23/2018 14:39:15 INFO 140609680480064] # Finished training epoch 14 on 260977 examples from 255 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:15 INFO 140609680480064] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:15 INFO 140609680480064] Loss (name: value) total: 6.23776557773\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:15 INFO 140609680480064] Loss (name: value) kld: 0.08228455817\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:15 INFO 140609680480064] Loss (name: value) recons: 6.15548101687\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:15 INFO 140609680480064] Loss (name: value) logppx: 6.23776557773\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:15 INFO 140609680480064] #quality_metric: host=algo-1, epoch=14, train total_loss <loss>=6.23776557773\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:15 INFO 140609680480064] patience losses:[6.2451230666216686, 6.2423242868161672, 6.2399860531676046] min patience loss:6.23998605317 current loss:6.23776557773 absolute loss difference:0.00222047543993\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:15 INFO 140609680480064] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3570, \"sum\": 3570.0, \"min\": 3570}, \"Total Records Seen\": {\"count\": 1, \"max\": 3653678, \"sum\": 3653678.0, \"min\": 3653678}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Reset Count\": {\"count\": 1, \"max\": 28, \"sum\": 28.0, \"min\": 28}}, \"EndTime\": 1527086355.388524, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 13}, \"StartTime\": 1527086355.388475}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:15 INFO 140609680480064] #throughput_metric: host=algo-1, train throughput=63405.5211194 records/second\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:15 INFO 140609680480064] \u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:15 INFO 140609680480064] # Starting training for epoch 15\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:19 INFO 140609680480064] # Finished training epoch 15 on 260977 examples from 255 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:19 INFO 140609680480064] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:19 INFO 140609680480064] Loss (name: value) total: 6.236746472\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:19 INFO 140609680480064] Loss (name: value) kld: 0.0837325848639\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:19 INFO 140609680480064] Loss (name: value) recons: 6.15301392873\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:19 INFO 140609680480064] Loss (name: value) logppx: 6.236746472\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:19 INFO 140609680480064] #quality_metric: host=algo-1, epoch=15, train total_loss <loss>=6.236746472\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:19 INFO 140609680480064] patience losses:[6.2423242868161672, 6.2399860531676046, 6.2377655777276733] min patience loss:6.23776557773 current loss:6.236746472 absolute loss difference:0.00101910572426\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:19 INFO 140609680480064] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Total Batches Seen\": {\"count\": 1, \"max\": 3825, \"sum\": 3825.0, \"min\": 3825}, \"Total Records Seen\": {\"count\": 1, \"max\": 3914655, \"sum\": 3914655.0, \"min\": 3914655}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Reset Count\": {\"count\": 1, \"max\": 30, \"sum\": 30.0, \"min\": 30}}, \"EndTime\": 1527086359.550178, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 14}, \"StartTime\": 1527086359.55013}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:19 INFO 140609680480064] #throughput_metric: host=algo-1, train throughput=62712.6962324 records/second\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:19 INFO 140609680480064] \u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:19 INFO 140609680480064] # Starting training for epoch 16\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:23 INFO 140609680480064] # Finished training epoch 16 on 260977 examples from 255 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:23 INFO 140609680480064] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:23 INFO 140609680480064] Loss (name: value) total: 6.23473310471\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:23 INFO 140609680480064] Loss (name: value) kld: 0.0852585308108\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:23 INFO 140609680480064] Loss (name: value) recons: 6.14947454602\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:23 INFO 140609680480064] Loss (name: value) logppx: 6.23473310471\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:23 INFO 140609680480064] #quality_metric: host=algo-1, epoch=16, train total_loss <loss>=6.23473310471\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:23 INFO 140609680480064] patience losses:[6.2399860531676046, 6.2377655777276733, 6.2367464720034134] min patience loss:6.236746472 current loss:6.23473310471 absolute loss difference:0.0020133672976\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:23 INFO 140609680480064] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Total Batches Seen\": {\"count\": 1, \"max\": 4080, \"sum\": 4080.0, \"min\": 4080}, \"Total Records Seen\": {\"count\": 1, \"max\": 4175632, \"sum\": 4175632.0, \"min\": 4175632}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Reset Count\": {\"count\": 1, \"max\": 32, \"sum\": 32.0, \"min\": 32}}, \"EndTime\": 1527086363.657038, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 15}, \"StartTime\": 1527086363.656974}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:23 INFO 140609680480064] #throughput_metric: host=algo-1, train throughput=63549.3743629 records/second\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:23 INFO 140609680480064] \u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:23 INFO 140609680480064] # Starting training for epoch 17\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:27 INFO 140609680480064] # Finished training epoch 17 on 260977 examples from 255 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:27 INFO 140609680480064] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:27 INFO 140609680480064] Loss (name: value) total: 6.23309197145\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:27 INFO 140609680480064] Loss (name: value) kld: 0.0878059055729\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:27 INFO 140609680480064] Loss (name: value) recons: 6.14528604769\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:27 INFO 140609680480064] Loss (name: value) logppx: 6.23309197145\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:27 INFO 140609680480064] #quality_metric: host=algo-1, epoch=17, train total_loss <loss>=6.23309197145\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:27 INFO 140609680480064] patience losses:[6.2377655777276733, 6.2367464720034134, 6.2347331047058105] min patience loss:6.23473310471 current loss:6.23309197145 absolute loss difference:0.00164113325231\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:27 INFO 140609680480064] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Total Batches Seen\": {\"count\": 1, \"max\": 4335, \"sum\": 4335.0, \"min\": 4335}, \"Total Records Seen\": {\"count\": 1, \"max\": 4436609, \"sum\": 4436609.0, \"min\": 4436609}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Reset Count\": {\"count\": 1, \"max\": 34, \"sum\": 34.0, \"min\": 34}}, \"EndTime\": 1527086367.730131, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 16}, \"StartTime\": 1527086367.730071}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:27 INFO 140609680480064] #throughput_metric: host=algo-1, train throughput=64076.2681405 records/second\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:27 INFO 140609680480064] \u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:27 INFO 140609680480064] # Starting training for epoch 18\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[05/23/2018 14:39:31 INFO 140609680480064] # Finished training epoch 18 on 260977 examples from 255 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:31 INFO 140609680480064] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:31 INFO 140609680480064] Loss (name: value) total: 6.22858788733\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:31 INFO 140609680480064] Loss (name: value) kld: 0.094045179907\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:31 INFO 140609680480064] Loss (name: value) recons: 6.13454268399\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:31 INFO 140609680480064] Loss (name: value) logppx: 6.22858788733\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:31 INFO 140609680480064] #quality_metric: host=algo-1, epoch=18, train total_loss <loss>=6.22858788733\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:31 INFO 140609680480064] patience losses:[6.2367464720034134, 6.2347331047058105, 6.233091971453498] min patience loss:6.23309197145 current loss:6.22858788733 absolute loss difference:0.00450408411961\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:31 INFO 140609680480064] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Total Batches Seen\": {\"count\": 1, \"max\": 4590, \"sum\": 4590.0, \"min\": 4590}, \"Total Records Seen\": {\"count\": 1, \"max\": 4697586, \"sum\": 4697586.0, \"min\": 4697586}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Reset Count\": {\"count\": 1, \"max\": 36, \"sum\": 36.0, \"min\": 36}}, \"EndTime\": 1527086371.779651, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 17}, \"StartTime\": 1527086371.779589}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:31 INFO 140609680480064] #throughput_metric: host=algo-1, train throughput=64449.3933459 records/second\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:31 INFO 140609680480064] \u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:31 INFO 140609680480064] # Starting training for epoch 19\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:35 INFO 140609680480064] # Finished training epoch 19 on 260977 examples from 255 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:35 INFO 140609680480064] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:35 INFO 140609680480064] Loss (name: value) total: 6.22517355938\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:35 INFO 140609680480064] Loss (name: value) kld: 0.0983734835304\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:35 INFO 140609680480064] Loss (name: value) recons: 6.12680011076\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:35 INFO 140609680480064] Loss (name: value) logppx: 6.22517355938\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:35 INFO 140609680480064] #quality_metric: host=algo-1, epoch=19, train total_loss <loss>=6.22517355938\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:35 INFO 140609680480064] patience losses:[6.2347331047058105, 6.233091971453498, 6.2285878873338882] min patience loss:6.22858788733 current loss:6.22517355938 absolute loss difference:0.00341432795805\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:35 INFO 140609680480064] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Total Batches Seen\": {\"count\": 1, \"max\": 4845, \"sum\": 4845.0, \"min\": 4845}, \"Total Records Seen\": {\"count\": 1, \"max\": 4958563, \"sum\": 4958563.0, \"min\": 4958563}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Reset Count\": {\"count\": 1, \"max\": 38, \"sum\": 38.0, \"min\": 38}}, \"EndTime\": 1527086375.927252, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 18}, \"StartTime\": 1527086375.927192}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:35 INFO 140609680480064] #throughput_metric: host=algo-1, train throughput=62925.1448939 records/second\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:35 INFO 140609680480064] \u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:35 INFO 140609680480064] # Starting training for epoch 20\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:39 INFO 140609680480064] # Finished training epoch 20 on 260977 examples from 255 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:39 INFO 140609680480064] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:39 INFO 140609680480064] Loss (name: value) total: 6.22343820871\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:39 INFO 140609680480064] Loss (name: value) kld: 0.102268532415\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:39 INFO 140609680480064] Loss (name: value) recons: 6.12116967557\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:39 INFO 140609680480064] Loss (name: value) logppx: 6.22343820871\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:39 INFO 140609680480064] #quality_metric: host=algo-1, epoch=20, train total_loss <loss>=6.22343820871\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:39 INFO 140609680480064] patience losses:[6.233091971453498, 6.2285878873338882, 6.2251735593758379] min patience loss:6.22517355938 current loss:6.22343820871 absolute loss difference:0.00173535066492\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:39 INFO 140609680480064] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Total Batches Seen\": {\"count\": 1, \"max\": 5100, \"sum\": 5100.0, \"min\": 5100}, \"Total Records Seen\": {\"count\": 1, \"max\": 5219540, \"sum\": 5219540.0, \"min\": 5219540}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Reset Count\": {\"count\": 1, \"max\": 40, \"sum\": 40.0, \"min\": 40}}, \"EndTime\": 1527086379.99448, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 19}, \"StartTime\": 1527086379.994438}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:39 INFO 140609680480064] #throughput_metric: host=algo-1, train throughput=64168.9096523 records/second\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:39 INFO 140609680480064] \u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:39 INFO 140609680480064] # Starting training for epoch 21\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:44 INFO 140609680480064] # Finished training epoch 21 on 260977 examples from 255 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:44 INFO 140609680480064] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:44 INFO 140609680480064] Loss (name: value) total: 6.22098092846\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:44 INFO 140609680480064] Loss (name: value) kld: 0.105428003973\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:44 INFO 140609680480064] Loss (name: value) recons: 6.11555292466\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:44 INFO 140609680480064] Loss (name: value) logppx: 6.22098092846\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:44 INFO 140609680480064] #quality_metric: host=algo-1, epoch=21, train total_loss <loss>=6.22098092846\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:44 INFO 140609680480064] patience losses:[6.2285878873338882, 6.2251735593758379, 6.2234382087109132] min patience loss:6.22343820871 current loss:6.22098092846 absolute loss difference:0.00245728025249\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:44 INFO 140609680480064] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Total Batches Seen\": {\"count\": 1, \"max\": 5355, \"sum\": 5355.0, \"min\": 5355}, \"Total Records Seen\": {\"count\": 1, \"max\": 5480517, \"sum\": 5480517.0, \"min\": 5480517}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Reset Count\": {\"count\": 1, \"max\": 42, \"sum\": 42.0, \"min\": 42}}, \"EndTime\": 1527086384.08347, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 20}, \"StartTime\": 1527086384.08341}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:44 INFO 140609680480064] #throughput_metric: host=algo-1, train throughput=63826.6448757 records/second\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:44 INFO 140609680480064] \u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:44 INFO 140609680480064] # Starting training for epoch 22\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:48 INFO 140609680480064] # Finished training epoch 22 on 260977 examples from 255 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:48 INFO 140609680480064] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:48 INFO 140609680480064] Loss (name: value) total: 6.21845552407\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:48 INFO 140609680480064] Loss (name: value) kld: 0.108298257198\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:48 INFO 140609680480064] Loss (name: value) recons: 6.11015725042\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:48 INFO 140609680480064] Loss (name: value) logppx: 6.21845552407\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:48 INFO 140609680480064] #quality_metric: host=algo-1, epoch=22, train total_loss <loss>=6.21845552407\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:48 INFO 140609680480064] patience losses:[6.2251735593758379, 6.2234382087109132, 6.2209809284584194] min patience loss:6.22098092846 current loss:6.21845552407 absolute loss difference:0.00252540438783\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:48 INFO 140609680480064] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Total Batches Seen\": {\"count\": 1, \"max\": 5610, \"sum\": 5610.0, \"min\": 5610}, \"Total Records Seen\": {\"count\": 1, \"max\": 5741494, \"sum\": 5741494.0, \"min\": 5741494}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Reset Count\": {\"count\": 1, \"max\": 44, \"sum\": 44.0, \"min\": 44}}, \"EndTime\": 1527086388.184519, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 21}, \"StartTime\": 1527086388.18448}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:48 INFO 140609680480064] #throughput_metric: host=algo-1, train throughput=63640.367988 records/second\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:48 INFO 140609680480064] \u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:48 INFO 140609680480064] # Starting training for epoch 23\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[05/23/2018 14:39:52 INFO 140609680480064] # Finished training epoch 23 on 260977 examples from 255 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:52 INFO 140609680480064] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:52 INFO 140609680480064] Loss (name: value) total: 6.21752161138\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:52 INFO 140609680480064] Loss (name: value) kld: 0.110444693881\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:52 INFO 140609680480064] Loss (name: value) recons: 6.10707690669\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:52 INFO 140609680480064] Loss (name: value) logppx: 6.21752161138\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:52 INFO 140609680480064] #quality_metric: host=algo-1, epoch=23, train total_loss <loss>=6.21752161138\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:52 INFO 140609680480064] patience losses:[6.2234382087109132, 6.2209809284584194, 6.2184555240705901] min patience loss:6.21845552407 current loss:6.21752161138 absolute loss difference:0.000933912688611\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:52 INFO 140609680480064] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:52 INFO 140609680480064] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Total Batches Seen\": {\"count\": 1, \"max\": 5865, \"sum\": 5865.0, \"min\": 5865}, \"Total Records Seen\": {\"count\": 1, \"max\": 6002471, \"sum\": 6002471.0, \"min\": 6002471}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Reset Count\": {\"count\": 1, \"max\": 46, \"sum\": 46.0, \"min\": 46}}, \"EndTime\": 1527086392.379425, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 22}, \"StartTime\": 1527086392.379387}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:52 INFO 140609680480064] #throughput_metric: host=algo-1, train throughput=62215.5484301 records/second\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:52 INFO 140609680480064] \u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:52 INFO 140609680480064] # Starting training for epoch 24\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:56 INFO 140609680480064] # Finished training epoch 24 on 260977 examples from 255 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:56 INFO 140609680480064] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:56 INFO 140609680480064] Loss (name: value) total: 6.216461881\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:56 INFO 140609680480064] Loss (name: value) kld: 0.111814367976\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:56 INFO 140609680480064] Loss (name: value) recons: 6.10464748869\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:56 INFO 140609680480064] Loss (name: value) logppx: 6.216461881\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:56 INFO 140609680480064] #quality_metric: host=algo-1, epoch=24, train total_loss <loss>=6.216461881\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:56 INFO 140609680480064] patience losses:[6.2209809284584194, 6.2184555240705901, 6.2175216113819793] min patience loss:6.21752161138 current loss:6.216461881 absolute loss difference:0.00105973038019\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:56 INFO 140609680480064] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Total Batches Seen\": {\"count\": 1, \"max\": 6120, \"sum\": 6120.0, \"min\": 6120}, \"Total Records Seen\": {\"count\": 1, \"max\": 6263448, \"sum\": 6263448.0, \"min\": 6263448}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Reset Count\": {\"count\": 1, \"max\": 48, \"sum\": 48.0, \"min\": 48}}, \"EndTime\": 1527086396.388311, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 23}, \"StartTime\": 1527086396.388272}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:56 INFO 140609680480064] #throughput_metric: host=algo-1, train throughput=65102.0842759 records/second\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:56 INFO 140609680480064] \u001b[0m\n",
      "\u001b[31m[05/23/2018 14:39:56 INFO 140609680480064] # Starting training for epoch 25\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:00 INFO 140609680480064] # Finished training epoch 25 on 260977 examples from 255 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:00 INFO 140609680480064] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:00 INFO 140609680480064] Loss (name: value) total: 6.21530988918\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:00 INFO 140609680480064] Loss (name: value) kld: 0.113291136395\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:00 INFO 140609680480064] Loss (name: value) recons: 6.10201876023\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:00 INFO 140609680480064] Loss (name: value) logppx: 6.21530988918\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:00 INFO 140609680480064] #quality_metric: host=algo-1, epoch=25, train total_loss <loss>=6.21530988918\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:00 INFO 140609680480064] patience losses:[6.2184555240705901, 6.2175216113819793, 6.2164618810017904] min patience loss:6.216461881 current loss:6.21530988918 absolute loss difference:0.00115199182548\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:00 INFO 140609680480064] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Total Batches Seen\": {\"count\": 1, \"max\": 6375, \"sum\": 6375.0, \"min\": 6375}, \"Total Records Seen\": {\"count\": 1, \"max\": 6524425, \"sum\": 6524425.0, \"min\": 6524425}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Reset Count\": {\"count\": 1, \"max\": 50, \"sum\": 50.0, \"min\": 50}}, \"EndTime\": 1527086400.50516, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 24}, \"StartTime\": 1527086400.5051}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:00 INFO 140609680480064] #throughput_metric: host=algo-1, train throughput=63394.6185834 records/second\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:00 INFO 140609680480064] \u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:00 INFO 140609680480064] # Starting training for epoch 26\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:04 INFO 140609680480064] # Finished training epoch 26 on 260977 examples from 255 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:04 INFO 140609680480064] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:04 INFO 140609680480064] Loss (name: value) total: 6.21440573674\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:04 INFO 140609680480064] Loss (name: value) kld: 0.113950146501\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:04 INFO 140609680480064] Loss (name: value) recons: 6.10045559453\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:04 INFO 140609680480064] Loss (name: value) logppx: 6.21440573674\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:04 INFO 140609680480064] #quality_metric: host=algo-1, epoch=26, train total_loss <loss>=6.21440573674\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:04 INFO 140609680480064] patience losses:[6.2175216113819793, 6.2164618810017904, 6.2153098891763126] min patience loss:6.21530988918 current loss:6.21440573674 absolute loss difference:0.00090415244009\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:04 INFO 140609680480064] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:04 INFO 140609680480064] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Total Batches Seen\": {\"count\": 1, \"max\": 6630, \"sum\": 6630.0, \"min\": 6630}, \"Total Records Seen\": {\"count\": 1, \"max\": 6785402, \"sum\": 6785402.0, \"min\": 6785402}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Reset Count\": {\"count\": 1, \"max\": 52, \"sum\": 52.0, \"min\": 52}}, \"EndTime\": 1527086404.493865, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 25}, \"StartTime\": 1527086404.49381}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:04 INFO 140609680480064] #throughput_metric: host=algo-1, train throughput=65433.5199773 records/second\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:04 INFO 140609680480064] \u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:04 INFO 140609680480064] # Starting training for epoch 27\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:08 INFO 140609680480064] # Finished training epoch 27 on 260977 examples from 255 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:08 INFO 140609680480064] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:08 INFO 140609680480064] Loss (name: value) total: 6.21447797289\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:08 INFO 140609680480064] Loss (name: value) kld: 0.115226314874\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:08 INFO 140609680480064] Loss (name: value) recons: 6.09925165176\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:08 INFO 140609680480064] Loss (name: value) logppx: 6.21447797289\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:08 INFO 140609680480064] #quality_metric: host=algo-1, epoch=27, train total_loss <loss>=6.21447797289\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:08 INFO 140609680480064] patience losses:[6.2164618810017904, 6.2153098891763126, 6.2144057367362224] min patience loss:6.21440573674 current loss:6.21447797289 absolute loss difference:7.2236154594e-05\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:08 INFO 140609680480064] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:08 INFO 140609680480064] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Total Batches Seen\": {\"count\": 1, \"max\": 6885, \"sum\": 6885.0, \"min\": 6885}, \"Total Records Seen\": {\"count\": 1, \"max\": 7046379, \"sum\": 7046379.0, \"min\": 7046379}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Reset Count\": {\"count\": 1, \"max\": 54, \"sum\": 54.0, \"min\": 54}}, \"EndTime\": 1527086408.617439, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 26}, \"StartTime\": 1527086408.61739}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:08 INFO 140609680480064] #throughput_metric: host=algo-1, train throughput=63291.7636521 records/second\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:08 INFO 140609680480064] \u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:08 INFO 140609680480064] # Starting training for epoch 28\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:12 INFO 140609680480064] # Finished training epoch 28 on 260977 examples from 255 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:12 INFO 140609680480064] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:12 INFO 140609680480064] Loss (name: value) total: 6.21428623012\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:12 INFO 140609680480064] Loss (name: value) kld: 0.115853257419\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:12 INFO 140609680480064] Loss (name: value) recons: 6.09843297472\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:12 INFO 140609680480064] Loss (name: value) logppx: 6.21428623012\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:12 INFO 140609680480064] #quality_metric: host=algo-1, epoch=28, train total_loss <loss>=6.21428623012\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:12 INFO 140609680480064] patience losses:[6.2153098891763126, 6.2144057367362224, 6.2144779728908164] min patience loss:6.21440573674 current loss:6.21428623012 absolute loss difference:0.000119506611544\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:12 INFO 140609680480064] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:12 INFO 140609680480064] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Total Batches Seen\": {\"count\": 1, \"max\": 7140, \"sum\": 7140.0, \"min\": 7140}, \"Total Records Seen\": {\"count\": 1, \"max\": 7307356, \"sum\": 7307356.0, \"min\": 7307356}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Reset Count\": {\"count\": 1, \"max\": 56, \"sum\": 56.0, \"min\": 56}}, \"EndTime\": 1527086412.814829, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 27}, \"StartTime\": 1527086412.814768}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:12 INFO 140609680480064] #throughput_metric: host=algo-1, train throughput=62178.1792042 records/second\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:12 INFO 140609680480064] \u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:12 INFO 140609680480064] # Starting training for epoch 29\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[05/23/2018 14:40:16 INFO 140609680480064] # Finished training epoch 29 on 260977 examples from 255 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:16 INFO 140609680480064] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:16 INFO 140609680480064] Loss (name: value) total: 6.21296051624\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:16 INFO 140609680480064] Loss (name: value) kld: 0.116266710706\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:16 INFO 140609680480064] Loss (name: value) recons: 6.09669382993\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:16 INFO 140609680480064] Loss (name: value) logppx: 6.21296051624\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:16 INFO 140609680480064] #quality_metric: host=algo-1, epoch=29, train total_loss <loss>=6.21296051624\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:16 INFO 140609680480064] patience losses:[6.2144057367362224, 6.2144779728908164, 6.2142862301246788] min patience loss:6.21428623012 current loss:6.21296051624 absolute loss difference:0.00132571388693\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:16 INFO 140609680480064] #progress_metric: host=algo-1, completed 58 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Total Batches Seen\": {\"count\": 1, \"max\": 7395, \"sum\": 7395.0, \"min\": 7395}, \"Total Records Seen\": {\"count\": 1, \"max\": 7568333, \"sum\": 7568333.0, \"min\": 7568333}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Reset Count\": {\"count\": 1, \"max\": 58, \"sum\": 58.0, \"min\": 58}}, \"EndTime\": 1527086416.832816, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 28}, \"StartTime\": 1527086416.832773}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:16 INFO 140609680480064] #throughput_metric: host=algo-1, train throughput=64955.8608414 records/second\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:16 INFO 140609680480064] \u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:16 INFO 140609680480064] # Starting training for epoch 30\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:20 INFO 140609680480064] # Finished training epoch 30 on 260977 examples from 255 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:20 INFO 140609680480064] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:20 INFO 140609680480064] Loss (name: value) total: 6.21279188418\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:20 INFO 140609680480064] Loss (name: value) kld: 0.117291736574\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:20 INFO 140609680480064] Loss (name: value) recons: 6.09550016254\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:20 INFO 140609680480064] Loss (name: value) logppx: 6.21279188418\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:20 INFO 140609680480064] #quality_metric: host=algo-1, epoch=30, train total_loss <loss>=6.21279188418\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:20 INFO 140609680480064] patience losses:[6.2144779728908164, 6.2142862301246788, 6.2129605162377448] min patience loss:6.21296051624 current loss:6.21279188418 absolute loss difference:0.000168632058536\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:20 INFO 140609680480064] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:20 INFO 140609680480064] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Total Batches Seen\": {\"count\": 1, \"max\": 7650, \"sum\": 7650.0, \"min\": 7650}, \"Total Records Seen\": {\"count\": 1, \"max\": 7829310, \"sum\": 7829310.0, \"min\": 7829310}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Reset Count\": {\"count\": 1, \"max\": 60, \"sum\": 60.0, \"min\": 60}}, \"EndTime\": 1527086420.90819, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 29}, \"StartTime\": 1527086420.90813}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:20 INFO 140609680480064] #throughput_metric: host=algo-1, train throughput=64039.7442222 records/second\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:20 INFO 140609680480064] \u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:20 INFO 140609680480064] # Starting training for epoch 31\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:25 INFO 140609680480064] # Finished training epoch 31 on 260977 examples from 255 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:25 INFO 140609680480064] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:25 INFO 140609680480064] Loss (name: value) total: 6.21244075813\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:25 INFO 140609680480064] Loss (name: value) kld: 0.117423420882\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:25 INFO 140609680480064] Loss (name: value) recons: 6.09501737146\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:25 INFO 140609680480064] Loss (name: value) logppx: 6.21244075813\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:25 INFO 140609680480064] #quality_metric: host=algo-1, epoch=31, train total_loss <loss>=6.21244075813\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:25 INFO 140609680480064] patience losses:[6.2142862301246788, 6.2129605162377448, 6.2127918841792091] min patience loss:6.21279188418 current loss:6.21244075813 absolute loss difference:0.000351126053754\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:25 INFO 140609680480064] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:25 INFO 140609680480064] #progress_metric: host=algo-1, completed 62 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Total Batches Seen\": {\"count\": 1, \"max\": 7905, \"sum\": 7905.0, \"min\": 7905}, \"Total Records Seen\": {\"count\": 1, \"max\": 8090287, \"sum\": 8090287.0, \"min\": 8090287}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Reset Count\": {\"count\": 1, \"max\": 62, \"sum\": 62.0, \"min\": 62}}, \"EndTime\": 1527086425.03841, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 30}, \"StartTime\": 1527086425.038358}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:25 INFO 140609680480064] #throughput_metric: host=algo-1, train throughput=63190.2049539 records/second\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:25 INFO 140609680480064] \u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:25 INFO 140609680480064] # Starting training for epoch 32\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:29 INFO 140609680480064] # Finished training epoch 32 on 260977 examples from 255 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:29 INFO 140609680480064] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:29 INFO 140609680480064] Loss (name: value) total: 6.21247142717\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:29 INFO 140609680480064] Loss (name: value) kld: 0.117777593755\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:29 INFO 140609680480064] Loss (name: value) recons: 6.09469385334\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:29 INFO 140609680480064] Loss (name: value) logppx: 6.21247142717\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:29 INFO 140609680480064] #quality_metric: host=algo-1, epoch=32, train total_loss <loss>=6.21247142717\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:29 INFO 140609680480064] patience losses:[6.2129605162377448, 6.2127918841792091, 6.2124407581254548] min patience loss:6.21244075813 current loss:6.21247142717 absolute loss difference:3.06690440457e-05\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:29 INFO 140609680480064] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:29 INFO 140609680480064] #progress_metric: host=algo-1, completed 64 % of epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Total Batches Seen\": {\"count\": 1, \"max\": 8160, \"sum\": 8160.0, \"min\": 8160}, \"Total Records Seen\": {\"count\": 1, \"max\": 8351264, \"sum\": 8351264.0, \"min\": 8351264}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Reset Count\": {\"count\": 1, \"max\": 64, \"sum\": 64.0, \"min\": 64}}, \"EndTime\": 1527086429.172821, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 31}, \"StartTime\": 1527086429.17277}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:29 INFO 140609680480064] #throughput_metric: host=algo-1, train throughput=63126.0860987 records/second\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:29 INFO 140609680480064] \u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:29 INFO 140609680480064] # Starting training for epoch 33\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m[05/23/2018 14:40:33 INFO 140609680480064] # Finished training epoch 33 on 260977 examples from 255 batches, each of size 1024.\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:33 INFO 140609680480064] Metrics for Training:\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:33 INFO 140609680480064] Loss (name: value) total: 6.21146457523\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:33 INFO 140609680480064] Loss (name: value) kld: 0.117858771132\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:33 INFO 140609680480064] Loss (name: value) recons: 6.09360578574\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:33 INFO 140609680480064] Loss (name: value) logppx: 6.21146457523\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:33 INFO 140609680480064] #quality_metric: host=algo-1, epoch=33, train total_loss <loss>=6.21146457523\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:33 INFO 140609680480064] patience losses:[6.2127918841792091, 6.2124407581254548, 6.2124714271695005] min patience loss:6.21244075813 current loss:6.21146457523 absolute loss difference:0.000976182900223\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:33 INFO 140609680480064] Bad epoch: loss has not improved (enough). Bad count:4\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:33 INFO 140609680480064] Bad epochs exceeded patience. Stopping training early!\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:33 INFO 140609680480064] Early stop condition met. Stopping training.\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:33 INFO 140609680480064] #progress_metric: host=algo-1, completed 100 % epochs\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 255, \"sum\": 255.0, \"min\": 255}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Total Batches Seen\": {\"count\": 1, \"max\": 8415, \"sum\": 8415.0, \"min\": 8415}, \"Total Records Seen\": {\"count\": 1, \"max\": 8612241, \"sum\": 8612241.0, \"min\": 8612241}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 260977, \"sum\": 260977.0, \"min\": 260977}, \"Reset Count\": {\"count\": 1, \"max\": 66, \"sum\": 66.0, \"min\": 66}}, \"EndTime\": 1527086433.260211, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\", \"epoch\": 32}, \"StartTime\": 1527086433.26015}\n",
      "\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:33 INFO 140609680480064] #throughput_metric: host=algo-1, train throughput=63851.9289832 records/second\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:33 WARNING 140609680480064] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:33 INFO 140609680480064] Best model based on early stopping at epoch 33. Best loss: 6.21146457523\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:33 INFO 140609680480064] Topics from epoch:final (num_topics:20):\u001b[0m\n",
      "\u001b[31m220 981 145 624 320 315 551 947 108 204 242 598 261 662 467 123 119 484 267 264\u001b[0m\n",
      "\u001b[31m697 795 849 312 341 84 447 439 192 233 180 705 919 839 153 657 63 432 710 453\u001b[0m\n",
      "\u001b[31m849 341 467 862 131 447 546 84 795 919 705 888 192 312 839 63 981 296 386 249\u001b[0m\n",
      "\u001b[31m373 690 28 548 259 9 390 239 704 263 431 969 44 67 334 209 105 439 784 325\u001b[0m\n",
      "\u001b[31m312 705 341 84 447 710 131 572 849 403 192 336 180 697 166 839 919 855 189 472\u001b[0m\n",
      "\u001b[31m953 397 379 848 425 705 447 53 467 242 107 248 631 971 395 854 536 792 706 734\u001b[0m\n",
      "\u001b[31m220 320 145 315 108 204 624 267 261 947 627 731 516 0 313 683 802 393 484 440\u001b[0m\n",
      "\u001b[31m220 849 192 705 393 919 904 131 689 862 302 447 376 953 315 35 568 937 795 401\u001b[0m\n",
      "\u001b[31m705 849 341 795 605 953 336 131 710 54 919 473 233 312 848 84 941 657 866 447\u001b[0m\n",
      "\u001b[31m351 378 411 914 510 385 754 778 427 848 658 584 651 815 798 819 633 256 741 763\u001b[0m\n",
      "\u001b[31m705 680 489 192 131 341 233 849 936 432 795 438 631 914 467 837 798 331 466 862\u001b[0m\n",
      "\u001b[31m705 849 862 605 166 341 941 572 919 952 376 14 312 522 192 937 180 597 84 470\u001b[0m\n",
      "\u001b[31m439 849 447 180 341 919 84 572 566 795 131 705 192 233 336 133 459 53 312 517\u001b[0m\n",
      "\u001b[31m341 192 849 705 439 131 312 84 180 952 233 447 397 919 14 467 432 481 253 133\u001b[0m\n",
      "\u001b[31m312 439 705 795 849 697 447 84 192 233 131 685 862 166 341 376 63 249 566 397\u001b[0m\n",
      "\u001b[31m439 849 862 839 341 552 795 855 710 705 192 84 312 467 843 131 605 362 470 14\u001b[0m\n",
      "\u001b[31m862 312 685 84 705 849 341 249 697 131 397 233 192 839 447 53 331 855 180 336\u001b[0m\n",
      "\u001b[31m839 166 705 312 439 84 657 443 131 341 14 862 133 336 362 481 192 153 849 572\u001b[0m\n",
      "\u001b[31m705 84 312 849 439 566 192 447 180 249 131 336 795 63 517 133 605 685 296 341\u001b[0m\n",
      "\u001b[31m849 341 447 705 192 131 964 839 439 63 795 180 685 84 992 249 952 421 362 891\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:33 INFO 140609680480064] Serializing model to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:33 INFO 140609680480064] Saved checkpoint to \"/tmp/tmpfjh2TY/state-0001.params\"\u001b[0m\n",
      "\u001b[31m[05/23/2018 14:40:33 INFO 140609680480064] Test data is not provided.\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 145515.51008224487, \"sum\": 145515.51008224487, \"min\": 145515.51008224487}, \"finalize.time\": {\"count\": 1, \"max\": 69.71406936645508, \"sum\": 69.71406936645508, \"min\": 69.71406936645508}, \"initialize.time\": {\"count\": 1, \"max\": 10200.802087783813, \"sum\": 10200.802087783813, \"min\": 10200.802087783813}, \"model.serialize.time\": {\"count\": 1, \"max\": 2.7289390563964844, \"sum\": 2.7289390563964844, \"min\": 2.7289390563964844}, \"setuptime\": {\"count\": 1, \"max\": 16.370058059692383, \"sum\": 16.370058059692383, \"min\": 16.370058059692383}, \"early_stop.time\": {\"count\": 33, \"max\": 4.4040679931640625, \"sum\": 108.87384414672852, \"min\": 0.1709461212158203}, \"update.time\": {\"count\": 33, \"max\": 4196.950912475586, \"sum\": 135162.86516189575, \"min\": 3988.1298542022705}, \"epochs\": {\"count\": 1, \"max\": 50, \"sum\": 50.0, \"min\": 50}}, \"EndTime\": 1527086433.335275, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/NTM\"}, \"StartTime\": 1527086287.860117}\n",
      "\u001b[0m\n",
      "===== Job Complete =====\n",
      "Billable seconds: 279\n"
     ]
    }
   ],
   "source": [
    "ntm.fit({'train': 's3://{}/data/nytimes-model/recordio/ntm.data'.format('scw-use1-cors-test-3')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "A trained model does nothing on its own. We now want to use the model to perform inference. For this example, that means predicting the topic mixture representing a given document or comment.\n",
    "\n",
    "This is simplified by the deploy function provided by the Amazon SageMaker Python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: ntm-2018-05-23-14-49-44-899\n",
      "INFO:sagemaker:Creating endpoint with name ntm-2018-05-23-14-34-25-654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "ntm_predictor = ntm.deploy(initial_instance_count=1,\n",
    "                           instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Inference\n",
    "\n",
    "With this real-time endpoint at our fingertips we can finally perform inference on our training and test data.  We should first discuss the meaning of the SageMaker NTM inference output.\n",
    "\n",
    "For each document we wish to compute its corresponding `topic_weights`. Each set of topic weights is a probability distribution over the number of topics, which is 5 in this example. Of the 5 topics discovered during NTM training each element of the topic weights is the proportion to which the input document is represented by the corresponding topic.\n",
    "\n",
    "For example, if the topic weights of an input document $\\mathbf{w}$ is,\n",
    "\n",
    "$$\\theta = \\left[ 0.3, 0.2, 0, 0.5, 0 \\right]$$\n",
    "\n",
    "then $\\mathbf{w}$ is 30% generated from Topic #1, 20% from Topic #2, and 50% from Topic #4. Below, we compute the topic mixtures for the first ten traning documents.\n",
    "\n",
    "First, we setup our serializes and deserializers which allow us to convert NumPy arrays to CSV strings which we can pass into our HTTP POST request to our hosted endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "ntm_predictor.content_type = 'text/csv'\n",
    "ntm_predictor.serializer = csv_serializer\n",
    "ntm_predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "test_comments = np.load('test_comments.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03881462 0.0330111  0.02953554 0.44098815 0.03035911 0.0249091\n",
      " 0.01516498 0.02849843 0.02895007 0.02496313 0.02657804 0.0277223\n",
      " 0.02970661 0.0321646  0.03495577 0.03026009 0.0334294  0.02988204\n",
      " 0.03124016 0.02886663]\n"
     ]
    }
   ],
   "source": [
    "results = ntm_predictor.predict(test_comments)\n",
    "predictions = np.array([prediction['topic_weights'] for prediction in results['predictions']])\n",
    "print(predictions[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 0, 3, 3, 3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint with name: ntm-2018-05-22-22-03-44-121\n"
     ]
    }
   ],
   "source": [
    "sagemaker.Session().delete_endpoint(ntm_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
